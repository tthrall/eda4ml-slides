---
title: "Sampling and Study Design"
subtitle: "Why *how* you collect data matters more than *how much*"
author: "EDA for Machine Learning"
format:
  revealjs:
    theme: [dark, eda4ml-slides.scss]
    slide-number: true
    preview-links: auto
    incremental: false
    code-fold: false
    echo: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(knitr)
```

# Opening

## The Big Data Paradox

If we have millions of records, why does sampling theory matter?

. . .

**Answer**: As soon as you ask "what would happen if..." you're generalizing beyond your data.

. . .

::: {.callout-note}
## ML Connection
Training data should be a *sample* from the deployment environment.
:::

::: {.notes}
This is the key insight for a data science audience. Even "big data" is a sample from some larger process. The question is whether it's a *representative* sample.
:::

## Chapter Roadmap

1. **Observational studies**: Learning from data you didn't generate
2. **Experimental studies**: Learning from conditions you control  
3. **Measurement**: Bias, chance error, and uncertainty
4. **The role of EDA**: Bridging design and modeling

::: {.notes}
Instructor note: This chapter draws heavily from Freedman, Pisani, and Purves (FPP). The examples are classic but the lessons are timeless.
:::

# Observational Studies

## Literary Digest, 1936: The Largest Poll in History

```{r}
#| label: lit-digest-tbl
#| echo: false

lit_digest_tbl <- tibble::tribble(
  ~Source, ~`FDR Predicted %`, 
  "Literary Digest (n = 2.4 million)", 43L, 
  "Gallup prediction of Digest", 44L, 
  "Gallup prediction of election (n = 50,000)", 56L, 
  "Actual election result", 62L
)

lit_digest_tbl |> knitr::kable()
```

. . .

**The Digest's error of 19 percentage points is the largest in polling history.**

::: {.notes}
Pause here to let the numbers sink in. 2.4 million respondents got it catastrophically wrong. 50,000 got it roughly right. This sets up the key lesson.
:::

## What Went Wrong?

The Digest sampled from:

- Automobile registrations
- Telephone directories
- Magazine subscription lists

. . .

In 1936, these **skewed wealthy**—and wealthy voters favored Landon.

. . .

::: {.callout-warning}
## Selection Bias
The sampling frame excluded the population of interest.
:::

::: {.callout-note}
## ML Connection
If your training data excludes a subpopulation, your model won't serve them.
:::

## Truman vs. Dewey, 1948: Quota Sampling Fails

```{r}
#| label: truman-dewey-tbl
#| echo: false

truman_dewey_tbl <- tibble::tribble(
  ~Source, ~Truman, ~Dewey, ~Thurmond, ~Wallace, 
  "Crossley", 45L, 50L, 2L, 3L, 
  "Gallup", 44L, 50L, 2L, 4L, 
  "Roper", 38L, 53L, 5L, 4L, 
  "**Actual result**", 50L, 45L, 3L, 2L
)

truman_dewey_tbl |> knitr::kable()
```

. . .

All three major polls predicted Dewey by 5+ points. All three were wrong.

::: {.notes}
This is the famous "Dewey Defeats Truman" headline moment. Ask the class if they've seen the photo of Truman holding that newspaper.
:::

## Quota Sampling: The Problem

**Quota sampling**: Match sample demographics to population demographics.

- Interviewers given quotas: X women, Y employed, Z from each region...
- Otherwise free to choose subjects

. . .

**The hidden bias**: Interviewers chose "convenient" subjects within quotas—who tended to vote Republican.

::: {.notes}
The key insight is that there are many factors influencing voting, and you can't control for all of them with quotas. Interviewers unconsciously selected people who were easier to interview.
:::

## Probability Sampling

**Key insight**: Too many known and unknown factors to control them all.

. . .

**Solution**: Let *chance* create a representative sample.

. . .

**Simple random sampling**: Every individual has a known, equal probability of selection.

. . .

::: {.callout-tip}
## Hallmark of Probability Sampling
The probability of including any given individual can be calculated *in advance*.
:::

## Practical Probability Sampling

Simple random sampling is often impractical (cost, logistics).

**Multi-stage cluster sampling**: Randomly select regions → towns → precincts → households

. . .

**Key features preserved**:

- No interviewer discretion in subject selection
- Prescribed procedure involving planned use of chance

## Post-1948: Probability Sampling Works

```{r}
#| label: us-elections-tbl
#| echo: false

us_elections_48_04 <- tibble::tribble(
  ~Year, ~`Sample Size`, ~Winner, ~`Gallup %`, ~`Actual %`, ~Error, 
  1952, 5385, "Eisenhower", 51.0, 55.1, -4.1,  
  1960, 8015, "Kennedy", 51.0, 49.7, 1.3,  
  1976, 3439, "Carter", 48.0, 50.1, -2.1,  
  1984, 3456, "Reagan", 59.0, 58.8, 0.2,  
  2000, 3571, "Bush", 48.0, 47.9, 0.1, 
  2004, 2014, "Bush", 49.0, 50.6, -1.6
)

us_elections_48_04 |> knitr::kable(digits = 1)
```

Errors mostly within ±3 percentage points—with samples of 2,000-8,000, not millions.

::: {.notes}
This is a subset of the full table in the textbook. The point is the contrast with the Literary Digest: *smaller* samples with *probability* sampling beat *huge* samples with biased sampling.
:::

## UC Berkeley Admissions: Simpson's Paradox

In the 1970s, concern arose that graduate admissions were biased against women.

. . .

**Overall admission rate**: Men 44%, Women 35%

. . .

But department-by-department analysis told a different story...

::: {.notes}
This is one of the most famous examples of Simpson's Paradox. Build the suspense before revealing the twist.
:::

## The Paradox Revealed

Most departments admitted a **higher** percentage of female applicants.

. . .

**What happened?** Women applied disproportionately to departments with lower overall admission rates.

. . .

Aggregating across departments **reversed** the apparent pattern.

. . .

::: {.callout-note}
## ML Connection
A feature (gender) appeared predictive of the outcome (admission) only because both were associated with a **confounder** (department choice). Stratify by potential confounders.
:::

## UC Berkeley: The Sampling Lesson

- Data included *all* applicants in 1973 (not a sample)
- Standard statistical formulas assume probability sampling
- If data aren't from a probability sample, **standard errors don't apply**

. . .

::: {.callout-note}
## ML Connection
Your test set is a sample; your deployment data may not be from the same distribution.
:::

::: {.notes}
This is a subtle but important point. Many ML practitioners compute confidence intervals on test metrics, but if the test set isn't representative of deployment, those intervals are meaningless.
:::

# Experimental Studies

## Why Experiment?

**Observational data**: Subjects choose their own "treatment"

. . .

**Confounding**: Factors that affect both treatment choice *and* outcome

. . .

**Experiment**: Researcher assigns treatment, *breaking* confounding

::: {.notes}
Draw a simple DAG on the board if you have time: Confounder → Treatment → Outcome, with Confounder also → Outcome. Randomization breaks the Confounder → Treatment arrow.
:::

## Salk Vaccine Trial: NFIP Design

```{r}
#| label: salk-nfip
#| echo: false

salk_nfip <- tibble::tribble(
  ~Grade, ~Group, ~Size, ~`Polio Rate (per 100k)`, 
  "2", "Vaccinated (consent)", "225,000", 25, 
  "1, 3", "Control", "725,000", 54, 
  "2", "No consent", "125,000", 44
)

salk_nfip |> knitr::kable()
```

. . .

**Problems**:

- Grade might affect polio transmission
- Consent might be confounded with risk factors

## Salk Vaccine Trial: Double-Blind Design

```{r}
#| label: salk-blind
#| echo: false

salk_blind <- tibble::tribble(
  ~Group, ~Size, ~`Polio Rate (per 100k)`, 
  "Vaccinated", "200,000", 28, 
  "Placebo control", "200,000", 71, 
  "No consent", "350,000", 46
)

salk_blind |> knitr::kable()
```

. . .

**Key insight**: Non-consent group had *lower* rate than placebo group.

Consent itself was confounded with risk!

::: {.notes}
Higher SES parents were more likely to consent, but their children were at higher risk (less early exposure → less natural immunity). This is a beautiful example of confounding that only became visible with proper randomization.
:::

## Double-Blind Design: Why It Matters

**Randomization**: Consenting parents' children assigned to vaccine *or placebo* by chance

**Blinding**: Neither parents, doctors, nor evaluators knew assignment

. . .

This eliminates:

- Selection bias in treatment assignment
- Unconscious bias in outcome measurement

::: {.callout-note}
## ML Connection
In supervised ML, annotators should not have access to information that could bias their labels. The NFIP's initial design mirrors *label leakage*.
:::

## The Portacaval Shunt: Design Determines Conclusions

```{r}
#| label: portacaval-tbl
#| echo: false

portacaval_tbl <- tibble::tribble(
  ~`Study Design`, ~`Marked Enthusiasm`, ~Moderate, ~None,
  "No controls", 24, 7, 1, 
  "Controls, not randomized", 10, 3, 2, 
  "Randomized controlled", 0, 1, 3 
)

portacaval_tbl |> knitr::kable()
```

. . .

The **same surgery** looked beneficial or useless depending on study design.

::: {.notes}
This is one of the most striking tables in all of statistics. 75% of uncontrolled studies were enthusiastic; 0% of randomized studies were. The surgery doesn't work, but bad study design made it look like it did.
:::

## The Mechanism of Bias

```{r}
#| label: shunt-survival
#| echo: false

shunt_survival <- tibble::tribble(
  ~Design, ~`Surgery Survival %`, ~`Control Survival %`, 
  "Randomized", 60, 60, 
  "Non-randomized", 60, 45
)

shunt_survival |> knitr::kable()
```

. . .

Surgery patients: ~60% survival in both study types

Control patients: 60% (randomized) vs 45% (non-randomized)

. . .

**Non-randomized studies used sicker patients as controls.**

::: {.callout-note}
## ML Connection
Evaluating a model on a non-exchangeable test set overstates performance. Randomized train/test splits guard against this bias.
:::

## Experimental Design Principles

1. **Randomization**: Breaks confounding
2. **Control group**: Provides counterfactual  
3. **Blinding**: Prevents unconscious bias in measurement

. . .

::: {.callout-note}
## ML Connection
A/B tests are randomized controlled experiments. Observational "causal" claims require strong assumptions.
:::

# Measurement and Uncertainty

## NB10: Precision Measurement

**NB10**: A standard weight, nominally 10 grams

**Study**: 100 measurements under identical conditions at the National Bureau of Standards

```{r}
#| label: NB10-data
#| echo: false

# NB10 deficit data from chapter
nb10_deficits <- c(409, 400, 406, 399, 402, 406, 401, 403, 401, 403,
                   398, 403, 407, 402, 401, 399, 400, 401, 405, 402,
                   408, 399, 399, 402, 399, 397, 407, 401, 399, 401,
                   403, 400, 410, 401, 407, 423, 406, 406, 402, 405,
                   405, 409, 399, 402, 407, 406, 413, 409, 404, 402,
                   404, 406, 407, 405, 411, 410, 410, 410, 401, 402,
                   404, 405, 392, 407, 406, 404, 403, 408, 404, 407,
                   412, 406, 409, 400, 408, 404, 401, 404, 408, 406,
                   408, 406, 401, 412, 393, 437, 418, 415, 404, 401,
                   401, 407, 412, 375, 409, 406, 398, 406, 403, 404)

NB10_long <- tibble::tibble(deficit = nb10_deficits)
```

```{r}
#| label: fig-NB10
#| echo: false
#| fig-height: 4

NB10_long |>
  ggplot(aes(x = deficit)) +
  geom_histogram(bins = 20, fill = "coral", color = "white") +
  labs(
    title = "NB10: Micrograms Below 10 Grams",
    x = "Deficit (micrograms)",
    y = "Count"
  ) +
  theme_minimal()
```

## Bias vs. Chance Error

Each measurement = **true value** + **bias** + **chance error**

. . .

- **Chance error**: Varies randomly, averages toward zero
- **Bias**: Systematic, does *not* average out

. . .

NB10 is **biased**: It weighs ~405 μg less than 10g

. . .

::: {.callout-note}
## ML Connection
Model error = bias + variance.
:::

## How Accurate is the Sample Average?

$$\text{SE of mean} = \frac{\text{SD}}{\sqrt{n}}$$

. . .

For NB10:

$$\text{SE} = \frac{6.5}{\sqrt{100}} = 0.65 \text{ micrograms}$$

. . .

Our estimate of 405 μg is probably within ~2 μg of the true value.

. . .

::: {.callout-tip}
## Key Insight
Uncertainty shrinks with $\sqrt{n}$, not $n$. Precision and accuracy are distinct.
:::

## Outliers and Non-Normality

```{r}
#| label: fig-NB10-zscore
#| echo: false
#| fig-height: 4

NB10_z <- NB10_long |> 
  dplyr::mutate(
    z = (deficit - mean(deficit)) / sd(deficit))

NB10_z |>
  ggplot(aes(x = z)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, 
                 fill = "steelblue", color = "white", alpha = 0.7) +
  stat_function(fun = dnorm, color = "red", linewidth = 1) +
  labs(
    title = "NB10: Z-Scores with Standard Normal Curve",
    x = "Z-score",
    y = "Density"
  ) +
  theme_minimal()
```

Measurements at z ≈ ±5 would be < 1 in a million under normality.

**Even careful measurement processes produce non-normal tails.**

::: {.callout-note}
## ML Connection
Don't assume normality. **Look at your data.**
:::

# The Role of EDA

## EDA Bridges Design and Modeling

**Study design** determines what data *could* be collected.

. . .

**EDA** reveals what was *actually* collected.

. . .

EDA answers:

- Does the feature distribution match deployment expectations?
- Are there systematic patterns in missing data?
- Do labels exhibit expected reliability?
- Are there outliers warranting investigation?

::: {.notes}
This is the bridge to the rest of the course. Study design sets intentions; EDA verifies whether those intentions were achieved.
:::

## The Bottom Line

No algorithm can overcome fundamentally flawed data collection.

. . .

**EDA is the diagnostic step that reveals whether data support the intended use.**

. . .

The examples in this chapter—from the *Literary Digest* poll to the NB10 measurements—show that study design flaws are often invisible until the data are examined.

# Synthesis

## Core Principles for Data Scientists

1. **How data are collected determines what conclusions are valid**

2. Sample size without representative sampling is worthless

3. Observational data cannot establish causation without strong assumptions

4. Randomized experiments are the gold standard for causal claims

5. **All estimates have uncertainty—quantify it**

## Key Concepts

| Concept | Definition |
|---------|------------|
| Selection bias | Sample systematically differs from population |
| Confounding | Third variable creates spurious association |
| Randomization | Assignment by chance mechanism |
| Double-blind | Neither subject nor evaluator knows assignment |
| Standard error | SD of a sample statistic: $\sigma / \sqrt{n}$ |

## Looking Ahead: ML Implications

- **Train/validation/test splits** are sampling problems

- **Distribution shift**: Deployment ≠ training distribution

- **Fairness**: If subgroups are undersampled, models underperform for them

- **Causal inference**: When can observational data support cause-and-effect claims?

::: {.notes}
These are the bridges to later ML material. Each of these could be a full lecture in its own right.
:::

## Team Exercises

1. A quiz has 10 questions. The average number right is 6.4 with SD = 2.0. What is the average number wrong? What is its SD?

2. Left-handedness decreases with age in survey data (10% at age 20, 4% at age 70). Does this mean people switch hands as they age?

3. The 25th percentile of height is 62.2 inches; the 75th is 65.8 inches. If the distribution is normal, find the 90th percentile.

::: {.notes}
These are the FPP exercises from the chapter. They reinforce the core concepts and can be used as in-class activities or homework.
:::

## Discussion Questions

1. What modern datasets might have Literary Digest-style selection bias?

2. When is a randomized experiment unethical or impractical?

3. How would you detect distribution shift between training and deployment?

::: {.notes}
Use these for class discussion or as take-home reflection questions.
:::

# Appendix: Instructor Notes {visibility="uncounted"}

## Timing Guide {visibility="uncounted"}

| Section | Slides | Suggested Time |
|---------|--------|----------------|
| Opening | 1-2 | 5 min |
| Observational Studies | 3-12 | 25 min |
| Experimental Studies | 13-20 | 20 min |
| Measurement/Uncertainty | 21-24 | 15 min |
| Role of EDA | 25-26 | 5 min |
| Synthesis | 27-32 | 15 min |

**Total**: ~85 minutes with discussion

## Customization Options {visibility="uncounted"}

**For shorter sessions** (50 min): Cut slides 8, 12, 24; condense synthesis

**For longer sessions**: Expand Simpson's Paradox with actual department data; add more discussion time

**For ML-focused audience**: Expand "ML Connection" callouts with code examples

**For statistics-focused audience**: Show the mathematical derivations from FPP

## Data Dependencies {visibility="uncounted"}

The NB10 data is embedded directly in this file (no external file dependency).

If you prefer to load from file, replace the `nb10_deficits` vector with:

```r
NB10_long <- here::here(
  "data", "retain", "NB10_long.txt"
) |> 
  readr::read_tsv()
```
