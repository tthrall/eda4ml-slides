[
  {
    "objectID": "ts-time-domain-slides.html#from-understanding-to-prediction",
    "href": "ts-time-domain-slides.html#from-understanding-to-prediction",
    "title": "Time Domain Methods",
    "section": "From Understanding to Prediction",
    "text": "From Understanding to Prediction\nChapter 12 established that time series have memory—the present depends on the past.\n\nNow we exploit that dependence for forecasting:\n\nGiven observations up to the present, what should we expect tomorrow, next quarter, next year?\n\n\n\nThis is the archetypal decision-support application of time series analysis."
  },
  {
    "objectID": "ts-time-domain-slides.html#the-central-question",
    "href": "ts-time-domain-slides.html#the-central-question",
    "title": "Time Domain Methods",
    "section": "The Central Question",
    "text": "The Central Question\n\\[\\hat{X}(t_f + h) = \\; ?\\]\n\nGiven observations \\(X(t_0), X(t_0+1), \\ldots, X(t_f)\\), predict the value \\(h\\) steps ahead.\n\n\nKey insight: The autocorrelation structure tells us how to weight past observations.\n\nStrong recent dependence → recent values matter most\nPeriodic structure → values from the same season matter\nLong memory → distant past still informative"
  },
  {
    "objectID": "ts-time-domain-slides.html#building-blocks",
    "href": "ts-time-domain-slides.html#building-blocks",
    "title": "Time Domain Methods",
    "section": "Building Blocks",
    "text": "Building Blocks\nAll time domain models build from these components:\n\n\n\nComponent\nRole\n\n\n\n\nWhite noise \\(W(t)\\)\nIndependent shocks; the baseline\n\n\nBack-shift \\(\\mathcal{B}\\)\n\\(\\mathcal{B}X(t) = X(t-1)\\)\n\n\nDifferencing \\(\\nabla\\)\n\\(\\nabla = 1 - \\mathcal{B}\\) removes trend\n\n\nAR polynomial \\(\\phi(\\mathcal{B})\\)\nAutoregressive structure\n\n\nMA polynomial \\(\\theta(\\mathcal{B})\\)\nMoving average structure\n\n\n\n\nThe art is combining these to capture the dependence structure revealed by ACF and PACF."
  },
  {
    "objectID": "ts-time-domain-slides.html#ar1-the-simplest-memory",
    "href": "ts-time-domain-slides.html#ar1-the-simplest-memory",
    "title": "Time Domain Methods",
    "section": "AR(1): The Simplest Memory",
    "text": "AR(1): The Simplest Memory\nAn AR(1) process:\n\\[X(t) = \\phi_1 X(t-1) + W(t)\\]\n\nThe current value is a fraction of the previous value, plus a random shock.\n\n\nConstraints:\n\n\\(|\\phi_1| &lt; 1\\) ensures stationarity\n\\(\\phi_1 &gt; 0\\): positive dependence (most common)\n\\(\\phi_1 &lt; 0\\): alternating pattern"
  },
  {
    "objectID": "ts-time-domain-slides.html#ar1-acf-and-pacf",
    "href": "ts-time-domain-slides.html#ar1-acf-and-pacf",
    "title": "Time Domain Methods",
    "section": "AR(1): ACF and PACF",
    "text": "AR(1): ACF and PACF\n\n\nFigure 1: Simulated AR(1) with φ = 0.8\n\nACF: Geometric decay \\(\\rho(h) = \\phi_1^{|h|}\\)\nPACF: Cuts off after lag 1"
  },
  {
    "objectID": "ts-time-domain-slides.html#ar1-the-signature",
    "href": "ts-time-domain-slides.html#ar1-the-signature",
    "title": "Time Domain Methods",
    "section": "AR(1): The Signature",
    "text": "AR(1): The Signature\n\\[\\rho_X(h) = \\phi_1^{|h|}\\]\n\nThe autocorrelation at lag \\(h\\) is the AR coefficient raised to the \\(|h|\\) power.\n\n\nPACF: \\[\\phi_{hh} = \\begin{cases} \\phi_1 & h = 1 \\\\ 0 & h &gt; 1 \\end{cases}\\]\n\n\nThe PACF cuts off after lag 1—this is the diagnostic signature of AR(1)."
  },
  {
    "objectID": "ts-time-domain-slides.html#arp-longer-memory",
    "href": "ts-time-domain-slides.html#arp-longer-memory",
    "title": "Time Domain Methods",
    "section": "AR(p): Longer Memory",
    "text": "AR(p): Longer Memory\nAn AR(p) process depends on \\(p\\) past values:\n\\[X(t) = \\phi_1 X(t-1) + \\phi_2 X(t-2) + \\cdots + \\phi_p X(t-p) + W(t)\\]\n\nPolynomial form: \\[\\phi(\\mathcal{B})(X(t) - \\mu) = W(t)\\]\nwhere \\(\\phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p\\)"
  },
  {
    "objectID": "ts-time-domain-slides.html#arp-diagnostic-signature",
    "href": "ts-time-domain-slides.html#arp-diagnostic-signature",
    "title": "Time Domain Methods",
    "section": "AR(p): Diagnostic Signature",
    "text": "AR(p): Diagnostic Signature\n\n\n\nFeature\nAR(p) Pattern\n\n\n\n\nACF\nDecays gradually (exponential or damped oscillation)\n\n\nPACF\nCuts off after lag \\(p\\)\n\n\n\n\nThe PACF cutoff identifies the order. If you see that PACF is significant at lags 1 through \\(p\\) and near zero thereafter, consider an AR(p) model."
  },
  {
    "objectID": "ts-time-domain-slides.html#ar2-example-recruitment",
    "href": "ts-time-domain-slides.html#ar2-example-recruitment",
    "title": "Time Domain Methods",
    "section": "AR(2) Example: Recruitment",
    "text": "AR(2) Example: Recruitment\n\n\nFigure 2: Recruitment data: ACF and PACF suggest AR(2)\n\nACF shows damped oscillation (decays gradually)\nPACF has significant values at lags 1 and 2, then cuts off\nThis pattern suggests AR(2)"
  },
  {
    "objectID": "ts-time-domain-slides.html#ma1-dependence-through-shocks",
    "href": "ts-time-domain-slides.html#ma1-dependence-through-shocks",
    "title": "Time Domain Methods",
    "section": "MA(1): Dependence Through Shocks",
    "text": "MA(1): Dependence Through Shocks\nAn MA(1) process:\n\\[X(t) = W(t) + \\theta_1 W(t-1)\\]\n\nThe current value depends on the current shock and the previous shock.\n\n\nKey difference from AR:\n\nAR: infinite memory (geometric decay)\nMA: finite memory (sharp cutoff)"
  },
  {
    "objectID": "ts-time-domain-slides.html#ma1-acf-and-pacf",
    "href": "ts-time-domain-slides.html#ma1-acf-and-pacf",
    "title": "Time Domain Methods",
    "section": "MA(1): ACF and PACF",
    "text": "MA(1): ACF and PACF\nThe autocorrelation structure:\n\\[\\rho_X(h) = \\begin{cases} 1 & h = 0 \\\\ \\frac{\\theta_1}{1 + \\theta_1^2} & h = \\pm 1 \\\\ 0 & |h| &gt; 1 \\end{cases}\\]\n\n\n\n\nFeature\nMA(1) Pattern\n\n\n\n\nACF\nCuts off after lag 1\n\n\nPACF\nDecays gradually\n\n\n\n\n\nThe opposite of AR(1)!"
  },
  {
    "objectID": "ts-time-domain-slides.html#maq-the-general-case",
    "href": "ts-time-domain-slides.html#maq-the-general-case",
    "title": "Time Domain Methods",
    "section": "MA(q): The General Case",
    "text": "MA(q): The General Case\nAn MA(q) process:\n\\[X(t) = W(t) + \\theta_1 W(t-1) + \\cdots + \\theta_q W(t-q)\\]\n\nPolynomial form: \\[X(t) - \\mu = \\theta(\\mathcal{B}) W(t)\\]\nwhere \\(\\theta(z) = 1 + \\theta_1 z + \\cdots + \\theta_q z^q\\)\n\n\nDiagnostic signature: ACF cuts off after lag \\(q\\); PACF decays."
  },
  {
    "objectID": "ts-time-domain-slides.html#invertibility",
    "href": "ts-time-domain-slides.html#invertibility",
    "title": "Time Domain Methods",
    "section": "Invertibility",
    "text": "Invertibility\nProblem: Different MA models can produce identical distributions.\n\nExample: These two MA(1) processes are indistinguishable:\n\\[\n\\begin{align}\n  X(t) &= W(t) + 0.2 W(t-1) & \\sigma_W^2 = 25 \\\\ \\\\\n  Y(t) &= V(t) + 5 V(t-1) & \\sigma_V^2 = 1\n\\end{align}\n\\]\n\n\nSolution: Require invertibility—all roots of \\(\\theta(z)\\) outside the unit circle.\nThis ensures a unique representation and allows expressing \\(W(t)\\) in terms of past \\(X\\) values."
  },
  {
    "objectID": "ts-time-domain-slides.html#acf-vs-pacf-the-key-patterns",
    "href": "ts-time-domain-slides.html#acf-vs-pacf-the-key-patterns",
    "title": "Time Domain Methods",
    "section": "ACF vs PACF: The Key Patterns",
    "text": "ACF vs PACF: The Key Patterns\n\n\n\nModel\nACF\nPACF\n\n\n\n\nAR(p)\nDecays gradually\nCuts off after lag \\(p\\)\n\n\nMA(q)\nCuts off after lag \\(q\\)\nDecays gradually\n\n\nARMA(p,q)\nDecays gradually\nDecays gradually\n\n\nWhite noise\nAll ≈ 0\nAll ≈ 0\n\n\n\n\nThe identification strategy:\n\nIf PACF cuts off → AR\nIf ACF cuts off → MA\n\nIf both decay → ARMA (or consider differencing)"
  },
  {
    "objectID": "ts-time-domain-slides.html#visual-guide-ar-signatures",
    "href": "ts-time-domain-slides.html#visual-guide-ar-signatures",
    "title": "Time Domain Methods",
    "section": "Visual Guide: AR Signatures",
    "text": "Visual Guide: AR Signatures\n\n\nFigure 3: AR(1) and AR(2) signatures"
  },
  {
    "objectID": "ts-time-domain-slides.html#visual-guide-ma-signatures",
    "href": "ts-time-domain-slides.html#visual-guide-ma-signatures",
    "title": "Time Domain Methods",
    "section": "Visual Guide: MA Signatures",
    "text": "Visual Guide: MA Signatures\n\n\nFigure 4: MA(1) and MA(2) signatures"
  },
  {
    "objectID": "ts-time-domain-slides.html#combining-ar-and-ma",
    "href": "ts-time-domain-slides.html#combining-ar-and-ma",
    "title": "Time Domain Methods",
    "section": "Combining AR and MA",
    "text": "Combining AR and MA\nAn ARMA(p,q) process:\n\\[\\phi(\\mathcal{B})(X(t) - \\mu) = \\theta(\\mathcal{B}) W(t)\\]\n\nExpanded: \\[X(t) = \\mu + \\sum_{j=1}^{p} \\phi_j (X(t-j) - \\mu) + W(t) + \\sum_{k=1}^{q} \\theta_k W(t-k)\\]\n\n\nWhy combine? Parsimony—a low-order ARMA may capture structure that would require high-order pure AR or MA."
  },
  {
    "objectID": "ts-time-domain-slides.html#arma-two-representations",
    "href": "ts-time-domain-slides.html#arma-two-representations",
    "title": "Time Domain Methods",
    "section": "ARMA: Two Representations",
    "text": "ARMA: Two Representations\nCausal form (as filtered white noise): \\[X(t) - \\mu = \\sum_{\\nu=0}^{\\infty} \\psi_\\nu W(t-\\nu)\\]\n\nInverted form (white noise as filtered \\(X\\)): \\[W(t) = \\sum_{\\nu=0}^{\\infty} \\pi_\\nu (X(t-\\nu) - \\mu)\\]\n\n\nBoth representations exist when AR and MA polynomials have roots outside the unit circle."
  },
  {
    "objectID": "ts-time-domain-slides.html#arma-diagnostics",
    "href": "ts-time-domain-slides.html#arma-diagnostics",
    "title": "Time Domain Methods",
    "section": "ARMA Diagnostics",
    "text": "ARMA Diagnostics\nFor ARMA(p,q):\n\nACF: Decays gradually (mixture of exponential and/or damped sinusoid)\nPACF: Decays gradually\n\n\nIdentification challenge: Both decay, so order selection is less clear.\n\n\nPractical approach:\n\nTry to identify if pure AR or MA fits (check for cutoffs)\nIf both decay, consider low-order ARMA(1,1) or ARMA(2,1)\nUse information criteria (AIC, BIC) to compare models"
  },
  {
    "objectID": "ts-time-domain-slides.html#the-problem-with-trends",
    "href": "ts-time-domain-slides.html#the-problem-with-trends",
    "title": "Time Domain Methods",
    "section": "The Problem with Trends",
    "text": "The Problem with Trends\nMany real series are non-stationary:\n\nGlobal temperatures: upward trend\nStock prices: random walk behavior\nEconomic series: growth over time\n\n\nThe ACF and PACF diagnostics assume stationarity.\n\n\nSolution: Transform to stationarity, then model."
  },
  {
    "objectID": "ts-time-domain-slides.html#differencing-removes-trend",
    "href": "ts-time-domain-slides.html#differencing-removes-trend",
    "title": "Time Domain Methods",
    "section": "Differencing Removes Trend",
    "text": "Differencing Removes Trend\nThe differencing operator: \\[\\nabla X(t) = X(t) - X(t-1) = (1 - \\mathcal{B})X(t)\\]\n\nIf \\(X(t)\\) has a linear trend \\(\\mu + \\beta t\\): \\[E\\{\\nabla X(t)\\} = \\beta \\quad \\text{(constant)}\\]\n\n\nHigher-order differencing \\(\\nabla^d\\) removes polynomial trends of degree \\(d\\)."
  },
  {
    "objectID": "ts-time-domain-slides.html#arimap-d-q",
    "href": "ts-time-domain-slides.html#arimap-d-q",
    "title": "Time Domain Methods",
    "section": "ARIMA(p, d, q)",
    "text": "ARIMA(p, d, q)\nAn ARIMA(p, d, q) model:\n\\[\\phi(\\mathcal{B}) \\nabla^d X(t) = \\theta(\\mathcal{B}) W(t)\\]\n\n\n\\(d = 0\\): stationary ARMA\n\\(d = 1\\): first differences are ARMA (handles linear trend, random walk)\n\\(d = 2\\): second differences are ARMA (handles quadratic trend)\n\n\n\nThe “I” stands for Integrated—we integrate (cumsum) the ARMA process to get \\(X(t)\\)."
  },
  {
    "objectID": "ts-time-domain-slides.html#random-walk-with-drift",
    "href": "ts-time-domain-slides.html#random-walk-with-drift",
    "title": "Time Domain Methods",
    "section": "Random Walk with Drift",
    "text": "Random Walk with Drift\nA fundamental non-stationary model:\n\\[X(t) = \\alpha + X(t-1) + W(t)\\]\n\nThis is ARIMA(0, 1, 0) with drift \\(\\alpha\\).\n\n\nProperties:\n\n\\(\\nabla X(t) = \\alpha + W(t)\\) (differenced series is white noise + constant)\nVariance grows linearly with time\nForecast: straight line from last observation with slope \\(\\alpha\\)"
  },
  {
    "objectID": "ts-time-domain-slides.html#random-walk-forecast",
    "href": "ts-time-domain-slides.html#random-walk-forecast",
    "title": "Time Domain Methods",
    "section": "Random Walk Forecast",
    "text": "Random Walk Forecast\n\n\nFigure 5: Random walk with drift: forecast is a ray\nPrediction intervals widen with forecast horizon—uncertainty grows."
  },
  {
    "objectID": "ts-time-domain-slides.html#seasonality-in-time-series",
    "href": "ts-time-domain-slides.html#seasonality-in-time-series",
    "title": "Time Domain Methods",
    "section": "Seasonality in Time Series",
    "text": "Seasonality in Time Series\nMany series have periodic patterns:\n\nMonthly data: annual cycle\nQuarterly data: seasonal business patterns\nDaily data: weekly patterns\n\n\nSeasonal differencing removes the periodic component:\n\\[\\nabla_s X(t) = X(t) - X(t-s)\\]\nwhere \\(s\\) is the seasonal period (e.g., \\(s = 12\\) for monthly data)."
  },
  {
    "objectID": "ts-time-domain-slides.html#sarima-models",
    "href": "ts-time-domain-slides.html#sarima-models",
    "title": "Time Domain Methods",
    "section": "SARIMA Models",
    "text": "SARIMA Models\nA SARIMA model combines:\n\nNon-seasonal ARIMA(p, d, q)\nSeasonal ARIMA(P, D, Q)\\(_s\\)\n\n\n\\[\\phi(\\mathcal{B})\\Phi(\\mathcal{B}^s) \\nabla^d \\nabla_s^D X(t) = \\theta(\\mathcal{B})\\Theta(\\mathcal{B}^s) W(t)\\]\n\n\nNotation: ARIMA\\((p,d,q) \\times (P,D,Q)_s\\)"
  },
  {
    "objectID": "ts-time-domain-slides.html#co₂-example-trend-season",
    "href": "ts-time-domain-slides.html#co₂-example-trend-season",
    "title": "Time Domain Methods",
    "section": "CO₂ Example: Trend + Season",
    "text": "CO₂ Example: Trend + Season\n\n\nFigure 6: Mauna Loa CO₂: strong trend and annual cycle\nThis series needs both:\n\nRegular differencing (\\(d = 1\\)) for the trend\nSeasonal differencing (\\(D = 1\\), \\(s = 12\\)) for the annual cycle"
  },
  {
    "objectID": "ts-time-domain-slides.html#co₂-after-differencing",
    "href": "ts-time-domain-slides.html#co₂-after-differencing",
    "title": "Time Domain Methods",
    "section": "CO₂: After Differencing",
    "text": "CO₂: After Differencing\n\n\nFigure 7: CO₂ after ∇∇₁₂: approximately stationary\nAfter both differencing operations, the series appears stationary—ready for ARMA modeling."
  },
  {
    "objectID": "ts-time-domain-slides.html#co₂-sarima-forecast",
    "href": "ts-time-domain-slides.html#co₂-sarima-forecast",
    "title": "Time Domain Methods",
    "section": "CO₂: SARIMA Forecast",
    "text": "CO₂: SARIMA Forecast\n\n\nFigure 8: Five-year forecast from ARIMA(1,1,1)×(0,1,1)₁₂\nThe forecast captures both the upward trend and the seasonal oscillation."
  },
  {
    "objectID": "ts-time-domain-slides.html#the-linear-predictor",
    "href": "ts-time-domain-slides.html#the-linear-predictor",
    "title": "Time Domain Methods",
    "section": "The Linear Predictor",
    "text": "The Linear Predictor\nThe best linear predictor of \\(X(t_f + h)\\) given the past:\n\\[\\hat{X}(t_f + h) = \\alpha + \\sum_{\\nu=0}^{T-1} \\beta_\\nu X(t_0 + \\nu)\\]\n\nThe coefficients \\(\\beta_\\nu\\) are chosen to minimize mean squared prediction error:\n\\[MSE = E\\{(X(t_f + h) - \\hat{X}(t_f + h))^2\\}\\]"
  },
  {
    "objectID": "ts-time-domain-slides.html#arma-forecasting",
    "href": "ts-time-domain-slides.html#arma-forecasting",
    "title": "Time Domain Methods",
    "section": "ARMA Forecasting",
    "text": "ARMA Forecasting\nFor an ARMA process, the predictor has a simple recursive form.\n\nOne step ahead (\\(h = 1\\)): \\[\\hat{X}(t_f + 1) = \\sum_{j=1}^{p} \\phi_j X(t_f + 1 - j) + \\sum_{k=1}^{q} \\theta_k \\hat{W}(t_f + 1 - k)\\]\n\n\nKey insight: Future white noise terms have expected value 0."
  },
  {
    "objectID": "ts-time-domain-slides.html#prediction-error",
    "href": "ts-time-domain-slides.html#prediction-error",
    "title": "Time Domain Methods",
    "section": "Prediction Error",
    "text": "Prediction Error\nThe mean squared prediction error for horizon \\(h\\):\n\\[MSE(h) = \\sigma_W^2 \\sum_{\\nu=0}^{h-1} \\psi_\\nu^2\\]\n\nwhere \\(\\psi_\\nu\\) are the coefficients in the causal (MA\\((\\infty)\\)) representation.\n\n\nImplications:\n\n\\(MSE(1) = \\sigma_W^2\\) (one step ahead error is just the innovation variance)\n\\(MSE(h) \\to \\sigma_X^2\\) as \\(h \\to \\infty\\) (long-range forecast converges to unconditional variance)"
  },
  {
    "objectID": "ts-time-domain-slides.html#prediction-intervals",
    "href": "ts-time-domain-slides.html#prediction-intervals",
    "title": "Time Domain Methods",
    "section": "Prediction Intervals",
    "text": "Prediction Intervals\nFor Gaussian processes, the \\((1-\\alpha)\\) prediction interval:\n\\[\\hat{X}(t_f + h) \\pm z_{\\alpha/2} \\sqrt{MSE(h)}\\]\n\nKey feature: Intervals widen with forecast horizon.\n\n\nThis reflects a fundamental truth: uncertainty grows as we forecast further into the future."
  },
  {
    "objectID": "ts-time-domain-slides.html#ar2-forecast-example",
    "href": "ts-time-domain-slides.html#ar2-forecast-example",
    "title": "Time Domain Methods",
    "section": "AR(2) Forecast Example",
    "text": "AR(2) Forecast Example\n\n\nFigure 9: Recruitment: 24-month AR(2) forecast\nThe oscillatory forecast reflects the AR(2) structure; intervals widen with horizon."
  },
  {
    "objectID": "ts-time-domain-slides.html#simple-exponential-smoothing",
    "href": "ts-time-domain-slides.html#simple-exponential-smoothing",
    "title": "Time Domain Methods",
    "section": "Simple Exponential Smoothing",
    "text": "Simple Exponential Smoothing\nA widely used forecasting method:\n\\[\\hat{X}(t+1) = \\alpha X(t) + (1-\\alpha) \\hat{X}(t)\\]\n\nEquivalently: \\[\\hat{X}(t+1) = \\hat{X}(t) + \\alpha (X(t) - \\hat{X}(t))\\]\n\n\nInterpretation: Update the forecast by a fraction \\(\\alpha\\) of the most recent error."
  },
  {
    "objectID": "ts-time-domain-slides.html#ewma-and-arima-connection",
    "href": "ts-time-domain-slides.html#ewma-and-arima-connection",
    "title": "Time Domain Methods",
    "section": "EWMA and ARIMA Connection",
    "text": "EWMA and ARIMA Connection\nSimple exponential smoothing is the optimal forecast for an ARIMA(0,1,1) model:\n\\[\\nabla X(t) = W(t) - \\lambda W(t-1)\\]\n\nwhere \\(\\alpha = 1 - \\lambda\\) is the smoothing parameter.\n\n\nKey insight: Exponential smoothing is not just a heuristic—it has a solid statistical foundation."
  },
  {
    "objectID": "ts-time-domain-slides.html#beyond-simple-smoothing",
    "href": "ts-time-domain-slides.html#beyond-simple-smoothing",
    "title": "Time Domain Methods",
    "section": "Beyond Simple Smoothing",
    "text": "Beyond Simple Smoothing\nHolt-Winters methods extend exponential smoothing:\n\n\n\nComponent\nWhat it captures\n\n\n\n\nLevel \\(\\ell(t)\\)\nLocal mean\n\n\nTrend \\(b(t)\\)\nLocal slope\n\n\nSeason \\(s(t)\\)\nPeriodic pattern\n\n\n\n\nThese ETS (Error-Trend-Seasonal) models form an alternative to SARIMA, widely used in business forecasting."
  },
  {
    "objectID": "ts-time-domain-slides.html#identification-estimation-diagnostics",
    "href": "ts-time-domain-slides.html#identification-estimation-diagnostics",
    "title": "Time Domain Methods",
    "section": "Identification → Estimation → Diagnostics",
    "text": "Identification → Estimation → Diagnostics\n\n\n\n\n\nStep\nAction\n\n\n\n\n1. Plot\nExamine series for trend, seasonality, variance changes\n\n\n2. Transform\nDifference or transform to achieve stationarity\n\n\n3. Identify\nUse ACF/PACF to suggest model order\n\n\n4. Estimate\nFit candidate models; compare via AIC/BIC\n\n\n5. Diagnose\nCheck residuals: should look like white noise\n\n\n\n\n\n\nIterate: If residuals show structure, refine the model and repeat."
  },
  {
    "objectID": "ts-time-domain-slides.html#residual-diagnostics",
    "href": "ts-time-domain-slides.html#residual-diagnostics",
    "title": "Time Domain Methods",
    "section": "Residual Diagnostics",
    "text": "Residual Diagnostics\nAfter fitting, residuals should behave like white noise:\n\n\nACF near zero at all lags\nNo patterns in residual plot\nLjung-Box test non-significant\n\n\n\n\n\nCode\n# Ljung-Box test for residual autocorrelation\nstats::Box.test(residuals, lag = 20, type = \"Ljung-Box\")\n\n\nIf residuals show structure, the model hasn’t captured all the dependence."
  },
  {
    "objectID": "ts-time-domain-slides.html#model-comparison",
    "href": "ts-time-domain-slides.html#model-comparison",
    "title": "Time Domain Methods",
    "section": "Model Comparison",
    "text": "Model Comparison\nWhen multiple models seem plausible, use information criteria:\n\nAIC (Akaike): \\(-2\\log L + 2k\\)\nBIC (Bayesian): \\(-2\\log L + k \\log T\\)\n\n\nwhere \\(k\\) = number of parameters, \\(T\\) = sample size.\n\n\nBIC penalizes complexity more heavily → favors simpler models.\nLower is better for both criteria."
  },
  {
    "objectID": "ts-time-domain-slides.html#key-insights",
    "href": "ts-time-domain-slides.html#key-insights",
    "title": "Time Domain Methods",
    "section": "Key Insights",
    "text": "Key Insights\n\nAR models: Current value depends on past values\n\nACF decays; PACF cuts off at order \\(p\\)\n\nMA models: Current value depends on past shocks\n\nACF cuts off at order \\(q\\); PACF decays\n\nARIMA: Differencing handles non-stationarity\n\nThe “I” integrates an ARMA process\n\nSARIMA: Captures seasonal patterns\n\nCombines regular and seasonal differencing"
  },
  {
    "objectID": "ts-time-domain-slides.html#the-forecasting-reality",
    "href": "ts-time-domain-slides.html#the-forecasting-reality",
    "title": "Time Domain Methods",
    "section": "The Forecasting Reality",
    "text": "The Forecasting Reality\n\nPrediction intervals widen with forecast horizon\n\nUncertainty grows; we cannot predict arbitrarily far ahead\n\nExponential smoothing has ARIMA foundations\n\nEWMA is optimal for IMA(1,1)\n\nModel diagnostics are essential\n\nResiduals should be white noise\nInformation criteria guide model selection"
  },
  {
    "objectID": "ts-time-domain-slides.html#the-dual-aims-revisited",
    "href": "ts-time-domain-slides.html#the-dual-aims-revisited",
    "title": "Time Domain Methods",
    "section": "The Dual Aims, Revisited",
    "text": "The Dual Aims, Revisited\nTime domain methods emphasize prediction:\n\nGiven the past, what comes next?\nHow much uncertainty surrounds that forecast?\n\n\nBut prediction rests on understanding:\n\nThe ACF/PACF patterns reveal the memory structure\nThe model encodes why the past predicts the future\n\n\n\nNext: Chapter 14 takes the complementary view—frequency domain methods reveal the periodic structure that drives the process."
  },
  {
    "objectID": "ts-time-domain-slides.html#diagnostic-checklist",
    "href": "ts-time-domain-slides.html#diagnostic-checklist",
    "title": "Time Domain Methods",
    "section": "Diagnostic Checklist",
    "text": "Diagnostic Checklist\nWhen building a time series forecast:\n\nPlot the series: Identify trend, seasonality, variance changes\nTransform if needed: Differencing, log transform\nExamine ACF/PACF: Does PACF cut off? Does ACF cut off?\nFit candidate models: Start simple (AR or MA), add complexity if needed\nCheck residuals: ACF should be flat; Ljung-Box non-significant\nCompare models: Use AIC/BIC for selection\nForecast with intervals: Report uncertainty honestly"
  },
  {
    "objectID": "ts-time-domain-slides.html#team-exercise-1-ar1-vs.-ma1-signatures",
    "href": "ts-time-domain-slides.html#team-exercise-1-ar1-vs.-ma1-signatures",
    "title": "Time Domain Methods",
    "section": "Team Exercise 1: AR(1) vs. MA(1) Signatures",
    "text": "Team Exercise 1: AR(1) vs. MA(1) Signatures\nUsing stats::arima.sim():\n\nGenerate 200 observations from AR(1) with \\(\\phi = 0.7\\).\nGenerate 200 observations from MA(1) with \\(\\theta = 0.7\\).\nPlot the ACF and PACF for each. Can you tell them apart?\nWhich cuts off sharply? Which decays exponentially?"
  },
  {
    "objectID": "ts-time-domain-slides.html#team-exercise-2-model-selection",
    "href": "ts-time-domain-slides.html#team-exercise-2-model-selection",
    "title": "Time Domain Methods",
    "section": "Team Exercise 2: Model Selection",
    "text": "Team Exercise 2: Model Selection\nFor the recruitment data (astsa::rec):\n\nFit AR(1), AR(2), and AR(3) using stats::ar.ols().\nCompare their AIC values.\nDoes AIC agree with what the PACF suggests?\nWhat are the tradeoffs of parsimony vs. fit?"
  },
  {
    "objectID": "ts-time-domain-slides.html#team-exercise-3-forecast-uncertainty",
    "href": "ts-time-domain-slides.html#team-exercise-3-forecast-uncertainty",
    "title": "Time Domain Methods",
    "section": "Team Exercise 3: Forecast Uncertainty",
    "text": "Team Exercise 3: Forecast Uncertainty\nUsing the global temperature series (astsa::gtemp_land):\n\nFit an ARIMA model to the first differences.\nGenerate a 20-year forecast with prediction intervals.\nWhy do the intervals widen over time?\nWhat does this imply for long-range climate projection?\n\n\nExercise 1 is the key diagnostic skill. Exercise 2 practices model selection. Exercise 3 addresses forecast humility."
  },
  {
    "objectID": "ts-time-domain-slides.html#discussion-questions",
    "href": "ts-time-domain-slides.html#discussion-questions",
    "title": "Time Domain Methods",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nA model with great in-sample fit forecasts poorly. What went wrong?\nWhen would you choose a simpler model despite higher AIC?\nHow do ARIMA models compare to machine learning for time series?"
  },
  {
    "objectID": "ts-data-slides.html#understanding-and-prediction",
    "href": "ts-data-slides.html#understanding-and-prediction",
    "title": "Time Series Data",
    "section": "Understanding and Prediction",
    "text": "Understanding and Prediction\nThroughout this book we have seen that understanding and decision support are intertwined.\n\nTime series analysis makes this especially vivid.\n\n\n\n\n\nDomain\nQuestion\n\n\n\n\nTime\nHow does the past predict the future?\n\n\nFrequency\nWhat structure generated this process?\n\n\n\n\n\n\n\nThese are not competing approaches—they are dual perspectives on the same underlying reality, connected by mathematics."
  },
  {
    "objectID": "ts-data-slides.html#time-series-have-memory",
    "href": "ts-data-slides.html#time-series-have-memory",
    "title": "Time Series Data",
    "section": "Time Series Have Memory",
    "text": "Time Series Have Memory\nStandard assumption in statistics: Observations are independent\nTime series reality: Successive observations are dependent\n\nThis dependence is not merely a technical nuisance. It is information:\n\nInformation about how the system evolves\nInformation about underlying periodic structure\nInformation we must understand to make correct inferences"
  },
  {
    "objectID": "ts-data-slides.html#two-equivalent-descriptions",
    "href": "ts-data-slides.html#two-equivalent-descriptions",
    "title": "Time Series Data",
    "section": "Two Equivalent Descriptions",
    "text": "Two Equivalent Descriptions\nThe dependence structure of a stationary time series has two equivalent descriptions:\n\n\n\n\n\n\n\n\nDomain\nObject\nQuestion\n\n\n\n\nTime\nAutocorrelation function \\(\\rho(u)\\)\nHow does \\(X(t)\\) correlate with \\(X(t-u)\\)?\n\n\nFrequency\nSpectral density \\(f(\\lambda)\\)\nHow much variance comes from frequency \\(\\lambda\\)?\n\n\n\n\nThese contain the same information—they are Fourier transform pairs.\nUnderstanding both illuminates what either alone obscures."
  },
  {
    "objectID": "ts-data-slides.html#why-fourier-analysis-is-fundamental",
    "href": "ts-data-slides.html#why-fourier-analysis-is-fundamental",
    "title": "Time Series Data",
    "section": "Why Fourier Analysis Is Fundamental",
    "text": "Why Fourier Analysis Is Fundamental\nThis is not historical accident. There is a deep reason.\n\nThe back-shift operator \\(\\mathcal{B}\\) shifts a process in time:\n\\[[\\mathcal{B}X](t) = X(t-1)\\]\n\n\nComplex exponentials \\(e_\\lambda(t) = e^{i\\lambda t}\\) are eigenfunctions of \\(\\mathcal{B}\\):\n\\[\\mathcal{B}(e_\\lambda) = e^{-i\\lambda} \\cdot e_\\lambda\\]\n\n\nSinusoids are the natural “modes” of any time-invariant linear system."
  },
  {
    "objectID": "ts-data-slides.html#the-creation-myth",
    "href": "ts-data-slides.html#the-creation-myth",
    "title": "Time Series Data",
    "section": "The Creation Myth",
    "text": "The Creation Myth\nOne way to think about stationary processes:\n\nIn the beginning: White noise—independent observations, no memory\n\n\nThen: A linear, time-invariant filter acts on the white noise\n\n\nResult: Autocorrelated output—the filter’s fingerprint\n\n\nThe spectrum tells you what the filter did.\nThe ACF tells you how the present relates to the past."
  },
  {
    "objectID": "ts-data-slides.html#example-1-sunspots",
    "href": "ts-data-slides.html#example-1-sunspots",
    "title": "Time Series Data",
    "section": "Example 1: Sunspots",
    "text": "Example 1: Sunspots\n\n\nFigure 1: Monthly sunspot numbers, 1749–present\n\nVisible structure: ~11-year solar magnetic cycle\nIrregular amplitude: Peak heights vary substantially\nWhat do the two lenses reveal?"
  },
  {
    "objectID": "ts-data-slides.html#sunspots-time-domain-view",
    "href": "ts-data-slides.html#sunspots-time-domain-view",
    "title": "Time Series Data",
    "section": "Sunspots: Time Domain View",
    "text": "Sunspots: Time Domain View\n\n\nFigure 2: ACF of monthly sunspot numbers\n\nSlow decay: Strong persistence—the past predicts the future\nOscillation: Cycle of ~11 years\nBut cycle length is hard to read precisely"
  },
  {
    "objectID": "ts-data-slides.html#sunspots-frequency-domain-view",
    "href": "ts-data-slides.html#sunspots-frequency-domain-view",
    "title": "Time Series Data",
    "section": "Sunspots: Frequency Domain View",
    "text": "Sunspots: Frequency Domain View\n\n\nFigure 3: Spectrum of monthly sunspot numbers\n\nDominant peak: The ~11-year cycle appears as a sharp spectral peak\nFrequency ≈ 0.007 cycles/month → period ≈ 140 months ≈ 11.7 years\nThe spectrum makes the periodicity immediately visible"
  },
  {
    "objectID": "ts-data-slides.html#example-2-global-temperatures",
    "href": "ts-data-slides.html#example-2-global-temperatures",
    "title": "Time Series Data",
    "section": "Example 2: Global Temperatures",
    "text": "Example 2: Global Temperatures\n\n\nFigure 4: Annual temperature deviations from 1991–2020 average\n\nDominant feature: Non-linear trend, especially post-1980\nNo obvious periodicity: Unlike sunspots\nHow do the two lenses handle trend?"
  },
  {
    "objectID": "ts-data-slides.html#global-temperatures-time-domain",
    "href": "ts-data-slides.html#global-temperatures-time-domain",
    "title": "Time Series Data",
    "section": "Global Temperatures: Time Domain",
    "text": "Global Temperatures: Time Domain\n\n\nFigure 5: ACF of global land temperatures\n\nVery slow decay: Each year strongly predicts the next\nThis signals non-stationarity—the trend dominates\nMust remove trend before standard time series analysis"
  },
  {
    "objectID": "ts-data-slides.html#global-temperatures-frequency-domain",
    "href": "ts-data-slides.html#global-temperatures-frequency-domain",
    "title": "Time Series Data",
    "section": "Global Temperatures: Frequency Domain",
    "text": "Global Temperatures: Frequency Domain\n\n\nFigure 6: Spectrum of global land temperatures\n\nLow-frequency dominance: Most variance at lowest frequencies\nThis is the spectral signature of trend\nNo periodic peaks—the story is secular change, not cycles"
  },
  {
    "objectID": "ts-data-slides.html#example-3-soi-and-fish-recruitment",
    "href": "ts-data-slides.html#example-3-soi-and-fish-recruitment",
    "title": "Time Series Data",
    "section": "Example 3: SOI and Fish Recruitment",
    "text": "Example 3: SOI and Fish Recruitment\n\n\nFigure 7: Southern Oscillation Index and Recruitment, 1950–1987\n\nTwo related series: Ocean temperature affects fish population\nMultiple periodicities: Annual cycle + El Niño (~4 years)\nPerfect case for spectral analysis"
  },
  {
    "objectID": "ts-data-slides.html#soi-dual-perspective",
    "href": "ts-data-slides.html#soi-dual-perspective",
    "title": "Time Series Data",
    "section": "SOI: Dual Perspective",
    "text": "SOI: Dual Perspective\n\n\nFigure 8: SOI: ACF and Spectrum\n\nACF: Oscillates, showing cyclical structure, but which cycles?\nSpectrum: Two clear peaks—annual (freq ≈ 1) and El Niño (freq ≈ 0.25)\nThe frequency domain separates the periodicities"
  },
  {
    "objectID": "ts-data-slides.html#recruitment-dual-perspective",
    "href": "ts-data-slides.html#recruitment-dual-perspective",
    "title": "Time Series Data",
    "section": "Recruitment: Dual Perspective",
    "text": "Recruitment: Dual Perspective\n\n\nFigure 9: Recruitment: ACF and Spectrum\n\nSimilar spectral peaks to SOI—the fish respond to the climate signal\nThis correspondence leads to coherence analysis (Chapter 14)\nUnderstanding the mechanism enables better forecasting"
  },
  {
    "objectID": "ts-data-slides.html#white-noise-the-blank-slate",
    "href": "ts-data-slides.html#white-noise-the-blank-slate",
    "title": "Time Series Data",
    "section": "White Noise: The Blank Slate",
    "text": "White Noise: The Blank Slate\nWhite noise is the reference point—a process with no memory:\n\nIndependent observations\nConstant variance\nZero autocorrelation at all nonzero lags\n\n\nTime domain: \\(\\rho(u) = 0\\) for all \\(u \\neq 0\\)\nFrequency domain: Flat spectrum—equal variance at all frequencies"
  },
  {
    "objectID": "ts-data-slides.html#white-noise-visual-signature",
    "href": "ts-data-slides.html#white-noise-visual-signature",
    "title": "Time Series Data",
    "section": "White Noise: Visual Signature",
    "text": "White Noise: Visual Signature\n\n\nBandwidth: 0.072 | Degrees of Freedom: 72.12 | split taper: 0% \n\n\n\n\nFigure 10: White noise: time series, ACF, and spectrum\n\nTime series: No visible pattern\nACF: All lags near zero (within confidence bands)\nSpectrum: Flat (within confidence bands)—no frequency dominates"
  },
  {
    "objectID": "ts-data-slides.html#departures-from-white-noise",
    "href": "ts-data-slides.html#departures-from-white-noise",
    "title": "Time Series Data",
    "section": "Departures from White Noise",
    "text": "Departures from White Noise\nAny structure in ACF or spectrum reveals departure from independence:\n\n\n\nObservation\nTime Domain\nFrequency Domain\n\n\n\n\nSlow ACF decay\nPersistence/trend\nLow-freq dominance\n\n\nACF oscillation\nCyclical dependence\nSpectral peaks\n\n\nSharp ACF cutoff\nShort memory (MA)\nSmooth spectrum\n\n\nGradual ACF decay\nLong memory (AR)\nPeaked spectrum\n\n\n\n\nThe two views are complementary diagnostics."
  },
  {
    "objectID": "ts-data-slides.html#stationarity-defined",
    "href": "ts-data-slides.html#stationarity-defined",
    "title": "Time Series Data",
    "section": "Stationarity Defined",
    "text": "Stationarity Defined\nA process is second-order stationary (weakly stationary) if:\n\n\\(E\\{X(t)\\} = \\mu\\) (constant mean)\n\\(\\text{Cov}(X(t+u), X(t)) = \\gamma(u)\\) (depends only on lag \\(u\\))\n\n\nWhy it matters:\n\nStationarity lets us estimate \\(\\gamma(u)\\) from a single realization\nThe spectrum is only defined for stationary processes\nNon-stationary series must be transformed first"
  },
  {
    "objectID": "ts-data-slides.html#stationarity-and-time-invariance-1",
    "href": "ts-data-slides.html#stationarity-and-time-invariance-1",
    "title": "Time Series Data",
    "section": "Stationarity and Time-Invariance",
    "text": "Stationarity and Time-Invariance\nStationarity is the statistical counterpart of time-invariance in systems theory.\n\nA linear filter is time-invariant if shifting the input shifts the output by the same amount—the filter doesn’t care what time it is.\n\n\nStationary processes are precisely those whose statistical properties don’t care what time it is.\n\n\nThis connection is why Fourier methods—natural for time-invariant systems—are fundamental for stationary processes."
  },
  {
    "objectID": "ts-data-slides.html#stationarity-visual-assessment",
    "href": "ts-data-slides.html#stationarity-visual-assessment",
    "title": "Time Series Data",
    "section": "Stationarity: Visual Assessment",
    "text": "Stationarity: Visual Assessment\n\n\nFigure 11: Stationary vs. non-stationary\n\nLeft: Fluctuates around a constant level—stationary\nRight: Wanders without returning—non-stationary"
  },
  {
    "objectID": "ts-data-slides.html#achieving-stationarity",
    "href": "ts-data-slides.html#achieving-stationarity",
    "title": "Time Series Data",
    "section": "Achieving Stationarity",
    "text": "Achieving Stationarity\nCommon transformations for non-stationary data:\n\n\n\nProblem\nTransformation\n\n\n\n\nTrend in mean\nDifferencing: \\(Y(t) = X(t) - X(t-1)\\)\n\n\nExponential growth\nLog transform, then difference\n\n\nChanging variance\nLog or Box-Cox transform\n\n\nSeasonality\nSeasonal differencing\n\n\n\n\nAfter transformation, check that ACF decays and spectrum is not dominated by lowest frequencies."
  },
  {
    "objectID": "ts-data-slides.html#the-autocorrelation-function",
    "href": "ts-data-slides.html#the-autocorrelation-function",
    "title": "Time Series Data",
    "section": "The Autocorrelation Function",
    "text": "The Autocorrelation Function\n\\[\\rho_X(u) = \\frac{\\gamma_X(u)}{\\gamma_X(0)} = \\text{Corr}(X(t+u), X(t))\\]\n\nProperties:\n\n\\(\\rho_X(0) = 1\\) (perfect self-correlation)\n\\(|\\rho_X(u)| \\leq 1\\) (it’s a correlation)\n\\(\\rho_X(-u) = \\rho_X(u)\\) (symmetric in lag)\n\n\n\nThe ACF answers: How predictable is the future from the past?"
  },
  {
    "objectID": "ts-data-slides.html#sample-acf",
    "href": "ts-data-slides.html#sample-acf",
    "title": "Time Series Data",
    "section": "Sample ACF",
    "text": "Sample ACF\n\n\nFigure 12: Sample ACF for SOI data\n\nBlue dashed lines: 95% confidence bounds under white noise null\nValues outside bounds → significant autocorrelation\nPattern of decay/oscillation suggests model structure"
  },
  {
    "objectID": "ts-data-slides.html#partial-autocorrelation-function-pacf",
    "href": "ts-data-slides.html#partial-autocorrelation-function-pacf",
    "title": "Time Series Data",
    "section": "Partial Autocorrelation Function (PACF)",
    "text": "Partial Autocorrelation Function (PACF)\nThe PACF measures correlation between \\(X(t+u)\\) and \\(X(t)\\) after removing the linear effect of intervening values.\n\n\\[\\phi_{uu} = \\text{Corr}(X(t+u) - \\hat{X}(t+u), \\; X(t) - \\hat{X}(t))\\]\n\n\nThe PACF answers: What is the direct effect of lag \\(u\\), controlling for shorter lags?"
  },
  {
    "objectID": "ts-data-slides.html#acf-and-pacf-together",
    "href": "ts-data-slides.html#acf-and-pacf-together",
    "title": "Time Series Data",
    "section": "ACF and PACF Together",
    "text": "ACF and PACF Together\n\n\nFigure 13: ACF and PACF for SOI data\n\nACF: Total correlation at each lag\nPACF: Direct effect at each lag, controlling for others\nTogether they suggest model structure (Chapter 13)"
  },
  {
    "objectID": "ts-data-slides.html#the-variance-problem",
    "href": "ts-data-slides.html#the-variance-problem",
    "title": "Time Series Data",
    "section": "The Variance Problem",
    "text": "The Variance Problem\nSuppose we estimate the mean \\(\\mu_X\\) from \\(T\\) observations.\nIf independent: \\[\\text{Var}(\\bar{X}) = \\frac{\\sigma_X^2}{T}\\]\n\nIf autocorrelated: \\[\\text{Var}(\\bar{X}) = \\frac{\\sigma_X^2}{T} \\left(1 + 2\\sum_{u=1}^{T-1}\\left(1 - \\frac{u}{T}\\right)\\rho_X(u)\\right)\\]\n\n\nIgnoring autocorrelation gives wrong standard errors.\nThis is a failure of understanding producing a failure of inference."
  },
  {
    "objectID": "ts-data-slides.html#effective-sample-size",
    "href": "ts-data-slides.html#effective-sample-size",
    "title": "Time Series Data",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\nQuestion: How many independent observations would give the same variance?\n\\[N_{\\text{eff}} = \\frac{T}{\\displaystyle 1 + 2\\sum_{u=1}^{T-1}\\left(1 - \\frac{u}{T}\\right)\\rho_X(u)}\\]\n\nExample: If \\(\\rho_X(1) = 0.8\\) and higher lags decay geometrically…\n\n\\(T = 100\\) observations\n\\(N_{\\text{eff}} \\approx 11\\) effective observations\n\nYour sample is 9× smaller than it appears!"
  },
  {
    "objectID": "ts-data-slides.html#practical-consequences",
    "href": "ts-data-slides.html#practical-consequences",
    "title": "Time Series Data",
    "section": "Practical Consequences",
    "text": "Practical Consequences\n\n\n\nIf you ignore autocorrelation…\nConsequence\n\n\n\n\nConfidence intervals\nToo narrow\n\n\nHypothesis tests\nToo many false positives\n\n\nStandard errors\nUnderestimated\n\n\nCross-validation\nTraining/test not independent\n\n\n\n\nThe lesson: Understanding the dependence structure is essential for valid inference.\nThis is not merely academic—it affects real decisions."
  },
  {
    "objectID": "ts-data-slides.html#two-chapters-two-emphases",
    "href": "ts-data-slides.html#two-chapters-two-emphases",
    "title": "Time Series Data",
    "section": "Two Chapters, Two Emphases",
    "text": "Two Chapters, Two Emphases\nChapter 13: Time Domain Methods\n\nARIMA models: AR, MA, and their combinations\nForecasting and prediction intervals\nEmphasis: predicting the future from the past\n\n\nChapter 14: Frequency Domain Methods\n\nPeriodogram and spectral density estimation\nCoherence between series\nEmphasis: understanding periodic structure\n\n\n\nBoth perspectives are needed for complete understanding."
  },
  {
    "objectID": "ts-data-slides.html#the-dual-aims-revisited",
    "href": "ts-data-slides.html#the-dual-aims-revisited",
    "title": "Time Series Data",
    "section": "The Dual Aims, Revisited",
    "text": "The Dual Aims, Revisited\nTime series analysis embodies the dual aims of data analysis:\n\nDecision support: Forecast tomorrow’s value, next quarter’s earnings, next year’s climate\n\n\nScientific understanding: What periodic phenomena drive the system? What filtering has occurred? What is the physics?\n\n\nEffective forecasting rests on understanding; understanding is tested by predictive success.\nThe two aims are inseparable."
  },
  {
    "objectID": "ts-data-slides.html#key-insights",
    "href": "ts-data-slides.html#key-insights",
    "title": "Time Series Data",
    "section": "Key Insights",
    "text": "Key Insights\n\nTime series have memory—the present depends on the past\nTwo equivalent descriptions: ACF (time domain) and spectrum (frequency domain)\nFourier analysis is fundamental because sinusoids are eigenfunctions of time-shift\nStationarity is the statistical counterpart of time-invariance\nIgnoring structure invalidates inference—ESS reveals your true sample size\nUnderstanding and prediction are intertwined—the dual aims made precise"
  },
  {
    "objectID": "ts-data-slides.html#diagnostic-checklist",
    "href": "ts-data-slides.html#diagnostic-checklist",
    "title": "Time Series Data",
    "section": "Diagnostic Checklist",
    "text": "Diagnostic Checklist\nWhen you encounter a new time series:\n\nPlot the series: Look for trend, seasonality, level shifts, changing variance\nCheck stationarity: Does \\(X\\) wander? Transform as needed\nExamine the ACF: How fast does it decay? Any oscillation?\nExamine the spectrum: Any dominant peaks? Low-frequency dominance?\nConsider ESS: How much independent information do you really have?\nAsk both questions: What predicts the future? What structure is present?"
  },
  {
    "objectID": "ts-data-slides.html#team-exercise-1-explore-an-astsa-dataset",
    "href": "ts-data-slides.html#team-exercise-1-explore-an-astsa-dataset",
    "title": "Time Series Data",
    "section": "Team Exercise 1: Explore an astsa Dataset",
    "text": "Team Exercise 1: Explore an astsa Dataset\nChoose a time series from astsa not discussed today (e.g., nyse, oil, prodn):\n\nPlot the series. What features do you observe?\nPlot the ACF. Is there evidence of autocorrelation?\nDoes the series appear stationary? What would you do if not?"
  },
  {
    "objectID": "ts-data-slides.html#team-exercise-2-simulate-white-noise",
    "href": "ts-data-slides.html#team-exercise-2-simulate-white-noise",
    "title": "Time Series Data",
    "section": "Team Exercise 2: Simulate White Noise",
    "text": "Team Exercise 2: Simulate White Noise\nGenerate 500 observations of Gaussian white noise using rnorm():\n\nPlot the series, ACF, and spectrum.\nHow do they compare to theoretical signatures (flat spectrum, ACF = 0 at all lags)?\nRepeat several times. How much sampling variation do you see?"
  },
  {
    "objectID": "ts-data-slides.html#team-exercise-3-effective-sample-size",
    "href": "ts-data-slides.html#team-exercise-3-effective-sample-size",
    "title": "Time Series Data",
    "section": "Team Exercise 3: Effective Sample Size",
    "text": "Team Exercise 3: Effective Sample Size\nFor an AR(1) process, \\(ESS \\approx T \\cdot (1 - \\phi) / (1 + \\phi)\\).\n\nCalculate ESS for \\(T = 200\\) when \\(\\phi = 0.5\\), \\(0.8\\), \\(0.95\\).\nWhat happens as \\(\\phi \\to 1\\)?\nYou have \\(T = 100\\) observations with estimated \\(\\phi = 0.7\\). How wide should your confidence interval for the mean be, compared to the naive interval?\n\n\nExercise 1 develops pattern recognition. Exercise 2 establishes the baseline. Exercise 3 is critical for inference."
  },
  {
    "objectID": "ts-data-slides.html#discussion-questions",
    "href": "ts-data-slides.html#discussion-questions",
    "title": "Time Series Data",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nYou fit a regression and the residuals have ACF(1) = 0.4. What are the implications?\nWhen is first differencing appropriate? When might it remove too much signal?\n“The spectrum and ACF contain the same information.” True in theory—why might one be more useful in practice?"
  },
  {
    "objectID": "text-as-data-slides.html#the-investment-analysts-problem",
    "href": "text-as-data-slides.html#the-investment-analysts-problem",
    "title": "Text as Data",
    "section": "The Investment Analyst’s Problem",
    "text": "The Investment Analyst’s Problem\nYou work at an investment firm. Your task:\n\n\nReview 500 quarterly reports and analyst reactions from companies in your portfolio. Identify emerging themes. Flag concerns. Recommend adjustments.\n\n\n\nYou cannot read 500 reports carefully. What do you do?"
  },
  {
    "objectID": "text-as-data-slides.html#questions-about-text",
    "href": "text-as-data-slides.html#questions-about-text",
    "title": "Text as Data",
    "section": "Questions About Text",
    "text": "Questions About Text\nGiven a collection of documents (a corpus), we might ask:\n\n\nWhat words distinguish this document? (distinctive terms)\n\n\n\n\nWhat themes appear across the corpus? (topic discovery)\n\n\n\n\nHow similar are two documents? (document comparison)\n\n\n\n\nHow has language changed over time? (temporal patterns)\n\n\n\n\nWhat is this document about? (summarization, classification)"
  },
  {
    "objectID": "text-as-data-slides.html#the-classical-era-1990s2010s",
    "href": "text-as-data-slides.html#the-classical-era-1990s2010s",
    "title": "Text as Data",
    "section": "The Classical Era (1990s–2010s)",
    "text": "The Classical Era (1990s–2010s)\nStatistical text analysis:\n\nWord counts, tf-idf, topic models\nFast, interpretable, scalable\nComputers “measure” text; humans interpret\n\n\nThis chapter focuses on these foundational methods."
  },
  {
    "objectID": "text-as-data-slides.html#the-llm-era-2020s",
    "href": "text-as-data-slides.html#the-llm-era-2020s",
    "title": "Text as Data",
    "section": "The LLM Era (2020s–)",
    "text": "The LLM Era (2020s–)\nLarge Language Models:\n\nDirect summarization, Q&A, classification\nFlexible, expensive, less transparent\nComputers “understand” text (or appear to)\n\n\n\n\n\n\n\n\nThe Goal of This Chapter\n\n\nBuild intuition about text-as-data so you can ask better questions and evaluate answers critically—whether from classical methods or LLMs."
  },
  {
    "objectID": "text-as-data-slides.html#from-text-to-numbers",
    "href": "text-as-data-slides.html#from-text-to-numbers",
    "title": "Text as Data",
    "section": "From Text to Numbers",
    "text": "From Text to Numbers\nTo analyze text computationally, we must convert it to numbers.\n\nStep 1: Tokenization — Break text into units (usually words)\nStep 2: Vocabulary — Identify the set of unique terms\nStep 3: Counting — Record which terms appear in which documents\n\n\nThe result: a term-document matrix (or document-term matrix)."
  },
  {
    "objectID": "text-as-data-slides.html#example-macbeths-soliloquy",
    "href": "text-as-data-slides.html#example-macbeths-soliloquy",
    "title": "Text as Data",
    "section": "Example: Macbeth’s Soliloquy",
    "text": "Example: Macbeth’s Soliloquy\n\n\nCode\nmacbeth_tokens &lt;- macbeth |&gt;\n  tidytext::unnest_tokens(word, text)\n\nmacbeth_tokens |&gt; head(6)\n\n\n# A tibble: 6 × 2\n  line_id word   \n    &lt;int&gt; &lt;chr&gt;  \n1       1 life's \n2       1 but    \n3       1 a      \n4       1 walking\n5       1 shadow \n6       1 a      \n\n\n\nThe soliloquy yields 38 word tokens in total."
  },
  {
    "objectID": "text-as-data-slides.html#the-stop-words-problem",
    "href": "text-as-data-slides.html#the-stop-words-problem",
    "title": "Text as Data",
    "section": "The Stop Words Problem",
    "text": "The Stop Words Problem\nMost frequent words:\n\n\n\n\n\nword\nn\n\n\n\n\na\n3\n\n\nand\n3\n\n\nis\n2\n\n\nan\n1\n\n\nbut\n1\n\n\nby\n1\n\n\nfrets\n1\n\n\nfull\n1\n\n\n\n\n\n\n“A” and “is” tell us nothing about Macbeth. These are stop words—common words that carry little information."
  },
  {
    "objectID": "text-as-data-slides.html#after-removing-stop-words",
    "href": "text-as-data-slides.html#after-removing-stop-words",
    "title": "Text as Data",
    "section": "After Removing Stop Words",
    "text": "After Removing Stop Words\n\n\nCode\nmacbeth_meaningful &lt;- macbeth_tokens |&gt;\n  dplyr::anti_join(tidytext::stop_words, by = \"word\") |&gt;\n  dplyr::count(word, sort = TRUE)\n\nmacbeth_meaningful |&gt; head(6) |&gt; knitr::kable()\n\n\n\n\n\nword\nn\n\n\n\n\nfrets\n1\n\n\nfury\n1\n\n\nheard\n1\n\n\nhour\n1\n\n\nidiot\n1\n\n\nlife’s\n1\n\n\n\n\n\n\nNow we see content words: shadow, player, stage, tale, sound, fury…"
  },
  {
    "objectID": "text-as-data-slides.html#the-term-document-matrix",
    "href": "text-as-data-slides.html#the-term-document-matrix",
    "title": "Text as Data",
    "section": "The Term-Document Matrix",
    "text": "The Term-Document Matrix\nFor a corpus of \\(D\\) documents with vocabulary of \\(V\\) terms:\n\n\\[\n\\underset{V \\times D}{\\mathbf{M}} = \\begin{pmatrix}\nm_{1,1} & m_{1,2} & \\cdots & m_{1,D} \\\\\nm_{2,1} & m_{2,2} & \\cdots & m_{2,D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nm_{V,1} & m_{V,2} & \\cdots & m_{V,D}\n\\end{pmatrix}\n\\]\n\n\nwhere \\(m_{v,d}\\) = count of term \\(v\\) in document \\(d\\).\n\n\nKey property: This matrix is extremely sparse. Most entries are zero."
  },
  {
    "objectID": "text-as-data-slides.html#connection-to-part-2",
    "href": "text-as-data-slides.html#connection-to-part-2",
    "title": "Text as Data",
    "section": "Connection to Part 2",
    "text": "Connection to Part 2\n\n\n\n\n\n\nText is Just Another Feature Matrix\n\n\nThe term-document matrix is an \\(n \\times d\\) matrix where:\n\nEach document is an observation (row)\nEach term is a feature (column)\nThe “curse of dimensionality” applies—\\(d\\) can be tens of thousands\n\n\n\n\n\nEverything we learned about high-dimensional data applies:\n\nPCA can find directions of variation\nClustering can group similar documents\nDistance measures can compare documents"
  },
  {
    "objectID": "text-as-data-slides.html#the-sparsity-challenge",
    "href": "text-as-data-slides.html#the-sparsity-challenge",
    "title": "Text as Data",
    "section": "The Sparsity Challenge",
    "text": "The Sparsity Challenge\n\nTerm-document matrices are extremely sparse\nTypical corpora: 99% zeros. Most words don’t appear in most documents."
  },
  {
    "objectID": "text-as-data-slides.html#the-problem-with-raw-counts",
    "href": "text-as-data-slides.html#the-problem-with-raw-counts",
    "title": "Text as Data",
    "section": "The Problem with Raw Counts",
    "text": "The Problem with Raw Counts\nIf we rank words by frequency across a corpus, we get:\n\nthe, and, to, of, a, in, that, is, was, for…\n\n\nThese words appear in every document. They don’t distinguish anything.\n\n\nWe need a measure that asks:\n\nWhich words are distinctive to this document?"
  },
  {
    "objectID": "text-as-data-slides.html#term-frequency-tf",
    "href": "text-as-data-slides.html#term-frequency-tf",
    "title": "Text as Data",
    "section": "Term Frequency (TF)",
    "text": "Term Frequency (TF)\nTerm frequency measures how often a term appears in a document:\n\\[\n\\text{tf}(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total terms in document } d}\n\\]\n\nHigh tf means the term is prominent in this document.\n\n\nBut “the” has high tf in every document—not distinctive."
  },
  {
    "objectID": "text-as-data-slides.html#inverse-document-frequency-idf",
    "href": "text-as-data-slides.html#inverse-document-frequency-idf",
    "title": "Text as Data",
    "section": "Inverse Document Frequency (IDF)",
    "text": "Inverse Document Frequency (IDF)\nInverse document frequency measures how rare a term is across the corpus:\n\\[\n\\text{idf}(t) = \\log \\frac{\\text{total documents}}{\\text{documents containing term } t}\n\\]\n\n\nIf term appears in all documents: \\(\\text{idf} = \\log(1) = 0\\)\nIf term appears in one document: \\(\\text{idf} = \\log(D)\\) (large)\n\n\n\nIDF downweights common terms, upweights rare ones."
  },
  {
    "objectID": "text-as-data-slides.html#tf-idf-the-product",
    "href": "text-as-data-slides.html#tf-idf-the-product",
    "title": "Text as Data",
    "section": "TF-IDF: The Product",
    "text": "TF-IDF: The Product\n\\[\n\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)\n\\]\n\n\n\n\n\n\n\nTF-IDF Intuition\n\n\nA term has high tf-idf if it is:\n\nFrequent in this document (high tf), AND\nRare across the corpus (high idf)\n\n\n\n\n\n\nThis surfaces distinctive words—the words that make this document different."
  },
  {
    "objectID": "text-as-data-slides.html#example-jane-austens-novels",
    "href": "text-as-data-slides.html#example-jane-austens-novels",
    "title": "Text as Data",
    "section": "Example: Jane Austen’s Novels",
    "text": "Example: Jane Austen’s Novels\n\n\nFigure 1: Top tf-idf words for each Jane Austen novel"
  },
  {
    "objectID": "text-as-data-slides.html#what-tf-idf-reveals",
    "href": "text-as-data-slides.html#what-tf-idf-reveals",
    "title": "Text as Data",
    "section": "What TF-IDF Reveals",
    "text": "What TF-IDF Reveals\nEach novel’s distinctive words are character names:\n\nEmma: Emma, Harriet, Weston, Knightley\nPride and Prejudice: Darcy, Bennet, Bingley, Elizabeth\nSense and Sensibility: Elinor, Marianne, Willoughby\n\n\nTF-IDF automatically surfaces what makes each document unique.\n\n\n\n\n\n\n\n\nFor Your Investment Analyst\n\n\nTF-IDF on quarterly reports would surface: company-specific terms, unusual product names, emerging risk language…"
  },
  {
    "objectID": "text-as-data-slides.html#beyond-individual-words",
    "href": "text-as-data-slides.html#beyond-individual-words",
    "title": "Text as Data",
    "section": "Beyond Individual Words",
    "text": "Beyond Individual Words\nTF-IDF tells us which words are distinctive.\nBut words cluster into themes:\n\n\n“inflation”, “rates”, “Fed”, “monetary” → Macroeconomic concerns\n“supply”, “chain”, “shortage”, “logistics” → Supply chain issues\n“AI”, “automation”, “workforce”, “efficiency” → Technology transformation\n\n\n\nTopic models discover these latent themes automatically."
  },
  {
    "objectID": "text-as-data-slides.html#the-topic-model-idea",
    "href": "text-as-data-slides.html#the-topic-model-idea",
    "title": "Text as Data",
    "section": "The Topic Model Idea",
    "text": "The Topic Model Idea\n\n\n\n\n\n\nTwo Key Assumptions\n\n\n\nEach document is a mixture of topics\n\nDocument A: 60% finance, 30% technology, 10% politics\n\nEach topic is a mixture of words\n\n“Finance” topic: high probability for “market”, “stock”, “earnings”…\n\n\n\n\n\n\nGiven a corpus, the algorithm discovers both: - What topics exist - Which topics each document expresses"
  },
  {
    "objectID": "text-as-data-slides.html#latent-dirichlet-allocation-lda",
    "href": "text-as-data-slides.html#latent-dirichlet-allocation-lda",
    "title": "Text as Data",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\nThe most common topic model is Latent Dirichlet Allocation.\n\nYes—the other LDA! (Recall Chapter 9’s Linear Discriminant Analysis.)\n\n\nThe name:\n\nLatent: The topics are hidden; we infer them from word patterns\nDirichlet: A probability distribution used to model mixtures\nAllocation: Assigning words to topics"
  },
  {
    "objectID": "text-as-data-slides.html#lda-the-generative-story",
    "href": "text-as-data-slides.html#lda-the-generative-story",
    "title": "Text as Data",
    "section": "LDA: The Generative Story",
    "text": "LDA: The Generative Story\nImagine each document was generated as follows:\n\n\nChoose a topic mixture for this document\n\n“This document is 70% Topic A, 30% Topic B”\n\n\n\n\n\nFor each word position:\n\nRoll the dice to pick a topic\nFrom that topic’s word distribution, generate a word\n\n\n\n\nLDA inverts this process: given documents, infer the topics."
  },
  {
    "objectID": "text-as-data-slides.html#example-ap-news-articles",
    "href": "text-as-data-slides.html#example-ap-news-articles",
    "title": "Text as Data",
    "section": "Example: AP News Articles",
    "text": "Example: AP News Articles\n2,246 Associated Press articles, fit with \\(K = 2\\) topics:\n\n\nFigure 2: Top words in each topic (AP articles, K=2)"
  },
  {
    "objectID": "text-as-data-slides.html#interpreting-the-topics",
    "href": "text-as-data-slides.html#interpreting-the-topics",
    "title": "Text as Data",
    "section": "Interpreting the Topics",
    "text": "Interpreting the Topics\nTopic 1: percent, million, billion, company, market…\n→ Business/Finance\n\nTopic 2: president, government, party, members, congress…\n→ Politics/Government\n\n\nThe algorithm knew nothing about these categories—it discovered them from word co-occurrence patterns."
  },
  {
    "objectID": "text-as-data-slides.html#choosing-k-the-number-of-topics",
    "href": "text-as-data-slides.html#choosing-k-the-number-of-topics",
    "title": "Text as Data",
    "section": "Choosing K: The Number of Topics",
    "text": "Choosing K: The Number of Topics\nLike \\(k\\)-means clustering, you must choose \\(K\\).\n\nToo few topics: Themes are too broad, mix unrelated content\nToo many topics: Themes are too narrow, hard to interpret\n\n\nPractical approaches:\n\nDomain knowledge: “I expect 5–10 themes in this corpus”\nCoherence metrics: Do top words in each topic make sense together?\nIterative exploration: Try several values of \\(K\\)"
  },
  {
    "objectID": "text-as-data-slides.html#document-topic-mixtures",
    "href": "text-as-data-slides.html#document-topic-mixtures",
    "title": "Text as Data",
    "section": "Document-Topic Mixtures",
    "text": "Document-Topic Mixtures\nLDA also tells you each document’s topic composition:\n\n\n\n\n\n\ndocument\nTopic 1 (Business)\nTopic 2 (Politics)\n\n\n\n\nArticle 127\n0.89\n0.11\n\n\nArticle 203\n0.23\n0.77\n\n\nArticle 891\n0.52\n0.48\n\n\n\n\n\n\n\n\nArticle 127: Mostly business\nArticle 203: Mostly politics\n\nArticle 891: Mixed coverage"
  },
  {
    "objectID": "text-as-data-slides.html#what-llms-do-well",
    "href": "text-as-data-slides.html#what-llms-do-well",
    "title": "Text as Data",
    "section": "What LLMs Do Well",
    "text": "What LLMs Do Well\nLarge Language Models excel at tasks that once required topic models:\n\n\nSummarization: “Summarize this quarterly report in 3 bullet points”\nClassification: “Is this article about politics, business, or sports?”\nTheme extraction: “What are the main concerns in these documents?”\nQuestion answering: “What did the CEO say about supply chains?”\n\n\n\nWhy learn classical methods at all?"
  },
  {
    "objectID": "text-as-data-slides.html#when-classical-methods-still-win",
    "href": "text-as-data-slides.html#when-classical-methods-still-win",
    "title": "Text as Data",
    "section": "When Classical Methods Still Win",
    "text": "When Classical Methods Still Win\n\n\n\n\n\n\nUse Classical Methods When:\n\n\n\nScale: Processing 100,000 documents with an LLM is slow and expensive\nAuditability: You need to explain exactly how conclusions were reached\nExploration: You don’t yet know what questions to ask\nQuantification: You need precise statistics (tf-idf scores, topic proportions)\n\n\n\n\n\nClassical methods are often a first pass—filter, cluster, and then use LLMs on selected subsets."
  },
  {
    "objectID": "text-as-data-slides.html#how-classical-concepts-help-with-llms",
    "href": "text-as-data-slides.html#how-classical-concepts-help-with-llms",
    "title": "Text as Data",
    "section": "How Classical Concepts Help with LLMs",
    "text": "How Classical Concepts Help with LLMs\nEven if you use LLMs for analysis, understanding text-as-data helps you:\n\n\nAsk better questions: “Which topics dominate?” is a topic model question\n\n\n\n\nEvaluate outputs critically: Is this term really distinctive? Is this theme coherent?\n\n\n\n\nRecognize failure modes: LLMs hallucinate; topic models produce incoherent topics\n\n\n\n\nDesign hybrid workflows: tf-idf to find distinctive documents → LLM to summarize them"
  },
  {
    "objectID": "text-as-data-slides.html#a-modern-workflow",
    "href": "text-as-data-slides.html#a-modern-workflow",
    "title": "Text as Data",
    "section": "A Modern Workflow",
    "text": "A Modern Workflow\n\nText EDA workflow combining classical and LLM methods"
  },
  {
    "objectID": "text-as-data-slides.html#key-takeaways",
    "href": "text-as-data-slides.html#key-takeaways",
    "title": "Text as Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nText is high-dimensional, sparse data — the term-document matrix perspective\n\n\n\nTF-IDF surfaces distinctive words — frequent here, rare elsewhere\n\n\n\n\nTopic models discover latent themes — documents as mixtures of topics\n\n\n\n\nChoose tools appropriately — classical for scale/exploration, LLMs for depth\n\n\n\n\nConcepts transfer — understanding text-as-data makes you a better LLM user"
  },
  {
    "objectID": "text-as-data-slides.html#key-formulas",
    "href": "text-as-data-slides.html#key-formulas",
    "title": "Text as Data",
    "section": "Key Formulas",
    "text": "Key Formulas\nTerm frequency: \\[\\text{tf}(t, d) = \\frac{n_{t,d}}{\\sum_t n_{t,d}}\\]\n\nInverse document frequency: \\[\\text{idf}(t) = \\log \\frac{|D|}{|\\{d : t \\in d\\}|}\\]\n\n\nTF-IDF: \\[\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)\\]"
  },
  {
    "objectID": "text-as-data-slides.html#connections-to-earlier-material",
    "href": "text-as-data-slides.html#connections-to-earlier-material",
    "title": "Text as Data",
    "section": "Connections to Earlier Material",
    "text": "Connections to Earlier Material\n\n\n\nPart 1–2 Concept\nText Application\n\n\n\n\nFeature matrix\nTerm-document matrix\n\n\nCurse of dimensionality\nVocabulary size (10,000+ terms)\n\n\nPCA\nTopic models reduce dimensions\n\n\nClustering\nDocuments cluster by theme\n\n\nInformation theory\nIDF relates to entropy/surprisal"
  },
  {
    "objectID": "text-as-data-slides.html#the-two-ldas",
    "href": "text-as-data-slides.html#the-two-ldas",
    "title": "Text as Data",
    "section": "The Two LDAs",
    "text": "The Two LDAs\n\n\n\n\n\n\nA Name Collision Resolved\n\n\nChapter 9: Linear Discriminant Analysis — supervised, finds directions that separate classes\nChapter 10: Latent Dirichlet Allocation — unsupervised, finds topics in text\nBoth involve “allocation” to categories. Context makes the meaning clear."
  },
  {
    "objectID": "text-as-data-slides.html#team-exercise-1-tf-idf-interpretation",
    "href": "text-as-data-slides.html#team-exercise-1-tf-idf-interpretation",
    "title": "Text as Data",
    "section": "Team Exercise 1: TF-IDF Interpretation",
    "text": "Team Exercise 1: TF-IDF Interpretation\nUsing the Jane Austen tf-idf results:\n\nWhy are character names the most distinctive words?\nWhat would happen to tf-idf scores if we combined all six novels into one document?\nHow might you modify tf-idf to find distinctive non-name words?"
  },
  {
    "objectID": "text-as-data-slides.html#team-exercise-2-topic-model-exploration",
    "href": "text-as-data-slides.html#team-exercise-2-topic-model-exploration",
    "title": "Text as Data",
    "section": "Team Exercise 2: Topic Model Exploration",
    "text": "Team Exercise 2: Topic Model Exploration\nFor the AP news articles:\n\nWhat would you expect to see with \\(K = 4\\) topics instead of \\(K = 2\\)?\nHow would you validate whether the discovered topics are “good”?\nPropose a metric for topic coherence."
  },
  {
    "objectID": "text-as-data-slides.html#team-exercise-3-classical-vs-llm",
    "href": "text-as-data-slides.html#team-exercise-3-classical-vs-llm",
    "title": "Text as Data",
    "section": "Team Exercise 3: Classical vs LLM",
    "text": "Team Exercise 3: Classical vs LLM\nYour firm has 10,000 customer reviews of a new product.\n\nDesign a workflow using only classical methods.\nDesign a workflow using only an LLM.\nDesign a hybrid workflow. Which approach would you recommend and why?"
  },
  {
    "objectID": "text-as-data-slides.html#team-exercise-4-investment-analyst-revisited",
    "href": "text-as-data-slides.html#team-exercise-4-investment-analyst-revisited",
    "title": "Text as Data",
    "section": "Team Exercise 4: Investment Analyst Revisited",
    "text": "Team Exercise 4: Investment Analyst Revisited\nReturn to the quarterly report scenario:\n\nWhat tf-idf patterns might indicate risk? Opportunity?\nHow would you track themes over time (Q1 → Q2 → Q3)?\nWhat questions would you ask an LLM after classical exploration?"
  },
  {
    "objectID": "text-as-data-slides.html#references",
    "href": "text-as-data-slides.html#references",
    "title": "Text as Data",
    "section": "References",
    "text": "References\n\nText Mining with R: A Tidy Approach — Silge & Robinson\nLatent Dirichlet Allocation — Blei, Ng, & Jordan (2003)\ntidytext package — R tools for tidy text analysis\nCRAN Task View: Natural Language Processing"
  },
  {
    "objectID": "text-as-data-slides.html#r-functions-reference",
    "href": "text-as-data-slides.html#r-functions-reference",
    "title": "Text as Data",
    "section": "R Functions Reference",
    "text": "R Functions Reference\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nunnest_tokens()\ntidytext\nTokenize text\n\n\nbind_tf_idf()\ntidytext\nCompute tf-idf\n\n\nLDA()\ntopicmodels\nFit topic model\n\n\ntidy()\ntidytext\nConvert to tidy format\n\n\ndfm()\nquanteda\nDocument-feature matrix"
  },
  {
    "objectID": "simulation-slides.html#the-central-question",
    "href": "simulation-slides.html#the-central-question",
    "title": "Statistical Simulation",
    "section": "The Central Question",
    "text": "The Central Question\nHow might our plans, methods, model \\(\\ldots\\) fail?\n\nSimulation lets us explore scenarios we can’t observe directly.\n\n\nGenerate data from known processes → see how methods behave."
  },
  {
    "objectID": "simulation-slides.html#uses-of-simulation",
    "href": "simulation-slides.html#uses-of-simulation",
    "title": "Statistical Simulation",
    "section": "Uses of Simulation",
    "text": "Uses of Simulation\n\nCompare methods: Which estimator performs better?\n\n\n\nAssess robustness: What happens with outliers? Small samples?\n\n\n\n\nUnderstand systems: How does randomness propagate?\n\n\n\n\nQuantify uncertainty: How precise are our estimates?"
  },
  {
    "objectID": "simulation-slides.html#the-eda-connection",
    "href": "simulation-slides.html#the-eda-connection",
    "title": "Statistical Simulation",
    "section": "The EDA Connection",
    "text": "The EDA Connection\nSimulation extends EDA beyond the data we have.\n\n\n“What would happen if \\(\\dots\\)?”\n\n\n\n\n\\(\\dots\\) the sample size were larger?\n\\(\\dots\\) the distribution had heavier tails?\n\\(\\dots\\) there were outliers?\n\n\n\nSimulation lets us explore failure modes before they occur in practice."
  },
  {
    "objectID": "simulation-slides.html#rs-distribution-functions",
    "href": "simulation-slides.html#rs-distribution-functions",
    "title": "Statistical Simulation",
    "section": "R’s Distribution Functions",
    "text": "R’s Distribution Functions\nFor each distribution, R provides four functions:\n\n\n\n\nPrefix\nFunction\nExample\n\n\n\n\nd\ndensity/mass\ndnorm(0) → 0.399\n\n\np\ncumulative probability\npnorm(1.96) → 0.975\n\n\nq\nquantile (inverse CDF)\nqnorm(0.975) → 1.96\n\n\nr\nrandom generation\nrnorm(10) → 10 values"
  },
  {
    "objectID": "simulation-slides.html#continuous-distributions-selected",
    "href": "simulation-slides.html#continuous-distributions-selected",
    "title": "Statistical Simulation",
    "section": "Continuous Distributions (selected)",
    "text": "Continuous Distributions (selected)\n\n\n\n\nTable 1\n\n\n\n\n\n\nFunction\nDistribution\n\n\n\n\n[d,p,q,r]norm\nNormal\n\n\n[d,p,q,r]unif\nUniform\n\n\n[d,p,q,r]exp\nExponential\n\n\n[d,p,q,r]gamma\nGamma\n\n\n[d,p,q,r]beta\nBeta\n\n\n[d,p,q,r]t\nStudent t\n\n\n[d,p,q,r]chisq\nChi-squared\n\n\n\n\n\n\n\n\nSee ?Distributions for a complete list."
  },
  {
    "objectID": "simulation-slides.html#discrete-distributions-selected",
    "href": "simulation-slides.html#discrete-distributions-selected",
    "title": "Statistical Simulation",
    "section": "Discrete Distributions (selected)",
    "text": "Discrete Distributions (selected)\n\n\n\n\nTable 2\n\n\n\n\n\n\nFunction\nDistribution\n\n\n\n\n[d,p,q,r]binom\nBinomial\n\n\n[d,p,q,r]pois\nPoisson\n\n\n[d,p,q,r]geom\nGeometric\n\n\n[d,p,q,r]nbinom\nNegative Binomial\n\n\n\n\n\n\n\n\n\nThe Poisson is the “law of rare events”—the limit of Binomial\\((n, p)\\) as \\(n \\to \\infty\\) and \\(p \\to 0\\) with \\(np \\to \\lambda\\)."
  },
  {
    "objectID": "simulation-slides.html#generating-random-samples",
    "href": "simulation-slides.html#generating-random-samples",
    "title": "Statistical Simulation",
    "section": "Generating Random Samples",
    "text": "Generating Random Samples\n\n\nCode\n# 100 standard normal values\nx &lt;- rnorm(n = 100, mean = 0, sd = 1)\n\n# 100 uniform values on [0, 1]\nu &lt;- runif(n = 100, min = 0, max = 1)\n\n# 100 Poisson values with mean 5\ncounts &lt;- rpois(n = 100, lambda = 5)\n\n\n\nKey insight: We specify the distribution; R generates samples."
  },
  {
    "objectID": "simulation-slides.html#mean-vs.-median",
    "href": "simulation-slides.html#mean-vs.-median",
    "title": "Statistical Simulation",
    "section": "Mean vs. Median",
    "text": "Mean vs. Median\nBoth estimate the center of a symmetric distribution.\n\nMean: \\(\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i\\)\n\n\nMedian: \\(\\hat{m}\\) = middle value (or average of two middle values)\n\n\nWhich is better? It depends on the distribution."
  },
  {
    "objectID": "simulation-slides.html#the-efficiency-question",
    "href": "simulation-slides.html#the-efficiency-question",
    "title": "Statistical Simulation",
    "section": "The Efficiency Question",
    "text": "The Efficiency Question\nFor normal data, theory tells us:\n\\[\\frac{\\text{Var}(\\hat{m})}{\\text{Var}(\\hat{\\mu})} \\approx \\frac{\\pi}{2} \\approx 1.57\\]\n\nThe median has 57% higher variance than the mean.\n\n\nCan we verify this by simulation?"
  },
  {
    "objectID": "simulation-slides.html#simulation-design",
    "href": "simulation-slides.html#simulation-design",
    "title": "Statistical Simulation",
    "section": "Simulation Design",
    "text": "Simulation Design\n\n\nCode\n# Parameters\nn &lt;- 30      # sample size\nR &lt;- 10000   # number of replications\n\n# Storage\nmeans &lt;- numeric(R)\nmedians &lt;- numeric(R)\n\n# Simulation loop\nfor (r in 1:R) {\n  x &lt;- rnorm(n)\n  means[r] &lt;- mean(x)\n  medians[r] &lt;- median(x)\n}\n\n# Compare variances\nvar(medians) / var(means)"
  },
  {
    "objectID": "simulation-slides.html#simulation-results",
    "href": "simulation-slides.html#simulation-results",
    "title": "Statistical Simulation",
    "section": "Simulation Results",
    "text": "Simulation Results\n\n\nFigure 1: Sampling distributions of mean and median (n=30, R=10,000)"
  },
  {
    "objectID": "simulation-slides.html#verifying-theory",
    "href": "simulation-slides.html#verifying-theory",
    "title": "Statistical Simulation",
    "section": "Verifying Theory",
    "text": "Verifying Theory\n\n\n\n\nTable 3\n\n\n\n\n\n\nEstimator\nVariance\nStd Error\n\n\n\n\nMean\n0.0338\n0.1837\n\n\nMedian\n0.0512\n0.2263\n\n\n\n\n\n\n\n\n\nVariance ratio: 1.517 (theory: \\(\\pi/2 \\approx\\) 1.571)\n\n\nSimulation confirms the theoretical result."
  },
  {
    "objectID": "simulation-slides.html#why-this-matters",
    "href": "simulation-slides.html#why-this-matters",
    "title": "Statistical Simulation",
    "section": "Why This Matters",
    "text": "Why This Matters\nWe verified a known result. But simulation shines when:\n\n\nTheory is intractable\nAssumptions are violated\nWe need to compare many methods\n\n\n\nEDA application: “How would my estimator behave if the data were slightly different?”"
  },
  {
    "objectID": "simulation-slides.html#estimating-π",
    "href": "simulation-slides.html#estimating-π",
    "title": "Statistical Simulation",
    "section": "Estimating π",
    "text": "Estimating π\nClassic example: estimate \\(\\pi\\) by random sampling.\n\nIdea: A quarter circle of radius 1 has area \\(\\pi/4\\).\n\n\n\nGenerate random points \\((X, Y)\\) uniform on \\([0,1]^2\\)\nCount points inside the quarter circle: \\(X^2 + Y^2 &lt; 1\\)\nProportion inside \\(\\approx \\pi/4\\)"
  },
  {
    "objectID": "simulation-slides.html#visual-intuition",
    "href": "simulation-slides.html#visual-intuition",
    "title": "Statistical Simulation",
    "section": "Visual Intuition",
    "text": "Visual Intuition\n\n\nFigure 2: Monte Carlo estimation of π"
  },
  {
    "objectID": "simulation-slides.html#the-monte-carlo-principle",
    "href": "simulation-slides.html#the-monte-carlo-principle",
    "title": "Statistical Simulation",
    "section": "The Monte Carlo Principle",
    "text": "The Monte Carlo Principle\nTo estimate \\(E[g(X)]\\):\n\\[\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n g(X_i)\\]\n\nwhere \\(X_1, \\ldots, X_n\\) are random samples from the distribution of \\(X\\).\n\n\nLaw of Large Numbers: \\(\\hat{\\theta} \\to E[g(X)]\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "simulation-slides.html#convergence-rate",
    "href": "simulation-slides.html#convergence-rate",
    "title": "Statistical Simulation",
    "section": "Convergence Rate",
    "text": "Convergence Rate\n\n\nFigure 3: Monte Carlo estimate converges to true value\n\nStandard error decreases as \\(1/\\sqrt{n}\\)."
  },
  {
    "objectID": "simulation-slides.html#the-problem",
    "href": "simulation-slides.html#the-problem",
    "title": "Statistical Simulation",
    "section": "The Problem",
    "text": "The Problem\nWhat if we want to estimate \\(P(Z &gt; 6)\\) where \\(Z \\sim N(0,1)\\)?\n\n\n\nCode\n# True probability\npnorm(6, lower.tail = FALSE)\n\n\n[1] 9.865876e-10\n\n\n\n\nAbout 1 in a billion. Naive simulation won’t work."
  },
  {
    "objectID": "simulation-slides.html#naive-approach-fails",
    "href": "simulation-slides.html#naive-approach-fails",
    "title": "Statistical Simulation",
    "section": "Naive Approach Fails",
    "text": "Naive Approach Fails\n\n\nCode\n# Generate 1 million standard normals\nz &lt;- rnorm(1e6)\n\n# Count how many exceed 6\nsum(z &gt; 6)\n\n\n[1] 0\n\n\n\nWe’d need billions of samples to see even one event."
  },
  {
    "objectID": "simulation-slides.html#importance-sampling",
    "href": "simulation-slides.html#importance-sampling",
    "title": "Statistical Simulation",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nKey idea: Sample from a different distribution, then reweight.\n\nInstead of sampling from \\(p(x)\\), sample from \\(q(x)\\) where rare events are common.\n\n\n\\[E_p[g(X)] = E_q\\left[g(X) \\cdot \\frac{p(X)}{q(X)}\\right]\\]\n\n\nThe ratio \\(w(x) = p(x)/q(x)\\) is the importance weight."
  },
  {
    "objectID": "simulation-slides.html#importance-sampling-for-pz-6",
    "href": "simulation-slides.html#importance-sampling-for-pz-6",
    "title": "Statistical Simulation",
    "section": "Importance Sampling for \\(P(Z > 6)\\)",
    "text": "Importance Sampling for \\(P(Z &gt; 6)\\)\nStrategy: Shift the normal distribution to center it near 6.\n\nLet \\(q(x) = \\phi(x - 6)\\) (normal centered at 6).\n\n\n\n\nCode\n# Sample from shifted normal\nn &lt;- 10000\nx &lt;- rnorm(n, mean = 6, sd = 1)\n\n# Importance weights: p(x) / q(x)\nlog_weights &lt;- dnorm(x, log = TRUE) - dnorm(x, mean = 6, log = TRUE)\nweights &lt;- exp(log_weights)\n\n# Estimate P(Z &gt; 6)\nestimate &lt;- mean((x &gt; 6) * weights)\nestimate\n\n\n[1] 9.91751e-10"
  },
  {
    "objectID": "simulation-slides.html#comparing-estimates",
    "href": "simulation-slides.html#comparing-estimates",
    "title": "Statistical Simulation",
    "section": "Comparing Estimates",
    "text": "Comparing Estimates\n\n\n\n\nTable 4\n\n\n\n\n\n\nMethod\nEstimate\n\n\n\n\nTrue value\n9.87e-10\n\n\nImportance sampling\n9.92e-10\n\n\nNaive (n = 10^6)\n0\n\n\n\n\n\n\n\n\n\nImportance sampling makes the impossible feasible."
  },
  {
    "objectID": "simulation-slides.html#quantifying-uncertainty",
    "href": "simulation-slides.html#quantifying-uncertainty",
    "title": "Statistical Simulation",
    "section": "Quantifying Uncertainty",
    "text": "Quantifying Uncertainty\nHow precise is our estimate?\n\nClassical approach: derive standard error formula.\n\n\nBootstrap approach: Resample from the data itself."
  },
  {
    "objectID": "simulation-slides.html#the-bootstrap-idea",
    "href": "simulation-slides.html#the-bootstrap-idea",
    "title": "Statistical Simulation",
    "section": "The Bootstrap Idea",
    "text": "The Bootstrap Idea\n\nTreat the sample as a proxy for the population\nDraw with replacement from the sample\nCompute statistic on each resample\nUse variability across resamples to estimate uncertainty"
  },
  {
    "objectID": "simulation-slides.html#bootstrap-in-action",
    "href": "simulation-slides.html#bootstrap-in-action",
    "title": "Statistical Simulation",
    "section": "Bootstrap in Action",
    "text": "Bootstrap in Action\n\n\nCode\n# Original sample\nx &lt;- c(12, 15, 18, 22, 25, 28, 31, 35, 42, 55)\n\n# Bootstrap\nB &lt;- 10000\nboot_means &lt;- numeric(B)\n\nfor (b in 1:B) {\n  x_star &lt;- sample(x, replace = TRUE)\n  boot_means[b] &lt;- mean(x_star)\n}\n\n# 95% confidence interval\nquantile(boot_means, c(0.025, 0.975))"
  },
  {
    "objectID": "simulation-slides.html#bootstrap-example",
    "href": "simulation-slides.html#bootstrap-example",
    "title": "Statistical Simulation",
    "section": "Bootstrap Example",
    "text": "Bootstrap Example\n\n\nFigure 4: Bootstrap distribution of the sample mean\n\nNormal theory 95% CI: [18.9, 37.7] — similar result."
  },
  {
    "objectID": "simulation-slides.html#bootstrap-for-any-statistic",
    "href": "simulation-slides.html#bootstrap-for-any-statistic",
    "title": "Statistical Simulation",
    "section": "Bootstrap for Any Statistic",
    "text": "Bootstrap for Any Statistic\nThe bootstrap works for almost any statistic:\n\n\nCorrelation coefficients\nRegression coefficients\nMedian, trimmed mean\nRatios, differences\n\n\n\nEDA application: “How much would this pattern change with different data?”"
  },
  {
    "objectID": "simulation-slides.html#simulation-in-the-ml-pipeline",
    "href": "simulation-slides.html#simulation-in-the-ml-pipeline",
    "title": "Statistical Simulation",
    "section": "Simulation in the ML Pipeline",
    "text": "Simulation in the ML Pipeline\nSimulation connects to machine learning:\n\nCross-validation: Repeatedly resample to estimate generalization error.\n\n\nStochastic gradient descent: Random sampling of training batches.\n\n\nBayesian ML: MCMC and variational inference sample from posteriors."
  },
  {
    "objectID": "simulation-slides.html#simulation-as-model-checking",
    "href": "simulation-slides.html#simulation-as-model-checking",
    "title": "Statistical Simulation",
    "section": "Simulation as Model Checking",
    "text": "Simulation as Model Checking\nAfter fitting a model:\n\n\nSimulate data from the fitted model\nCompare simulated data to real data\nDiscrepancies reveal model failures\n\n\n\nThis is the posterior predictive check—simulation for model criticism."
  },
  {
    "objectID": "simulation-slides.html#chapter-4-key-takeaways",
    "href": "simulation-slides.html#chapter-4-key-takeaways",
    "title": "Statistical Simulation",
    "section": "Chapter 4: Key Takeaways",
    "text": "Chapter 4: Key Takeaways\n\nSimulation explores what-if scenarios before they occur\nR provides d/p/q/r functions for common distributions\nMonte Carlo methods estimate quantities via random sampling\nImportance sampling handles rare events\nBootstrap quantifies uncertainty without formulas"
  },
  {
    "objectID": "simulation-slides.html#key-formulas",
    "href": "simulation-slides.html#key-formulas",
    "title": "Statistical Simulation",
    "section": "Key Formulas",
    "text": "Key Formulas\n\n\n\n\n\n\n\nConcept\nFormula\n\n\n\n\nMonte Carlo estimate\n\\(\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n g(X_i)\\)\n\n\nImportance weight\n\\(w(x) = p(x) / q(x)\\)\n\n\nVariance ratio (mean vs. median)\n\\(\\pi/2 \\approx 1.57\\)"
  },
  {
    "objectID": "simulation-slides.html#the-simulation-workflow",
    "href": "simulation-slides.html#the-simulation-workflow",
    "title": "Statistical Simulation",
    "section": "The Simulation Workflow",
    "text": "The Simulation Workflow\n\nSpecify the data-generating process\nGenerate random samples\nCompute statistics of interest\nRepeat many times\nSummarize the distribution of results"
  },
  {
    "objectID": "simulation-slides.html#team-exercise-1-mean-vs.-median-efficiency",
    "href": "simulation-slides.html#team-exercise-1-mean-vs.-median-efficiency",
    "title": "Statistical Simulation",
    "section": "Team Exercise 1: Mean vs. Median Efficiency",
    "text": "Team Exercise 1: Mean vs. Median Efficiency\nVerify the variance ratio for normal data:\n\nSet \\(n = 30\\) and \\(R = 5000\\) replications.\nFor each replication, generate a sample from rnorm(), compute mean and median.\nCalculate var(medians) / var(means). Compare to \\(\\pi/2 \\approx 1.57\\).\nWhat does this ratio tell us about the relative efficiency of the median?"
  },
  {
    "objectID": "simulation-slides.html#team-exercise-2-heavy-tails-and-the-cauchy",
    "href": "simulation-slides.html#team-exercise-2-heavy-tails-and-the-cauchy",
    "title": "Statistical Simulation",
    "section": "Team Exercise 2: Heavy Tails and the Cauchy",
    "text": "Team Exercise 2: Heavy Tails and the Cauchy\nRepeat Exercise 1, but use rcauchy() instead of rnorm().\n\nCalculate the sample variance of your \\(R\\) means. What do you observe?\nConstruct a histogram of sample means. Does it look normal?\nWhat does this imply about the Central Limit Theorem?"
  },
  {
    "objectID": "simulation-slides.html#team-exercise-3-bootstrap-confidence-interval",
    "href": "simulation-slides.html#team-exercise-3-bootstrap-confidence-interval",
    "title": "Statistical Simulation",
    "section": "Team Exercise 3: Bootstrap Confidence Interval",
    "text": "Team Exercise 3: Bootstrap Confidence Interval\nUse the bootstrap to estimate a 95% CI for the median of mtcars$mpg:\n\nResample with replacement \\(B = 2000\\) times.\nCompute the median for each bootstrap sample.\nUse the 2.5th and 97.5th percentiles as your CI.\nHow does the width compare to a bootstrap CI for the mean?"
  },
  {
    "objectID": "simulation-slides.html#team-exercise-4-importance-sampling",
    "href": "simulation-slides.html#team-exercise-4-importance-sampling",
    "title": "Statistical Simulation",
    "section": "Team Exercise 4: Importance Sampling",
    "text": "Team Exercise 4: Importance Sampling\nEstimate \\(P(Z &gt; 4)\\) using importance sampling. Compare to pnorm(4, lower.tail = FALSE).\n\nExercises 1–2 contrast well-behaved vs. pathological distributions. Exercise 3 introduces nonparametric inference. Exercise 4 practices the method of importance sampling."
  },
  {
    "objectID": "simulation-slides.html#discussion-questions",
    "href": "simulation-slides.html#discussion-questions",
    "title": "Statistical Simulation",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhen is simulation more trustworthy than mathematical derivation?\nA colleague sets set.seed(42) and runs one simulation. Is this reproducible research?\nHow many bootstrap replications are “enough”?"
  },
  {
    "objectID": "lin-reg-slides.html#linear-algebra-meets-statistics",
    "href": "lin-reg-slides.html#linear-algebra-meets-statistics",
    "title": "Linear Regression",
    "section": "Linear Algebra Meets Statistics",
    "text": "Linear Algebra Meets Statistics\nIn Part 1, we explored data through:\n\nScatter plots and conditional distributions\nClustering observations\nSimulation\n\n\nPart 2 takes a different approach: the geometry of linear algebra.\n\n\n\n\n\n\n\n\nCore insight\n\n\nLeast-squares regression is orthogonal projection onto feature space."
  },
  {
    "objectID": "lin-reg-slides.html#why-geometry",
    "href": "lin-reg-slides.html#why-geometry",
    "title": "Linear Regression",
    "section": "Why Geometry?",
    "text": "Why Geometry?\nLinear algebra provides:\n\nA unified language for regression, PCA, and classification\nGeometric intuition for what “fitting” means\nTools that scale to high dimensions\n\n\nThis chapter: Regression as projection\nNext chapters: PCA and LDA as finding optimal subspaces"
  },
  {
    "objectID": "lin-reg-slides.html#two-perspectives-on-data",
    "href": "lin-reg-slides.html#two-perspectives-on-data",
    "title": "Linear Regression",
    "section": "Two Perspectives on Data",
    "text": "Two Perspectives on Data\n\n\n\nPerspective\nFocus\nVisualization\n\n\n\n\nRow\nObservations as points\nScatter plots in feature space\n\n\nColumn\nFeatures as vectors\nVectors in observation space\n\n\n\n\nMost visualizations use the row perspective.\nMost linear algebra uses the column perspective.\n\n\nUnderstanding both is essential."
  },
  {
    "objectID": "lin-reg-slides.html#galtons-height-data",
    "href": "lin-reg-slides.html#galtons-height-data",
    "title": "Linear Regression",
    "section": "Galton’s Height Data",
    "text": "Galton’s Height Data\nIn 1885, Francis Galton studied heights of parents and children.\n\n\n205 families (oldest child only)\n179 sons, 26 daughters\nVariables: father’s height, mother’s height, child’s height"
  },
  {
    "objectID": "lin-reg-slides.html#the-data-matrix",
    "href": "lin-reg-slides.html#the-data-matrix",
    "title": "Linear Regression",
    "section": "The Data Matrix",
    "text": "The Data Matrix\n\n\n\n\nTable 1: Family heights (inches)\n\n\n\n\nFamily heights (inches)\n\n\nfather\nmother\nchild\ngender\n\n\n\n\n78.5\n67.0\n73.2\nmale\n\n\n75.5\n66.5\n73.5\nmale\n\n\n75.0\n64.0\n71.0\nmale\n\n\n75.0\n64.0\n70.5\nmale\n\n\n75.0\n58.5\n72.0\nmale\n\n\n74.0\n68.0\n69.5\nfemale\n\n\n\n\n\n\n\n\n\nGoal: Predict child’s height from parents’ heights."
  },
  {
    "objectID": "lin-reg-slides.html#d-visualization",
    "href": "lin-reg-slides.html#d-visualization",
    "title": "Linear Regression",
    "section": "3D Visualization",
    "text": "3D Visualization\n\n\nFigure 1: Family heights: sons (blue), daughters (red)"
  },
  {
    "objectID": "lin-reg-slides.html#from-line-to-plane",
    "href": "lin-reg-slides.html#from-line-to-plane",
    "title": "Linear Regression",
    "section": "From Line to Plane",
    "text": "From Line to Plane\nIn an earlier chapter, we regressed son’s height on father’s height → regression line\n\nNow: regress son’s height on both parents’ heights → regression plane\n\n\n\\[\n\\hat{s} = \\beta_0 + \\beta_m \\cdot m + \\beta_f \\cdot f\n\\]\nwhere \\(m\\) = mother’s height, \\(f\\) = father’s height"
  },
  {
    "objectID": "lin-reg-slides.html#the-regression-plane",
    "href": "lin-reg-slides.html#the-regression-plane",
    "title": "Linear Regression",
    "section": "The Regression Plane",
    "text": "The Regression Plane\n\n\nFigure 2: Regression plane with residual segments"
  },
  {
    "objectID": "lin-reg-slides.html#fitted-coefficients",
    "href": "lin-reg-slides.html#fitted-coefficients",
    "title": "Linear Regression",
    "section": "Fitted Coefficients",
    "text": "Fitted Coefficients\n\n\n\n\n\nTerm\nCoefficient\n\n\n\n\nIntercept\n19.94\n\n\nMother\n0.28\n\n\nFather\n0.47\n\n\n\n\n\n\nInterpretation:\n\nEach inch of mother’s height → 0.28 inches for son\nEach inch of father’s height → 0.47 inches for son"
  },
  {
    "objectID": "lin-reg-slides.html#matrix-notation",
    "href": "lin-reg-slides.html#matrix-notation",
    "title": "Linear Regression",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\\[\ny_\\bullet = X_{\\bullet,\\bullet} \\; \\beta_\\bullet + \\epsilon_\\bullet\n\\]\n\n\n\n\nSymbol\nName\nDimension\n\n\n\n\n\\(y_\\bullet\\)\nResponse (target)\n\\(n \\times 1\\)\n\n\n\\(X_{\\bullet,\\bullet}\\)\nFeature matrix\n\\(n \\times d\\)\n\n\n\\(\\beta_\\bullet\\)\nCoefficients\n\\(d \\times 1\\)\n\n\n\\(\\epsilon_\\bullet\\)\nResiduals\n\\(n \\times 1\\)"
  },
  {
    "objectID": "lin-reg-slides.html#feature-space",
    "href": "lin-reg-slides.html#feature-space",
    "title": "Linear Regression",
    "section": "Feature Space",
    "text": "Feature Space\nThe columns of \\(X\\) span a subspace called feature space:\n\\[\n\\text{col}(X) = \\text{span}(x_{\\bullet,1}, \\ldots, x_{\\bullet,d})\n\\]\n\nAny prediction \\(\\hat{y} = X\\beta\\) is a linear combination of feature vectors:\n\\[\n\\hat{y}_\\bullet = \\beta_1 x_{\\bullet,1} + \\cdots + \\beta_d x_{\\bullet,d}\n\\]\n\n\nSo \\(\\hat{y}\\) must lie in \\(\\text{col}(X)\\)."
  },
  {
    "objectID": "lin-reg-slides.html#the-central-question",
    "href": "lin-reg-slides.html#the-central-question",
    "title": "Linear Regression",
    "section": "The Central Question",
    "text": "The Central Question\nGiven \\(y_\\bullet\\) and \\(X_{\\bullet,\\bullet}\\):\n\nFind \\(\\beta_\\bullet\\) that minimizes \\(\\|\\epsilon_\\bullet\\|\\)\n\n\nThis is the least-squares problem.\n\n\n\n\n\n\n\n\nGeometric interpretation\n\n\nFind the point in feature space closest to \\(y\\)."
  },
  {
    "objectID": "lin-reg-slides.html#distance-and-norms",
    "href": "lin-reg-slides.html#distance-and-norms",
    "title": "Linear Regression",
    "section": "Distance and Norms",
    "text": "Distance and Norms\nThe Euclidean norm (length) of vector \\(v\\):\n\\[\n\\|v_\\bullet\\|_2 = \\sqrt{\\sum_{\\nu=1}^{n} v_\\nu^2}\n\\]\n\nMinimizing \\(\\|\\epsilon\\|^2\\) is the least-squares criterion:\n\\[\n\\min_\\beta \\sum_{\\nu=1}^{n} (y_\\nu - \\hat{y}_\\nu)^2\n\\]"
  },
  {
    "objectID": "lin-reg-slides.html#inner-products",
    "href": "lin-reg-slides.html#inner-products",
    "title": "Linear Regression",
    "section": "Inner Products",
    "text": "Inner Products\nThe inner product of vectors \\(v\\) and \\(w\\):\n\\[\n\\langle v, w \\rangle = v^\\top w = \\sum_{\\nu=1}^{n} v_\\nu w_\\nu\n\\]\n\nKey properties:\n\n\\(\\|v\\|^2 = \\langle v, v \\rangle\\)\n\\(v \\perp w\\) (orthogonal) when \\(\\langle v, w \\rangle = 0\\)\n\n\n\nInner products connect to covariance and correlation."
  },
  {
    "objectID": "lin-reg-slides.html#the-projection-insight",
    "href": "lin-reg-slides.html#the-projection-insight",
    "title": "Linear Regression",
    "section": "The Projection Insight",
    "text": "The Projection Insight\nTheorem: The least-squares solution \\(\\hat{y}\\) is the orthogonal projection of \\(y\\) onto \\(\\text{col}(X)\\).\n\n\n\n\n\n\n\n\n\nFigure 3: Orthogonal projection onto feature space"
  },
  {
    "objectID": "lin-reg-slides.html#why-orthogonal",
    "href": "lin-reg-slides.html#why-orthogonal",
    "title": "Linear Regression",
    "section": "Why Orthogonal?",
    "text": "Why Orthogonal?\nThe residual \\(\\epsilon = y - \\hat{y}\\) is perpendicular to feature space.\n\nIntuition: If \\(\\epsilon\\) had any component along \\(\\text{col}(X)\\), we could reduce its length by adjusting \\(\\hat{y}\\).\n\n\nMathematically: \\(\\epsilon \\perp x_{\\bullet,j}\\) for every feature vector \\(j\\)."
  },
  {
    "objectID": "lin-reg-slides.html#the-intercept-problem",
    "href": "lin-reg-slides.html#the-intercept-problem",
    "title": "Linear Regression",
    "section": "The Intercept Problem",
    "text": "The Intercept Problem\nThe regression plane \\(\\hat{s} = \\beta_0 + \\beta_m m + \\beta_f f\\) doesn’t pass through the origin.\n\nProblem: It’s not a proper subspace (subspaces must contain the origin).\n\n\nSolution: Center the data!"
  },
  {
    "objectID": "lin-reg-slides.html#centering-transformation",
    "href": "lin-reg-slides.html#centering-transformation",
    "title": "Linear Regression",
    "section": "Centering Transformation",
    "text": "Centering Transformation\nReplace each value with its deviation from the mean:\n\\[\n\\dot{v}_\\nu = v_\\nu - \\bar{v}\n\\]\n\nAfter centering:\n\nMean of each variable is zero\nThe regression “plane” passes through the origin\nNo intercept term needed"
  },
  {
    "objectID": "lin-reg-slides.html#matrix-form-of-centering",
    "href": "lin-reg-slides.html#matrix-form-of-centering",
    "title": "Linear Regression",
    "section": "Matrix Form of Centering",
    "text": "Matrix Form of Centering\n\\[\n\\dot{v}_\\bullet = C \\; v_\\bullet\n\\]\nwhere\n\\[\nC = I - \\frac{1}{n} \\mathbf{1}\\mathbf{1}^\\top\n\\]\n\n\\(C\\) is itself a projection matrix—it projects onto the subspace orthogonal to the constant vector."
  },
  {
    "objectID": "lin-reg-slides.html#deriving-the-solution",
    "href": "lin-reg-slides.html#deriving-the-solution",
    "title": "Linear Regression",
    "section": "Deriving the Solution",
    "text": "Deriving the Solution\nTo minimize \\(\\|\\epsilon\\|^2 = \\|y - X\\beta\\|^2\\):\n\nTake derivative with respect to \\(\\beta\\), set to zero:\n\\[\nX^\\top X \\; \\hat{\\beta} = X^\\top y\n\\]\n\n\nThese are the normal equations."
  },
  {
    "objectID": "lin-reg-slides.html#the-projection-matrix",
    "href": "lin-reg-slides.html#the-projection-matrix",
    "title": "Linear Regression",
    "section": "The Projection Matrix",
    "text": "The Projection Matrix\nSolving for \\(\\hat{\\beta}\\):\n\\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n\\]\n\nThe predicted values:\n\\[\n\\hat{y} = X \\hat{\\beta} = \\underbrace{X(X^\\top X)^{-1} X^\\top}_{P} y\n\\]\n\n\nMatrix \\(P\\) is the projection matrix (or “hat matrix”)."
  },
  {
    "objectID": "lin-reg-slides.html#properties-of-p",
    "href": "lin-reg-slides.html#properties-of-p",
    "title": "Linear Regression",
    "section": "Properties of P",
    "text": "Properties of P\nThe projection matrix \\(P = X(X^\\top X)^{-1}X^\\top\\) satisfies:\n\nIdempotent: \\(P^2 = P\\)\n(Projecting twice = projecting once)\n\n\nSymmetric: \\(P^\\top = P\\)\n(Makes it an orthogonal projection)\n\n\nComplementary: \\((I - P)\\) projects onto the orthogonal complement"
  },
  {
    "objectID": "lin-reg-slides.html#orthogonality-of-residuals",
    "href": "lin-reg-slides.html#orthogonality-of-residuals",
    "title": "Linear Regression",
    "section": "Orthogonality of Residuals",
    "text": "Orthogonality of Residuals\nSince \\(\\hat{y} = Py\\) and \\(\\epsilon = (I-P)y\\):\n\\[\n\\hat{y}^\\top \\epsilon = y^\\top P^\\top (I-P) y = y^\\top (P - P^2) y = 0\n\\]\n\n\n\n\n\n\n\nKey result\n\n\nPredicted values and residuals are orthogonal."
  },
  {
    "objectID": "lin-reg-slides.html#two-ways-to-see-regression",
    "href": "lin-reg-slides.html#two-ways-to-see-regression",
    "title": "Linear Regression",
    "section": "Two Ways to See Regression",
    "text": "Two Ways to See Regression\n\n\n\nView\nSpace\nDimension\n\n\n\n\nRow\nObservations as points in \\(\\mathbb{R}^d\\)\n\\(d\\)-dimensional\n\n\nColumn\nFeatures as vectors in \\(\\mathbb{R}^n\\)\n\\(n\\)-dimensional"
  },
  {
    "objectID": "lin-reg-slides.html#row-view-fitting-a-plane",
    "href": "lin-reg-slides.html#row-view-fitting-a-plane",
    "title": "Linear Regression",
    "section": "Row View: Fitting a Plane",
    "text": "Row View: Fitting a Plane\n\n\nFigure 4: Row view: points in feature space, plane fits through them"
  },
  {
    "objectID": "lin-reg-slides.html#column-view-projection",
    "href": "lin-reg-slides.html#column-view-projection",
    "title": "Linear Regression",
    "section": "Column View: Projection",
    "text": "Column View: Projection\nIn \\(n\\)-dimensional space (one dimension per observation):\n\nEach feature is a vector of length \\(n\\)\n\\(y\\) is a vector of length \\(n\\)\n\\(\\hat{y}\\) is the projection of \\(y\\) onto \\(\\text{span}(x_1, x_2, \\ldots, x_d)\\)\n\n\nThis is where the linear algebra happens!"
  },
  {
    "objectID": "lin-reg-slides.html#connecting-the-views",
    "href": "lin-reg-slides.html#connecting-the-views",
    "title": "Linear Regression",
    "section": "Connecting the Views",
    "text": "Connecting the Views\n\n\n\n\n\n\n\n\nOperation\nRow View\nColumn View\n\n\n\n\nFit model\nFind plane minimizing vertical distances\nProject \\(y\\) onto \\(\\text{col}(X)\\)\n\n\nResidual\nVertical distance to plane\n\\(y - \\hat{y}\\)\n\n\nPrediction\nPoint on plane\nVector in \\(\\text{col}(X)\\)"
  },
  {
    "objectID": "lin-reg-slides.html#why-this-matters-for-ml",
    "href": "lin-reg-slides.html#why-this-matters-for-ml",
    "title": "Linear Regression",
    "section": "Why This Matters for ML",
    "text": "Why This Matters for ML\nThe projection perspective is foundational for:\n\nRegularization: Ridge regression modifies the projection \\[\\hat{\\beta}_{\\text{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top y\\]\n\n\nDimension reduction: PCA finds the “best” low-dimensional subspace\n\n\nKernel methods: Project into implicit high-dimensional feature spaces\n\n\nNeural networks: Each layer performs a linear transformation (projection) followed by a nonlinearity"
  },
  {
    "objectID": "lin-reg-slides.html#covariance-as-inner-product",
    "href": "lin-reg-slides.html#covariance-as-inner-product",
    "title": "Linear Regression",
    "section": "Covariance as Inner Product",
    "text": "Covariance as Inner Product\nFor centered data:\n\\[\n\\text{Cov}(x, y) = \\frac{1}{n-1} \\langle \\dot{x}, \\dot{y} \\rangle\n\\]\n\nCorrelation is the cosine of the angle between centered vectors:\n\\[\nr = \\frac{\\langle \\dot{x}, \\dot{y} \\rangle}{\\|\\dot{x}\\| \\; \\|\\dot{y}\\|}\n\\]\n\n\nThis connects regression to the correlation coefficient from Part 1!"
  },
  {
    "objectID": "lin-reg-slides.html#key-takeaways",
    "href": "lin-reg-slides.html#key-takeaways",
    "title": "Linear Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRow vs Column: Two complementary views of the same data\nLeast squares = Projection: \\(\\hat{y}\\) is the orthogonal projection of \\(y\\) onto feature space\nCentering matters: Makes the geometry cleaner (proper subspaces)\nP is special: Symmetric, idempotent → orthogonal projection\nResiduals ⊥ Features: The defining property of least-squares"
  },
  {
    "objectID": "lin-reg-slides.html#key-formulas",
    "href": "lin-reg-slides.html#key-formulas",
    "title": "Linear Regression",
    "section": "Key Formulas",
    "text": "Key Formulas\nGeneric linear model: \\[y = X\\beta + \\epsilon\\]\n\nNormal equations: \\[X^\\top X \\hat{\\beta} = X^\\top y\\]\n\n\nProjection matrix: \\[P = X(X^\\top X)^{-1}X^\\top\\]\n\n\nPredicted values: \\[\\hat{y} = Py\\]"
  },
  {
    "objectID": "lin-reg-slides.html#team-exercise-1-by-hand",
    "href": "lin-reg-slides.html#team-exercise-1-by-hand",
    "title": "Linear Regression",
    "section": "Team Exercise 1: By Hand",
    "text": "Team Exercise 1: By Hand\nLet \\(y = (3, 1, 2)^\\top\\) and \\(x = (1, 2, 2)^\\top\\).\n\nFind the projection \\(\\hat{y}\\) of \\(y\\) onto \\(\\text{span}(x)\\)\nCompute the residual \\(\\epsilon = y - \\hat{y}\\)\nVerify that \\(\\epsilon \\perp x\\)"
  },
  {
    "objectID": "lin-reg-slides.html#team-exercise-2-interpretation",
    "href": "lin-reg-slides.html#team-exercise-2-interpretation",
    "title": "Linear Regression",
    "section": "Team Exercise 2: Interpretation",
    "text": "Team Exercise 2: Interpretation\nFor the Galton height regression:\n\nWhat does it mean geometrically that \\(\\hat{y} \\in \\text{col}(X)\\)?\nWhy must the residuals be orthogonal to every column of \\(X\\)?\nIf we added a third predictor (e.g., sibling count), how would the geometry change?"
  },
  {
    "objectID": "lin-reg-slides.html#team-exercise-3-correlation-as-geometry",
    "href": "lin-reg-slides.html#team-exercise-3-correlation-as-geometry",
    "title": "Linear Regression",
    "section": "Team Exercise 3: Correlation as Geometry",
    "text": "Team Exercise 3: Correlation as Geometry\nFor centered vectors \\(\\dot{x}\\) and \\(\\dot{y}\\):\n\nShow that \\(r^2 = \\cos^2(\\theta)\\) where \\(\\theta\\) is the angle between them\nWhat does \\(r = 1\\) mean geometrically?\nWhat does \\(r = 0\\) mean geometrically?"
  },
  {
    "objectID": "lin-reg-slides.html#references",
    "href": "lin-reg-slides.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\nStatistics by Freedman, Pisani, Purves — intuitive treatment of regression\nLinear Algebra and Its Applications by Strang — definitive LA text\nIntroduction to Statistical Learning (ISLR2) — regression in ML context\nThe Elements of Statistical Learning — advanced treatment"
  },
  {
    "objectID": "info-theory-slides.html#a-coda-to-part-1",
    "href": "info-theory-slides.html#a-coda-to-part-1",
    "title": "Information Theory",
    "section": "A Coda to Part 1",
    "text": "A Coda to Part 1\nPart 1 introduced EDA from a geometric perspective:\n\nScatter plots and conditional distributions\nClustering of observations\nSimulation\n\n\nThis chapter takes a different approach: information theory.\n\n\nThese concepts appear repeatedly in ML:\n\nDecision tree construction\nNeural network training (cross-entropy loss)\nModel comparison\n\n\nThis is positioned as a “coda”—self-contained vocabulary that students will encounter in specialized ML studies."
  },
  {
    "objectID": "info-theory-slides.html#chapter-roadmap",
    "href": "info-theory-slides.html#chapter-roadmap",
    "title": "Information Theory",
    "section": "Chapter Roadmap",
    "text": "Chapter Roadmap\n\n\n\nConcept\nIntuition\n\n\n\n\nEntropy\nHow uncertain is an outcome?\n\n\nMutual Information\nHow much information do (X, Y) share?\n\n\nKL Divergence\nHow costly is using the wrong distribution?\n\n\n\n\nAll three connect to the Twenty Questions game."
  },
  {
    "objectID": "info-theory-slides.html#sec-entropy",
    "href": "info-theory-slides.html#sec-entropy",
    "title": "Information Theory",
    "section": "Entropy: The Yes-No Questions Game",
    "text": "Entropy: The Yes-No Questions Game\nImagine a box of tickets, each bearing a capital letter.\n\nThe game:\n\nYou are shown the box of tickets.\nA ticket is drawn at random, but you can’t see it.\nYou ask yes-no questions until you can identify the letter with certainty.\nGoal: minimize average number of questions.\n\n\n\nEntropy measures the difficulty of this game."
  },
  {
    "objectID": "info-theory-slides.html#box-1-no-uncertainty",
    "href": "info-theory-slides.html#box-1-no-uncertainty",
    "title": "Information Theory",
    "section": "Box 1: No Uncertainty",
    "text": "Box 1: No Uncertainty\n\\[\\text{Box 1: } \\{A, A, A, A\\}\\]\n\nHow many questions needed?\n\n\nZero. You already know the answer.\n\n\n\\[H = 0\\]"
  },
  {
    "objectID": "info-theory-slides.html#box-2-one-bit-of-uncertainty",
    "href": "info-theory-slides.html#box-2-one-bit-of-uncertainty",
    "title": "Information Theory",
    "section": "Box 2: One Bit of Uncertainty",
    "text": "Box 2: One Bit of Uncertainty\n\\[\\text{Box 2: } \\{A, A, B, B\\}\\]\n\nOne question: “Is it A?”\n\n\nOne question always suffices.\n\n\n\\[H = 1 \\text{ bit}\\]"
  },
  {
    "objectID": "info-theory-slides.html#box-3-two-bits-of-uncertainty",
    "href": "info-theory-slides.html#box-3-two-bits-of-uncertainty",
    "title": "Information Theory",
    "section": "Box 3: Two Bits of Uncertainty",
    "text": "Box 3: Two Bits of Uncertainty\n\\[\\text{Box 3: } \\{A, B, C, D\\}\\]\n\nFirst question: “Is it A or B?”\nSecond question: “Is it [first of the pair]?”\n\n\nTwo questions always suffices.\n\n\n\\[H = 2 \\text{ bits}\\]"
  },
  {
    "objectID": "info-theory-slides.html#box-4-fractional-entropy",
    "href": "info-theory-slides.html#box-4-fractional-entropy",
    "title": "Information Theory",
    "section": "Box 4: Fractional Entropy",
    "text": "Box 4: Fractional Entropy\n\\[\\text{Box 4: } \\{A, A, B, C\\}\\]\n\nOptimal strategy:\n\nFirst ask: “Is it A?” (probability ½)\nIf yes → done (1 question)\nIf no → one more question to distinguish B from C\n\n\n\n\\[\\text{Average} = \\frac{1}{2}(1) + \\frac{1}{2}(2) = \\frac{3}{2} \\text{ questions}\\]\n\n\n\\[H = 1.5 \\text{ bits}\\]"
  },
  {
    "objectID": "info-theory-slides.html#the-entropy-formula",
    "href": "info-theory-slides.html#the-entropy-formula",
    "title": "Information Theory",
    "section": "The Entropy Formula",
    "text": "The Entropy Formula\nFor a probability distribution \\((p_1, p_2, \\ldots, p_K)\\):\n\\[H = -\\sum_{k=1}^{K} p_k \\log_2(p_k)\\]\n\nEquivalently:\n\\[H = \\sum_{k=1}^{K} p_k \\log_2\\left(\\frac{1}{p_k}\\right)\\]\n\n\nUnits: bits (binary digits) when using \\(\\log_2\\)"
  },
  {
    "objectID": "info-theory-slides.html#entropy-examples",
    "href": "info-theory-slides.html#entropy-examples",
    "title": "Information Theory",
    "section": "Entropy Examples",
    "text": "Entropy Examples\n\n\n\n\n\nBox\nContents\nProbabilities\nH\n\n\n\n\nBox 1\n{A,A,A,A}\n(1)\n0\n\n\nBox 2\n{A,A,B,B}\n(½, ½)\n1\n\n\nBox 3\n{A,B,C,D}\n(¼,¼,¼,¼)\n2\n\n\nBox 4\n{A,A,B,C}\n(½,¼,¼)\n1.5\n\n\n\n\n\n\nPattern: Entropy is maximized when all outcomes are equally likely."
  },
  {
    "objectID": "info-theory-slides.html#binary-search-strategy",
    "href": "info-theory-slides.html#binary-search-strategy",
    "title": "Information Theory",
    "section": "Binary Search Strategy",
    "text": "Binary Search Strategy\nGeneral approach:\n\nPartition tickets into two groups of equal (or nearly equal) probability\nAsk which group contains the drawn ticket\nRepeat within the identified group\n\n\nMaximum questions: \\(\\lceil \\log_2(K) \\rceil\\) for \\(K\\) distinct values\n\n\nAverage questions: Often fewer (as Box 4 shows)"
  },
  {
    "objectID": "info-theory-slides.html#sec-joint-entropy",
    "href": "info-theory-slides.html#sec-joint-entropy",
    "title": "Information Theory",
    "section": "Joint Entropy: Two Variables",
    "text": "Joint Entropy: Two Variables\nNow each ticket has a letter and a number.\n\n\\[\\text{Box 5: } \\{A_1, A_1, B_1, C_1, A_2, A_2, B_2, C_2\\}\\]\n\n\nThis is equivalent to:\n\nDraw letter from \\(\\{A, A, B, C\\}\\) → \\(H_{\\text{letter}} = 1.5\\)\nDraw number from \\(\\{1, 2\\}\\) → \\(H_{\\text{number}} = 1\\)\n\n\n\nIndependently!"
  },
  {
    "objectID": "info-theory-slides.html#entropy-of-independent-variables",
    "href": "info-theory-slides.html#entropy-of-independent-variables",
    "title": "Information Theory",
    "section": "Entropy of Independent Variables",
    "text": "Entropy of Independent Variables\nWhen \\(X\\) and \\(Y\\) are independent:\n\\[H_{X,Y} = H_X + H_Y\\]\n\nFor Box 5:\n\\[H = 1.5 + 1 = 2.5 \\text{ bits}\\]\n\n\nIntuition: No information about the letter helps you guess the number, and vice versa."
  },
  {
    "objectID": "info-theory-slides.html#sec-mutual-info",
    "href": "info-theory-slides.html#sec-mutual-info",
    "title": "Information Theory",
    "section": "Mutual Information: When Variables Are Dependent",
    "text": "Mutual Information: When Variables Are Dependent\n\\[\\text{Box 6: } \\{A_1, A_2, B_1, C_2\\}\\]\n\nSame marginal distributions:\n\nLetters: \\(\\{A, A, B, C\\}\\) → \\(H_{\\text{letter}} = 1.5\\)\nNumbers: \\(\\{1, 1, 2, 2\\}\\) → \\(H_{\\text{number}} = 1\\)\n\n\n\nBut now they’re dependent!"
  },
  {
    "objectID": "info-theory-slides.html#the-power-of-dependence",
    "href": "info-theory-slides.html#the-power-of-dependence",
    "title": "Information Theory",
    "section": "The Power of Dependence",
    "text": "The Power of Dependence\nNew strategy for Box 6:\n\nFirst ask: “Is the number 1?” (1 question)\nIf yes → letter is A or B (1 more question)\nIf no → letter is A or C (1 more question)\n\n\nTotal: 2 questions (not 2.5!)\n\n\n\\[H_{X,Y} = 2 &lt; H_X + H_Y = 2.5\\]"
  },
  {
    "objectID": "info-theory-slides.html#mutual-information-defined",
    "href": "info-theory-slides.html#mutual-information-defined",
    "title": "Information Theory",
    "section": "Mutual Information Defined",
    "text": "Mutual Information Defined\n\\[MI_{X,Y} = H_X + H_Y - H_{X,Y}\\]\n\nInterpretation: The reduction in uncertainty about \\(Y\\) from knowing \\(X\\) (and vice versa).\n\n\nFor Box 6:\n\\[MI = 1.5 + 1 - 2 = 0.5 \\text{ bits}\\]\n\n\nProperties:\n\n\\(MI \\geq 0\\) always\n\\(MI = 0\\) if and only if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "info-theory-slides.html#mutual-information-visual-intuition",
    "href": "info-theory-slides.html#mutual-information-visual-intuition",
    "title": "Information Theory",
    "section": "Mutual Information: Visual Intuition",
    "text": "Mutual Information: Visual Intuition\n\n\nFigure 1\n\\[H_{X,Y} = H_X + H_Y - MI_{X,Y}\\]"
  },
  {
    "objectID": "info-theory-slides.html#sec-kl-divergence",
    "href": "info-theory-slides.html#sec-kl-divergence",
    "title": "Information Theory",
    "section": "KL Divergence: The Cost of Being Wrong",
    "text": "KL Divergence: The Cost of Being Wrong\nScenario: You’re shown Box 5 (independent) and optimized your strategy accordingly, but the actual box is Box 6 (dependent).\n\n\nYour strategy expects 2.5 questions on average\nOptimal for Box 6 needs only 2 questions\n\n\n\nThe cost of misinformation: 0.5 extra questions per round"
  },
  {
    "objectID": "info-theory-slides.html#kl-divergence-defined",
    "href": "info-theory-slides.html#kl-divergence-defined",
    "title": "Information Theory",
    "section": "KL Divergence Defined",
    "text": "KL Divergence Defined\n\\[KL(P \\| Q) = \\sum_{x} P(x) \\log_2\\left(\\frac{P(x)}{Q(x)}\\right)\\]\n\nInterpretation: Expected extra bits needed when using code optimized for \\(Q\\) but the true distribution is \\(P\\).\n\n\n\\(P\\) = true distribution (what the box actually is)\n\\(Q\\) = assumed distribution (what you think it is)"
  },
  {
    "objectID": "info-theory-slides.html#kl-divergence-example",
    "href": "info-theory-slides.html#kl-divergence-example",
    "title": "Information Theory",
    "section": "KL Divergence Example",
    "text": "KL Divergence Example\n\n\n\n\n\nx\nP(x)\nQ(x)\nlog₂(P/Q)\nTerm\n\n\n\n\nA₁\n1/4\n1/4\n0\n0\n\n\nA₂\n1/4\n1/4\n0\n0\n\n\nB₁\n1/4\n1/8\n1\n1/4\n\n\nB₂\n0\n1/8\n—\n0\n\n\nC₁\n0\n1/8\n—\n0\n\n\nC₂\n1/4\n1/8\n1\n1/4\n\n\n\n\n\n\n\\[KL(P \\| Q) = 0 + 0 + \\frac{1}{4} + 0 + 0 + \\frac{1}{4} = \\frac{1}{2}\\]\n\n\nConfirms: Using the wrong distribution costs 0.5 bits."
  },
  {
    "objectID": "info-theory-slides.html#kl-divergence-properties",
    "href": "info-theory-slides.html#kl-divergence-properties",
    "title": "Information Theory",
    "section": "KL Divergence Properties",
    "text": "KL Divergence Properties\nNot symmetric: \\(KL(P \\| Q) \\neq KL(Q \\| P)\\) in general\n\nNon-negative: \\(KL(P \\| Q) \\geq 0\\)\n\n\nZero iff identical: \\(KL(P \\| Q) = 0 \\Leftrightarrow P = Q\\)\n\n\nNot a true distance (fails triangle inequality)—hence “divergence”"
  },
  {
    "objectID": "info-theory-slides.html#sec-ml-applications",
    "href": "info-theory-slides.html#sec-ml-applications",
    "title": "Information Theory",
    "section": "Where These Ideas Appear in ML",
    "text": "Where These Ideas Appear in ML\n\n\n\n\n\n\n\nConcept\nML Application\n\n\n\n\nEntropy\nDecision tree splits\n\n\nMutual Information\nFeature selection\n\n\nKL Divergence\nLoss functions, variational inference1\n\n\nCross-entropy\nClassification loss\n\n\n\nVariational autoencoders (VAEs) and related methods."
  },
  {
    "objectID": "info-theory-slides.html#cross-entropy-loss",
    "href": "info-theory-slides.html#cross-entropy-loss",
    "title": "Information Theory",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\nThe standard loss function for classification—cross-entropy—is derived from KL divergence.\n\nMinimizing cross-entropy loss is equivalent to minimizing the divergence between:\n\nthe model’s predicted probabilities\nthe observed outcomes"
  },
  {
    "objectID": "info-theory-slides.html#decision-trees-and-information-gain",
    "href": "info-theory-slides.html#decision-trees-and-information-gain",
    "title": "Information Theory",
    "section": "Decision Trees and Information Gain",
    "text": "Decision Trees and Information Gain\nWhen splitting a node:\n\\[\\text{Information Gain} = H(\\text{parent}) - \\sum_{\\text{children}} \\frac{n_{\\text{child}}}{n_{\\text{parent}}} H(\\text{child})\\]\n\nStrategy: Choose the split that maximizes information gain."
  },
  {
    "objectID": "info-theory-slides.html#feature-selection-with-mutual-information",
    "href": "info-theory-slides.html#feature-selection-with-mutual-information",
    "title": "Information Theory",
    "section": "Feature Selection with Mutual Information",
    "text": "Feature Selection with Mutual Information\nCorrelation captures linear relationships.\n\nMutual Information captures any dependence.\n\n\n\n\n\n\n\n\n\n\n\nCorrelation misses the parabolic relationship. Mutual information catches it."
  },
  {
    "objectID": "info-theory-slides.html#summary-three-linked-concepts",
    "href": "info-theory-slides.html#summary-three-linked-concepts",
    "title": "Information Theory",
    "section": "Summary: Three Linked Concepts",
    "text": "Summary: Three Linked Concepts\n\n\n\n\n\n\n\nConcept\nQuestion Answered\n\n\n\n\nEntropy \\(H\\)\nHow uncertain is \\(X\\)?\n\n\nMutual Information \\(MI\\)\nHow much does \\(X\\) tell us about \\(Y\\)?\n\n\nKL Divergence \\(KL\\)\nHow costly is assuming \\(Q\\) when truth is \\(P\\)?\n\n\n\n\n\\[MI_{X,Y} = KL\\big(P_{X,Y} \\| P_X \\cdot P_Y\\big)\\]\nMutual information is the KL divergence from independence."
  },
  {
    "objectID": "info-theory-slides.html#key-formulas",
    "href": "info-theory-slides.html#key-formulas",
    "title": "Information Theory",
    "section": "Key Formulas",
    "text": "Key Formulas\n\\[H(X) = -\\sum_x P(x) \\log_2 P(x)\\]\n\n\\[MI_{X,Y} = H_X + H_Y - H_{X,Y}\\]\n\n\n\\[KL(P \\| Q) = \\sum_x P(x) \\log_2 \\frac{P(x)}{Q(x)}\\]"
  },
  {
    "objectID": "info-theory-slides.html#team-exercise-1-entropy-of-a-fair-die",
    "href": "info-theory-slides.html#team-exercise-1-entropy-of-a-fair-die",
    "title": "Information Theory",
    "section": "Team Exercise 1: Entropy of a Fair Die",
    "text": "Team Exercise 1: Entropy of a Fair Die\n\nCalculate the entropy \\(H\\) for a fair six-sided die.\nCompare to a loaded die with \\(P(6) = 0.5\\) and others equal.\nWhich has higher entropy? Why does this make sense?\nGeneralize: what is \\(H\\) for a discrete uniform distribution on \\(K\\) outcomes?"
  },
  {
    "objectID": "info-theory-slides.html#team-exercise-2-ucb-admissions-revisited",
    "href": "info-theory-slides.html#team-exercise-2-ucb-admissions-revisited",
    "title": "Information Theory",
    "section": "Team Exercise 2: UCB Admissions Revisited",
    "text": "Team Exercise 2: UCB Admissions Revisited\nConsider a box of tickets matching the UC Berkeley admissions data.\n\nRestricting to just “Admitted” vs. “Rejected,” calculate \\(H_{\\text{decision}}\\).\nNow calculate the joint entropy \\(H_{\\text{decision, sex}}\\) and mutual information \\(MI\\).\nWhat does \\(MI \\approx 0\\) tell us about marginal sex bias?\nHow would you incorporate department to reveal Simpson’s paradox?"
  },
  {
    "objectID": "info-theory-slides.html#team-exercise-3-cross-entropy-loss",
    "href": "info-theory-slides.html#team-exercise-3-cross-entropy-loss",
    "title": "Information Theory",
    "section": "Team Exercise 3: Cross-Entropy Loss",
    "text": "Team Exercise 3: Cross-Entropy Loss\nA classifier predicts probabilities for 3 classes. True class is 1.\nCompare cross-entropy loss for these predictions:\n\n\n\nPrediction\n\\((\\hat{p}_1, \\hat{p}_2, \\hat{p}_3)\\)\n\n\n\n\nConfident & correct\n\\((0.9, 0.05, 0.05)\\)\n\n\nLess confident\n\\((0.6, 0.2, 0.2)\\)\n\n\nUniform\n\\((0.33, 0.33, 0.34)\\)\n\n\n\n\nCalculate the loss for each.\nWhat happens as \\(\\hat{p}_1 \\to 0\\)?\nWhy is this the standard loss for classification?\n\n\nExercise 1 builds intuition for entropy. Exercise 2 connects to conditioning chapter. Exercise 3 motivates cross-entropy in neural networks."
  },
  {
    "objectID": "info-theory-slides.html#discussion-questions",
    "href": "info-theory-slides.html#discussion-questions",
    "title": "Information Theory",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhen would mutual information identify a useful feature that correlation misses?\nWhy is KL divergence not symmetric? Give an example where direction matters.\nHow does information theory connect to compression?"
  },
  {
    "objectID": "graph-theory-slides.html#networks-are-everywhere",
    "href": "graph-theory-slides.html#networks-are-everywhere",
    "title": "Graph Theory for Machine Learning",
    "section": "Networks Are Everywhere",
    "text": "Networks Are Everywhere\nGraphs appear whenever entities are connected:\n\nSocial networks: People linked by friendship, collaboration, influence\nBiological networks: Proteins linked by interactions, genes by regulation\nInformation networks: Web pages linked by hyperlinks, papers by citations\nKnowledge graphs: Concepts linked by semantic relationships\n\n\nThe mathematical abstraction—nodes and edges—unifies these diverse domains."
  },
  {
    "objectID": "graph-theory-slides.html#understanding-and-prediction",
    "href": "graph-theory-slides.html#understanding-and-prediction",
    "title": "Graph Theory for Machine Learning",
    "section": "Understanding and Prediction",
    "text": "Understanding and Prediction\nThroughout this book we have seen that understanding and decision support are intertwined.\n\nGraph analysis makes this especially vivid.\n\n\n\n\n\n\n\nPerspective\nQuestion\n\n\n\n\nPrediction\nWhich nodes will connect next? What should we recommend?\n\n\nUnderstanding\nWhat communities exist? Who is influential? Why?\n\n\n\n\n\nThese perspectives are complementary—structure enables prediction, and prediction reveals structure."
  },
  {
    "objectID": "graph-theory-slides.html#the-questions-graphs-answer",
    "href": "graph-theory-slides.html#the-questions-graphs-answer",
    "title": "Graph Theory for Machine Learning",
    "section": "The Questions Graphs Answer",
    "text": "The Questions Graphs Answer\nStructural questions (understanding):\n\nWhat communities or clusters exist?\nWhich nodes are central or influential?\nHow does information or influence flow?\n\n\nPredictive questions (decision support):\n\nWill these two nodes connect? (link prediction)\nWhat should this user see next? (recommendation)\nWhat’s the shortest path from here to there? (routing)"
  },
  {
    "objectID": "graph-theory-slides.html#three-examples",
    "href": "graph-theory-slides.html#three-examples",
    "title": "Graph Theory for Machine Learning",
    "section": "Three Examples",
    "text": "Three Examples\nWe explore graphs through three examples of increasing complexity:\n\n\n\n\n\n\n\n\nExample\nDomain\nKey Concepts\n\n\n\n\nZachary’s Karate Club\nSocial network\nCommunity detection, centrality\n\n\nMovieLens\nRecommendations\nBipartite graphs, projection\n\n\nLearningGraph\nKnowledge graph\nTyped nodes, path queries, gap analysis\n\n\n\n\nEach illuminates different aspects of graph structure and different analytical questions."
  },
  {
    "objectID": "graph-theory-slides.html#the-story",
    "href": "graph-theory-slides.html#the-story",
    "title": "Graph Theory for Machine Learning",
    "section": "The Story",
    "text": "The Story\nIn the 1970s, sociologist Wayne Zachary studied a university karate club.\n\nA conflict arose between the instructor (Mr. Hi) and the club president (John A).\n\n\nThe club split. Zachary had recorded who interacted with whom outside the club.\n\n\nThe question: Could the social network predict which faction each member would join?"
  },
  {
    "objectID": "graph-theory-slides.html#the-network",
    "href": "graph-theory-slides.html#the-network",
    "title": "Graph Theory for Machine Learning",
    "section": "The Network",
    "text": "The Network\n\n\nFigure 1: Zachary’s Karate Club network (34 members, 78 connections)"
  },
  {
    "objectID": "graph-theory-slides.html#what-the-network-reveals",
    "href": "graph-theory-slides.html#what-the-network-reveals",
    "title": "Graph Theory for Machine Learning",
    "section": "What the Network Reveals",
    "text": "What the Network Reveals\nVisual inspection already suggests structure:\n\nTwo dense regions with sparse connections between them\nNode 1 (Mr. Hi) and node 34 (John A) are hubs of their respective groups\n\n\nZachary’s result: The network correctly predicted 33 of 34 members’ choices.\n\n\nThe one “misclassified” member (node 9) had stronger ties to Mr. Hi’s group but followed his close friend to John A’s faction."
  },
  {
    "objectID": "graph-theory-slides.html#centrality-who-matters",
    "href": "graph-theory-slides.html#centrality-who-matters",
    "title": "Graph Theory for Machine Learning",
    "section": "Centrality: Who Matters?",
    "text": "Centrality: Who Matters?\nDegree centrality: How many connections does a node have?\nBetweenness centrality: How often does a node lie on shortest paths between others?\n\n\nFigure 2: Karate Club with nodes sized by betweenness centrality\nNode 1 (Mr. Hi) has highest betweenness—he’s the bridge to many members."
  },
  {
    "objectID": "graph-theory-slides.html#community-detection",
    "href": "graph-theory-slides.html#community-detection",
    "title": "Graph Theory for Machine Learning",
    "section": "Community Detection",
    "text": "Community Detection\nCan an algorithm recover the factions without knowing the ground truth?\n\n\nFigure 3: Communities detected by Louvain algorithm\nThe Louvain algorithm finds a partition very close to the actual split."
  },
  {
    "objectID": "graph-theory-slides.html#prediction-meets-understanding",
    "href": "graph-theory-slides.html#prediction-meets-understanding",
    "title": "Graph Theory for Machine Learning",
    "section": "Prediction Meets Understanding",
    "text": "Prediction Meets Understanding\nThe Karate Club illustrates the dual perspective:\n\nUnderstanding: The network structure reveals social cohesion patterns. Communities exist because people preferentially associate with similar others.\n\n\nPrediction: That same structure predicts future behavior. When forced to choose, people follow their network ties.\n\n\nThis is the power of graph analysis: structure is predictive because structure reflects mechanism."
  },
  {
    "objectID": "graph-theory-slides.html#the-setting",
    "href": "graph-theory-slides.html#the-setting",
    "title": "Graph Theory for Machine Learning",
    "section": "The Setting",
    "text": "The Setting\nMovieLens is a movie recommendation dataset from the GroupLens research lab.\n\nThe data: Users rate movies on a 1–5 scale.\n\n\nThe question: Given a user’s rating history, what movies should we recommend?"
  },
  {
    "objectID": "graph-theory-slides.html#bipartite-structure",
    "href": "graph-theory-slides.html#bipartite-structure",
    "title": "Graph Theory for Machine Learning",
    "section": "Bipartite Structure",
    "text": "Bipartite Structure\n\n\nFigure 4: A bipartite graph connecting users to the movies they rated.\nUsers and movies are different types of nodes. Edges connect users to movies they’ve rated."
  },
  {
    "objectID": "graph-theory-slides.html#the-recommendation-question",
    "href": "graph-theory-slides.html#the-recommendation-question",
    "title": "Graph Theory for Machine Learning",
    "section": "The Recommendation Question",
    "text": "The Recommendation Question\nCollaborative filtering idea: Users with similar taste will like similar movies.\n\nGraph formulation:\n\nIf User A and User B both rated Movie X highly…\n…and User B also rated Movie Y highly…\n…then recommend Movie Y to User A.\n\n\n\nThis is a path query: find movies reachable through similar users."
  },
  {
    "objectID": "graph-theory-slides.html#projection-user-similarity",
    "href": "graph-theory-slides.html#projection-user-similarity",
    "title": "Graph Theory for Machine Learning",
    "section": "Projection: User Similarity",
    "text": "Projection: User Similarity\nWe can “project” the bipartite graph onto users:\n\nTwo users are connected if they rated the same movie. This projection enables standard graph algorithms on user similarity.\n\n\n\n\n\n\n\n\nFigure 5: User projection: U1 and U2 both rated Forrest Gump (red), creating a projected edge (black)."
  },
  {
    "objectID": "graph-theory-slides.html#from-structure-to-prediction",
    "href": "graph-theory-slides.html#from-structure-to-prediction",
    "title": "Graph Theory for Machine Learning",
    "section": "From Structure to Prediction",
    "text": "From Structure to Prediction\nUnderstanding: The projected graph reveals user clusters—groups with similar taste.\n\nPrediction: Recommendations come from highly-connected neighbors in this similarity graph.\n\n\nThe insight: Recommendation is fundamentally about graph structure. Collaborative filtering asks: “What do my graph neighbors like that I haven’t seen?”"
  },
  {
    "objectID": "graph-theory-slides.html#knowledge-graphs",
    "href": "graph-theory-slides.html#knowledge-graphs",
    "title": "Graph Theory for Machine Learning",
    "section": "Knowledge Graphs",
    "text": "Knowledge Graphs\nA knowledge graph represents structured knowledge:\n\nNodes have types (Person, Skill, Course, Work-Role)\nEdges have labels (has_skill, requires, teaches)\nProperties attach to edges (proficiency level)\n\n\nThis is richer than a simple network: it encodes semantics."
  },
  {
    "objectID": "graph-theory-slides.html#the-learninggraph",
    "href": "graph-theory-slides.html#the-learninggraph",
    "title": "Graph Theory for Machine Learning",
    "section": "The LearningGraph",
    "text": "The LearningGraph\n\n\n\n\nTable 1: LearningGraph structure\n\n\n\n\n\n\nNode Types\nEdge Types\n\n\n\n\nLearner (6)\nhas_skill (learner → skill)\n\n\nSkill (18)\nrequires_skill (role → skill)\n\n\nWork Role (3)\nprerequisite (skill → skill)\n\n\nCourse (15)\nteaches (course → skill)\n\n\nCompetency (7)\nskill_in_competency (skill → competency)\n\n\n\n\n\n\n\n\nBased on the IC Data Science Competency Resource Guide (2023).\nInspired by Workera.ai’s skills-intelligence platform."
  },
  {
    "objectID": "graph-theory-slides.html#the-schema",
    "href": "graph-theory-slides.html#the-schema",
    "title": "Graph Theory for Machine Learning",
    "section": "The Schema",
    "text": "The Schema\n\n\n\n\nTable 2: LearningGraph edge schema\n\n\n\n\n\n\nEdge Type\nSource\nTarget\nProperty\nSemantics\n\n\n\n\nhas_skill\nlearner\nskill\nproficiency\ncurrent level\n\n\nrequires_skill\nwork_role\nskill\nproficiency\nminimum threshold\n\n\nprerequisite\nskill\nskill\n—\nconceptual dependency\n\n\nteaches\ncourse\nskill\nproficiency\nmaximum ceiling\n\n\n\n\n\n\n\n\nThe schema defines what assertions are well-formed—enabling both validation and inference."
  },
  {
    "objectID": "graph-theory-slides.html#skill-prerequisites-as-a-dag",
    "href": "graph-theory-slides.html#skill-prerequisites-as-a-dag",
    "title": "Graph Theory for Machine Learning",
    "section": "Skill Prerequisites as a DAG",
    "text": "Skill Prerequisites as a DAG\n\n\nFigure 6: Skills dependent on prerequisite skills, a Directed Acyclic Graph (DAG). Note: Courses teach these skills but may bundle them differently.\nEntry point skills: Programming, Data Collection, Probability Theory, Linear Algebra"
  },
  {
    "objectID": "graph-theory-slides.html#gap-analysis",
    "href": "graph-theory-slides.html#gap-analysis",
    "title": "Graph Theory for Machine Learning",
    "section": "Gap Analysis",
    "text": "Gap Analysis\nQuestion: What skills does Alice need to become a Data Scientist?\n\n\n\n\nTable 3\n\n\n\n\n\n\nSkill\nCurrent\nRequired\nGap\n\n\n\n\nExploratory Data Analysis\n1\n3\n2\n\n\nInference and Prediction\n0\n2\n2\n\n\nStatistical Learning\n0\n2\n2\n\n\nFeature Engineering\n0\n2\n2\n\n\nProblem Formulation\n0\n2\n2\n\n\nData Narratives\n0\n2\n2\n\n\nLimitations\n0\n2\n2\n\n\nProgramming\n1\n2\n1"
  },
  {
    "objectID": "graph-theory-slides.html#learning-paths-from-profile-to-goal",
    "href": "graph-theory-slides.html#learning-paths-from-profile-to-goal",
    "title": "Graph Theory for Machine Learning",
    "section": "Learning Paths: From Profile to Goal",
    "text": "Learning Paths: From Profile to Goal\nA learning path isn’t simply a route from one skill to another.\n\nRealistically, it’s a path from a learner’s current skill profile to a target skill profile.\n\n\nAlice’s question: Given everything I already know, what’s the shortest path to Statistical Learning?\n\n\n\n\n\nSkill\nProficiency\n\n\n\n\nExploratory Data Analysis\n1\n\n\nLinear Algebra\n3\n\n\nLinear Models\n2\n\n\nOptimization\n2\n\n\nProbability Theory\n3\n\n\nProgramming\n1"
  },
  {
    "objectID": "graph-theory-slides.html#computing-the-learning-path",
    "href": "graph-theory-slides.html#computing-the-learning-path",
    "title": "Graph Theory for Machine Learning",
    "section": "Computing the Learning Path",
    "text": "Computing the Learning Path\n\n\nSkills Alice must acquire:\n\n\n1. Inference and Prediction\n2. Statistical Learning\n\n\n\nThe algorithm finds all missing prerequisite skills and returns them in a valid learning order."
  },
  {
    "objectID": "graph-theory-slides.html#why-this-isnt-exponentially-complex",
    "href": "graph-theory-slides.html#why-this-isnt-exponentially-complex",
    "title": "Graph Theory for Machine Learning",
    "section": "Why This Isn’t Exponentially Complex",
    "text": "Why This Isn’t Exponentially Complex\nConcern: “There are too many possible skill profiles!”\n\nResolution: We don’t search profile space—we search the prerequisite DAG.\n\n\n\n\n\nApproach\nSearch Space\nComplexity\n\n\n\n\nAll profiles\n\\(2^{|skills|}\\)\nExponential\n\n\nDAG from current profile\nAncestors of target\n\\(O(V + E)\\)\n\n\n\n\n\nAlice’s profile defines her frontier, the skills adjacent to her current skill profile. We only traverse from there to the target."
  },
  {
    "objectID": "graph-theory-slides.html#visualizing-the-path",
    "href": "graph-theory-slides.html#visualizing-the-path",
    "title": "Graph Theory for Machine Learning",
    "section": "Visualizing the Path",
    "text": "Visualizing the Path\n\n\nFigure 7: Alice’s learning path to Statistical Learning\nGreen = skills Alice has. Red = skills to acquire. Purple = target."
  },
  {
    "objectID": "graph-theory-slides.html#different-profiles-different-paths",
    "href": "graph-theory-slides.html#different-profiles-different-paths",
    "title": "Graph Theory for Machine Learning",
    "section": "Different Profiles, Different Paths",
    "text": "Different Profiles, Different Paths\n\n\n\n\nTable 4: Learning paths vary by starting profile\n\n\n\n\n\n\nLearner\nSteps Required\n\n\n\n\nAlice\n2\n\n\nBeth\n3\n\n\nCharlie\n0\n\n\nDan\n1\n\n\n\n\n\n\n\n\n\nKey insight: Learning paths are properties of learner-skill pairs, not skills alone.\n\n\nThe DAG of prerequisite skills is fixed; the learner’s position on it varies. The path to Statistical Learning doesn’t exist—only Alice’s path, Bob’s path, Carol’s path."
  },
  {
    "objectID": "graph-theory-slides.html#the-algorithm",
    "href": "graph-theory-slides.html#the-algorithm",
    "title": "Graph Theory for Machine Learning",
    "section": "The Algorithm",
    "text": "The Algorithm\nlg_learning_path &lt;- function(lg, learner_id, target_skill) {\n \n  # 1. Get learner's current skills\n  current &lt;- get_learner_skills(lg, learner_id)\n \n  # 2. Find all ancestors of target (transitive prerequisites)\n  ancestors &lt;- igraph::subcomponent(prereq_graph, target, mode = \"in\")\n \n  # 3. Filter to skills learner doesn't have\n  missing &lt;- setdiff(ancestors, current)\n \n  # 4. Return in topological order (valid learning sequence)\n  igraph::topo_sort(igraph::induced_subgraph(missing))\n}\nThis is graph reachability, not combinatorial search."
  },
  {
    "objectID": "graph-theory-slides.html#real-world-application-workera",
    "href": "graph-theory-slides.html#real-world-application-workera",
    "title": "Graph Theory for Machine Learning",
    "section": "Real-World Application: Workera",
    "text": "Real-World Application: Workera\nThis isn’t hypothetical. Companies like Workera (co-founded by Kian Katanforoosh, with Andrew Ng as chairman) have built businesses on exactly this structure.\n\n\n\n\nWorkera Capability\nGraph Interpretation\n\n\n\n\n3,000+ micro-skills\nFine-grained skill nodes\n\n\nPersonalized learning paths\nShortest path through prerequisites\n\n\nSkill gap assessment\nCompare current vs. required subgraphs\n\n\n“Sage” AI mentor\nLLM grounded by knowledge graph"
  },
  {
    "objectID": "graph-theory-slides.html#the-dual-perspective-again",
    "href": "graph-theory-slides.html#the-dual-perspective-again",
    "title": "Graph Theory for Machine Learning",
    "section": "The Dual Perspective Again",
    "text": "The Dual Perspective Again\nUnderstanding: The knowledge graph encodes domain expertise—what skills exist, how they relate, and the work-roles that require them.\n\nPrediction/Decision Support: Given where a learner is, what should they learn next? What’s the most efficient path to their goal?\n\n\nThis knowledge graph goes beyond storing knowledge; it enables reasoning about learning paths.\n\n\nWith these examples as motivation, we now turn to the formal concepts underlying graph analysis."
  },
  {
    "objectID": "graph-theory-slides.html#nodes-and-edges",
    "href": "graph-theory-slides.html#nodes-and-edges",
    "title": "Graph Theory for Machine Learning",
    "section": "Nodes and Edges",
    "text": "Nodes and Edges\nA graph \\(G = (V, E)\\) consists of:\n\n\\(V\\) = set of vertices (nodes)\n\\(E\\) = set of edges (connections between nodes)\n\n\nUndirected: Edge \\(\\{u, v\\}\\) is symmetric (friendship)\nDirected: Edge \\((u, v)\\) has direction (follows, links to)\n\n\nWeighted: Edges carry numerical values (distance, strength)\nLabeled: Edges have types (prerequisite, teaches, knows)"
  },
  {
    "objectID": "graph-theory-slides.html#matrix-representations",
    "href": "graph-theory-slides.html#matrix-representations",
    "title": "Graph Theory for Machine Learning",
    "section": "Matrix Representations",
    "text": "Matrix Representations\nThe adjacency matrix \\(A\\) encodes connections:\n\\[A_{ij} = \\begin{cases} 1 & \\text{if edge } (i,j) \\in E \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nThe degree matrix \\(D\\) is diagonal:\n\\[D_{ii} = \\sum_j A_{ij} = \\text{degree of node } i\\]\n\n\nThe Laplacian \\(L = D - A\\) has deep connections to graph structure."
  },
  {
    "objectID": "graph-theory-slides.html#paths-and-connectivity",
    "href": "graph-theory-slides.html#paths-and-connectivity",
    "title": "Graph Theory for Machine Learning",
    "section": "Paths and Connectivity",
    "text": "Paths and Connectivity\nA path is a sequence of edges connecting two nodes.\n\nShortest path: The path with minimum total weight (or fewest edges).\n\n\nA graph is connected if a path exists between every pair of nodes.\nConnected components: Maximal connected subgraphs.\n\n\nA directed acyclic graph (DAG) has no cycles—crucial for prerequisite structures."
  },
  {
    "objectID": "graph-theory-slides.html#centrality-who-matters-1",
    "href": "graph-theory-slides.html#centrality-who-matters-1",
    "title": "Graph Theory for Machine Learning",
    "section": "Centrality: Who Matters?",
    "text": "Centrality: Who Matters?\nDifferent questions → different centrality measures:\n\n\n\nMeasure\nQuestion\nKarate Club Leader\n\n\n\n\nDegree\nWho has the most connections?\nMr. Hi\n\n\nBetweenness\nWho controls information flow?\nMr. Hi\n\n\nCloseness\nWho can reach everyone quickly?\nCentral members\n\n\nPageRank\nWho is endorsed by important others?\nBoth leaders"
  },
  {
    "objectID": "graph-theory-slides.html#betweenness-centrality",
    "href": "graph-theory-slides.html#betweenness-centrality",
    "title": "Graph Theory for Machine Learning",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\n\\[\\gamma(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\\]\nwhere \\(\\sigma_{st}\\) = number of shortest paths from \\(s\\) to \\(t\\)\nand \\(\\sigma_{st}(v)\\) = number of those paths passing through \\(v\\)\n\nInterpretation: Nodes with high betweenness are bridges—removing them fragments the network."
  },
  {
    "objectID": "graph-theory-slides.html#pagerank",
    "href": "graph-theory-slides.html#pagerank",
    "title": "Graph Theory for Machine Learning",
    "section": "PageRank",
    "text": "PageRank\nOriginally for ranking web pages. The idea:\n\nA page is important if important pages link to it.\n\n\nRandom surfer model: Follow links randomly, occasionally jump to a random page. PageRank = long-run fraction of time at each node.\n\n\n\\[\\text{PR}(v) = \\frac{1-d}{N} + d \\sum_{u \\to v} \\frac{\\text{PR}(u)}{\\text{out-degree}(u)}\\]\nwhere \\(d \\approx 0.85\\) is the damping factor, and out-degree counts out-going edges."
  },
  {
    "objectID": "graph-theory-slides.html#communities",
    "href": "graph-theory-slides.html#communities",
    "title": "Graph Theory for Machine Learning",
    "section": "Communities",
    "text": "Communities\nA community is a group of nodes more densely connected internally than externally.\n\nModularity \\(Q\\) measures community quality by comparing actual edges to expectation:\n\\[Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)\\]\nwhere \\(m\\) = total edges, \\(k_i\\) = degree of node \\(i\\), and \\(\\delta(c_i, c_j) = 1\\) if \\(i, j\\) share a community.\n\n\n\\(Q &gt; 0.3\\) typically indicates significant community structure."
  },
  {
    "objectID": "graph-theory-slides.html#community-detection-algorithms",
    "href": "graph-theory-slides.html#community-detection-algorithms",
    "title": "Graph Theory for Machine Learning",
    "section": "Community Detection Algorithms",
    "text": "Community Detection Algorithms\n\n\n\n\n\n\n\n\nAlgorithm\nApproach\nComplexity\n\n\n\n\nGirvan-Newman\nIteratively remove high-betweenness edges\nSlow, interpretable\n\n\nLouvain\nGreedily optimize modularity\nFast, widely used\n\n\nSpectral\nEigenvectors of Laplacian\nPrincipled; minimizes edges between groups\n\n\nLeiden\nImproved Louvain, allows overlaps\nState of the art\n\n\n\n\nNo single “best” algorithm—choice depends on graph size, structure, and goals."
  },
  {
    "objectID": "graph-theory-slides.html#spectral-clustering-intuition",
    "href": "graph-theory-slides.html#spectral-clustering-intuition",
    "title": "Graph Theory for Machine Learning",
    "section": "Spectral Clustering Intuition",
    "text": "Spectral Clustering Intuition\nThe graph Laplacian \\(L = D - A\\) encodes connectivity.\n\nIts eigenvectors reveal structure:\n\nFirst eigenvector: constant (trivial)\nSecond eigenvector: signs indicate two-way partition\nMore eigenvectors: finer partitions\n\n\n\nFiedler vector (second eigenvector) provides a principled way to split a graph."
  },
  {
    "objectID": "graph-theory-slides.html#graphs-as-feature-sources",
    "href": "graph-theory-slides.html#graphs-as-feature-sources",
    "title": "Graph Theory for Machine Learning",
    "section": "Graphs as Feature Sources",
    "text": "Graphs as Feature Sources\nTraditional ML on graphs: extract features, then apply standard ML methods to features.\n\nNode features:\n\nDegree, centrality measures\nCommunity membership\nLocal clustering coefficient\n\n\n\nGraph features:\n\nDensity, diameter\nDegree distribution\nModularity"
  },
  {
    "objectID": "graph-theory-slides.html#link-prediction",
    "href": "graph-theory-slides.html#link-prediction",
    "title": "Graph Theory for Machine Learning",
    "section": "Link Prediction",
    "text": "Link Prediction\nQuestion: Which pairs of unconnected nodes will connect in the future?\n\nCommon neighbors: \\(|N(u) \\cap N(v)|\\)\nJaccard coefficient: \\(\\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}\\)\nAdamic-Adar: \\(\\sum_{w \\in N(u) \\cap N(v)} \\frac{1}{\\log |N(w)|}\\)\n\n\nThese are graph-derived features for a classification problem."
  },
  {
    "objectID": "graph-theory-slides.html#gnn-graph-neural-networks-brief",
    "href": "graph-theory-slides.html#gnn-graph-neural-networks-brief",
    "title": "Graph Theory for Machine Learning",
    "section": "GNN: Graph Neural Networks (Brief)",
    "text": "GNN: Graph Neural Networks (Brief)\nModern approach: learn node representations directly from graph structure.\n\nMessage passing: Nodes aggregate information from neighbors, iteratively.\n\n\n\\[h_v^{(k+1)} = \\text{UPDATE}\\left(h_v^{(k)}, \\text{AGGREGATE}\\left(\\{h_u^{(k)} : u \\in N(v)\\}\\right)\\right)\\]\n\n\nGNNs can learn features automatically—but require substantial data and compute.\nFor many problems, classical graph features be preferred: easier to interpret, and less compute-intensive."
  },
  {
    "objectID": "graph-theory-slides.html#structure-enables-both-goals",
    "href": "graph-theory-slides.html#structure-enables-both-goals",
    "title": "Graph Theory for Machine Learning",
    "section": "Structure Enables Both Goals",
    "text": "Structure Enables Both Goals\n\n\n\n\n\n\n\n\nPerspective\nMethods\nQuestions\n\n\n\n\nUnderstanding\nCentrality, communities, components\nWho matters? What groups exist? How does information flow?\n\n\nPrediction\nLink prediction, recommendation, node classification\nWhat will happen? What should we recommend?\n\n\n\n\nThe same graph structure serves both: understanding why and predicting what."
  },
  {
    "objectID": "graph-theory-slides.html#from-social-to-semantic",
    "href": "graph-theory-slides.html#from-social-to-semantic",
    "title": "Graph Theory for Machine Learning",
    "section": "From Social to Semantic",
    "text": "From Social to Semantic\nOur three examples show increasing semantic richness:\n\n\n\n\n\n\n\n\n\nExample\nNodes\nEdges\nSemantics\n\n\n\n\nKarate Club\nPeople\nInteracts\nHomogeneous\n\n\nMovieLens\nUsers, Movies\nRates\nBipartite, weighted\n\n\nLearningGraph\nLearners, Skills, Roles, Courses\nMultiple types\nKnowledge graph\n\n\n\n\nRicher semantics → richer reasoning, but also more complex modeling."
  },
  {
    "objectID": "graph-theory-slides.html#key-takeaways",
    "href": "graph-theory-slides.html#key-takeaways",
    "title": "Graph Theory for Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nGraphs encode relationships—and relationships are information.\nStructure reveals mechanism—communities exist because of homophily; centrality reflects influence pathways.\nThe same structure serves prediction and understanding—predicting future connections exploits the patterns that community detection reveals.\nKnowledge graphs add semantics—typed nodes and labeled edges enable richer queries and reasoning."
  },
  {
    "objectID": "graph-theory-slides.html#what-we-covered",
    "href": "graph-theory-slides.html#what-we-covered",
    "title": "Graph Theory for Machine Learning",
    "section": "What We Covered",
    "text": "What We Covered\n\nThree examples: Karate Club, MovieLens, LearningGraph\nFundamental concepts: Nodes, edges, matrices, paths\nCentrality measures: Degree, betweenness, PageRank\nCommunity detection: Modularity, Louvain, spectral methods\nML connections: Graph-derived features, predicting new edges, GNNs"
  },
  {
    "objectID": "graph-theory-slides.html#key-r-packages",
    "href": "graph-theory-slides.html#key-r-packages",
    "title": "Graph Theory for Machine Learning",
    "section": "Key R Packages",
    "text": "Key R Packages\n\n\n\nPackage\nPurpose\n\n\n\n\nigraph\nGraph creation, algorithms, analysis\n\n\ntidygraph\nTidy interface to igraph\n\n\nggraph\nggplot2-style visualization\n\n\nigraphdata\nBuilt-in datasets"
  },
  {
    "objectID": "graph-theory-slides.html#resources",
    "href": "graph-theory-slides.html#resources",
    "title": "Graph Theory for Machine Learning",
    "section": "Resources",
    "text": "Resources\nTextbooks:\n\nNewman, Networks (2nd ed., 2018) — comprehensive reference\nBarabási, Network Science (2016) — free online, beautifully illustrated\nEasley & Kleinberg, Networks, Crowds, and Markets (2010) — interdisciplinary\n\nTutorial:\n\nOgnyanova, “Network Visualization with R” (kateto.net)"
  },
  {
    "objectID": "graph-theory-slides.html#team-exercise-1-karate-club-centrality",
    "href": "graph-theory-slides.html#team-exercise-1-karate-club-centrality",
    "title": "Graph Theory for Machine Learning",
    "section": "Team Exercise 1: Karate Club Centrality",
    "text": "Team Exercise 1: Karate Club Centrality\nLoad the Karate Club network using igraph::make_graph(\"Zachary\"):\n\nCompute betweenness centrality for all nodes.\nWhich node has the highest betweenness?\nWhy does this make sense sociologically—what role does this node play?\nCompare to degree centrality. Do the rankings agree?"
  },
  {
    "objectID": "graph-theory-slides.html#team-exercise-2-community-detection",
    "href": "graph-theory-slides.html#team-exercise-2-community-detection",
    "title": "Graph Theory for Machine Learning",
    "section": "Team Exercise 2: Community Detection",
    "text": "Team Exercise 2: Community Detection\nApply the Louvain algorithm (igraph::cluster_louvain()) to the Karate Club:\n\nHow many communities does it find?\nCompare the detected communities to the known faction split (Mr. Hi vs. John A).\nWhich nodes, if any, are “misclassified”? Why might this be?\nTry Walktrap (igraph::cluster_walktrap()). Do the results differ?"
  },
  {
    "objectID": "graph-theory-slides.html#team-exercise-3-learninggraph-exploration",
    "href": "graph-theory-slides.html#team-exercise-3-learninggraph-exploration",
    "title": "Graph Theory for Machine Learning",
    "section": "Team Exercise 3: LearningGraph Exploration",
    "text": "Team Exercise 3: LearningGraph Exploration\nInstall eda4mldata from GitHub and load learning_graph:\n\nBuild the skill prerequisite graph from learning_graph$edges$prerequisite.\nVerify that it is acyclic. What would a cycle mean pedagogically?\nFind the skill with the most prerequisites (highest in-degree).\nFind the skill that is prerequisite to the most others (highest out-degree)."
  },
  {
    "objectID": "graph-theory-slides.html#team-exercise-4-bipartite-projection",
    "href": "graph-theory-slides.html#team-exercise-4-bipartite-projection",
    "title": "Graph Theory for Machine Learning",
    "section": "Team Exercise 4: Bipartite Projection",
    "text": "Team Exercise 4: Bipartite Projection\nCreate a bipartite graph: students × courses they’ve taken.\n\nAlice: Math, Stats, Programming\nBob: Stats, Programming, ML\nCarol: Math, Stats\nDan: Programming, ML\n\n\nProject onto students (edges connect students sharing courses).\nWhat are the edge weights? Which pair is most similar?\nProject onto courses. What does an edge between two courses mean?\nWhen is bipartite projection useful in practice?\n\n\nExercise 1 focuses on centrality interpretation. Exercise 2 practices community detection. Exercise 3 uses the textbook’s LearningGraph. Exercise 4 covers bipartite graphs."
  },
  {
    "objectID": "graph-theory-slides.html#discussion-questions",
    "href": "graph-theory-slides.html#discussion-questions",
    "title": "Graph Theory for Machine Learning",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nSocial networks, citation networks, and neural networks are all called “networks.” What do they share? How do they differ?\nWhen would betweenness centrality be more informative than degree centrality?\nHow do graph-derived features differ from tabular features for machine learning?"
  },
  {
    "objectID": "conditioning-slides.html#the-central-question",
    "href": "conditioning-slides.html#the-central-question",
    "title": "Conditional Distributions",
    "section": "The Central Question",
    "text": "The Central Question\nGiven information about one variable, what can we say about another?\n\nExample: If we know a father’s height, what can we predict about his son’s height?\n\n\nThis leads to the concept of conditional distributions."
  },
  {
    "objectID": "conditioning-slides.html#father-son-heights-review",
    "href": "conditioning-slides.html#father-son-heights-review",
    "title": "Conditional Distributions",
    "section": "Father-Son Heights: Review",
    "text": "Father-Son Heights: Review\n\n\nFigure 1: Heights of father-son pairs"
  },
  {
    "objectID": "conditioning-slides.html#conditional-distribution-of-sons-height",
    "href": "conditioning-slides.html#conditional-distribution-of-sons-height",
    "title": "Conditional Distributions",
    "section": "Conditional Distribution of Son’s Height",
    "text": "Conditional Distribution of Son’s Height\n\n\nFigure 2: Son’s height distribution for each father height interval"
  },
  {
    "objectID": "conditioning-slides.html#conditional-expectation",
    "href": "conditioning-slides.html#conditional-expectation",
    "title": "Conditional Distributions",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nThe conditional expectation \\(E(Y | X)\\) is the average value of \\(Y\\) given \\(X\\).\n\n\n\n\n\nTable 1: Average son’s height per father’s height interval\n\n\n\n\n\n\nFather height\nn\nMean son height\n\n\n\n\n59\n4\n64.7\n\n\n61\n16\n65.5\n\n\n63\n77\n66.3\n\n\n65\n208\n67.5\n\n\n67\n276\n68.2\n\n\n69\n275\n69.5\n\n\n71\n152\n70.2\n\n\n73\n63\n71.4\n\n\n75\n7\n71.6"
  },
  {
    "objectID": "conditioning-slides.html#the-graph-of-averages",
    "href": "conditioning-slides.html#the-graph-of-averages",
    "title": "Conditional Distributions",
    "section": "The Graph of Averages",
    "text": "The Graph of Averages\n\n\nFigure 3: Conditional mean of son’s height given father’s height"
  },
  {
    "objectID": "conditioning-slides.html#key-observation",
    "href": "conditioning-slides.html#key-observation",
    "title": "Conditional Distributions",
    "section": "Key Observation",
    "text": "Key Observation\nThe graph of averages is approximately linear.\n\nThis suggests we can approximate the conditional expectation with a straight line.\n\n\n→ The regression line"
  },
  {
    "objectID": "conditioning-slides.html#standardizing-variables",
    "href": "conditioning-slides.html#standardizing-variables",
    "title": "Conditional Distributions",
    "section": "Standardizing Variables",
    "text": "Standardizing Variables\nTo compare variables on different scales, convert to standard units (z-scores):\n\\[Z_x = \\frac{X - \\mu_x}{\\sigma_x}\\]\n\nInterpretation: Number of standard deviations above or below the mean."
  },
  {
    "objectID": "conditioning-slides.html#sample-z-scores",
    "href": "conditioning-slides.html#sample-z-scores",
    "title": "Conditional Distributions",
    "section": "Sample Z-Scores",
    "text": "Sample Z-Scores\nIn practice, we use sample estimates:\n\\[\\hat{Z}_x(x_k) = \\frac{x_k - \\bar{x}}{s_x}\\]\n\nwhere \\(\\bar{x}\\) is the sample mean and \\(s_x\\) is the sample standard deviation."
  },
  {
    "objectID": "conditioning-slides.html#the-sd-line",
    "href": "conditioning-slides.html#the-sd-line",
    "title": "Conditional Distributions",
    "section": "The SD Line",
    "text": "The SD Line\nThe SD line passes through the point of averages with slope \\(\\frac{s_y}{s_x}\\):\n\\[y = \\bar{y} + \\frac{s_y}{s_x}(x - \\bar{x})\\]\n\nEquivalently: \\(\\hat{Z}_y = \\hat{Z}_x\\)\n\n\nProperty: Minimizes sum of squared perpendicular distances to points."
  },
  {
    "objectID": "conditioning-slides.html#the-regression-line",
    "href": "conditioning-slides.html#the-regression-line",
    "title": "Conditional Distributions",
    "section": "The Regression Line",
    "text": "The Regression Line\nThe regression line has slope \\(r \\cdot \\frac{s_y}{s_x}\\):\n\\[y = \\bar{y} + r \\frac{s_y}{s_x}(x - \\bar{x})\\]\n\nEquivalently: \\(\\hat{Z}_y = r \\cdot \\hat{Z}_x\\)\n\n\nProperty: Minimizes sum of squared vertical distances to points."
  },
  {
    "objectID": "conditioning-slides.html#comparing-the-two-lines",
    "href": "conditioning-slides.html#comparing-the-two-lines",
    "title": "Conditional Distributions",
    "section": "Comparing the Two Lines",
    "text": "Comparing the Two Lines\n\n\nFigure 4: Regression line (blue) vs SD line (red)"
  },
  {
    "objectID": "conditioning-slides.html#why-the-difference",
    "href": "conditioning-slides.html#why-the-difference",
    "title": "Conditional Distributions",
    "section": "Why the Difference?",
    "text": "Why the Difference?\nBoth lines pass through \\((\\bar{x}, \\bar{y})\\).\n\nThe regression line is less steep because \\(|r| \\le 1\\).\n\n\nThis is the mathematical basis of regression to the mean."
  },
  {
    "objectID": "conditioning-slides.html#the-correlation-coefficient",
    "href": "conditioning-slides.html#the-correlation-coefficient",
    "title": "Conditional Distributions",
    "section": "The Correlation Coefficient",
    "text": "The Correlation Coefficient\n\\[r = \\frac{1}{n-1} \\sum_{k=1}^{n} \\hat{Z}_x(x_k) \\cdot \\hat{Z}_y(y_k)\\]\n\nProperties:\n\n\\(-1 \\le r \\le 1\\)\n\\(r = \\pm 1\\) only if points fall exactly on a line\n\\(r = 0\\) means no linear association"
  },
  {
    "objectID": "conditioning-slides.html#father-son-correlation",
    "href": "conditioning-slides.html#father-son-correlation",
    "title": "Conditional Distributions",
    "section": "Father-Son Correlation",
    "text": "Father-Son Correlation\n\n\n\n\nTable 2\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFather mean\n67.70\n\n\nSon mean\n68.70\n\n\nFather SD\n2.74\n\n\nSon SD\n2.81\n\n\nCorrelation r\n0.50\n\n\n\n\n\n\n\n\n\nWith \\(r \\approx 0.5\\), the regression line has about half the slope of the SD line."
  },
  {
    "objectID": "conditioning-slides.html#fitted-values-and-residuals",
    "href": "conditioning-slides.html#fitted-values-and-residuals",
    "title": "Conditional Distributions",
    "section": "Fitted Values and Residuals",
    "text": "Fitted Values and Residuals\nFor each observation:\n\nFitted value: \\(\\hat{y}_k = \\bar{y} + r \\frac{s_y}{s_x}(x_k - \\bar{x})\\)\nResidual: \\(e_k = y_k - \\hat{y}_k\\)\n\n\nResiduals measure how far each point falls from the regression line."
  },
  {
    "objectID": "conditioning-slides.html#distribution-of-residuals",
    "href": "conditioning-slides.html#distribution-of-residuals",
    "title": "Conditional Distributions",
    "section": "Distribution of Residuals",
    "text": "Distribution of Residuals\n\n\nFigure 5: Histogram of regression residuals"
  },
  {
    "objectID": "conditioning-slides.html#residuals-should-be",
    "href": "conditioning-slides.html#residuals-should-be",
    "title": "Conditional Distributions",
    "section": "Residuals Should Be…",
    "text": "Residuals Should Be…\n\nCentered around zero ✓\nRoughly symmetric ✓\nNo pattern when plotted against \\(x\\) or \\(\\hat{y}\\)\n\n\nPatterns in residuals suggest the model is missing something."
  },
  {
    "objectID": "conditioning-slides.html#a-special-case",
    "href": "conditioning-slides.html#a-special-case",
    "title": "Conditional Distributions",
    "section": "A Special Case",
    "text": "A Special Case\nIf \\((X, Y)\\) follows a bivariate normal distribution:\n\n\nThe conditional distribution \\(Y | X\\) is normal\nThe conditional mean \\(E(Y|X)\\) is exactly the regression line\nThe conditional SD is \\(\\sigma_y \\sqrt{1 - r^2}\\)\n\n\n\nFather-son heights are well approximated by a bivariate normal."
  },
  {
    "objectID": "conditioning-slides.html#variance-reduction",
    "href": "conditioning-slides.html#variance-reduction",
    "title": "Conditional Distributions",
    "section": "Variance Reduction",
    "text": "Variance Reduction\nConditioning on \\(X\\) reduces the variance of \\(Y\\):\n\\[\\text{Var}(Y|X) = \\sigma_y^2 (1 - r^2)\\]\n\nFor father-son data with \\(r \\approx 0.5\\):\n\\[\\sqrt{1 - r^2} \\approx 0.87\\]\n\n\nKnowing father’s height reduces son’s height SD by about 13%."
  },
  {
    "objectID": "conditioning-slides.html#robust-statistics",
    "href": "conditioning-slides.html#robust-statistics",
    "title": "Conditional Distributions",
    "section": "Robust Statistics",
    "text": "Robust Statistics\nThe mean and SD are sensitive to outliers.\n\nAlternatives:\n\n\n\nSensitive\nRobust\n\n\n\n\nMean\nMedian\n\n\nStandard deviation\nIQR\n\n\n\n\n\nThe regression line inherits this sensitivity."
  },
  {
    "objectID": "conditioning-slides.html#anscombes-quartet",
    "href": "conditioning-slides.html#anscombes-quartet",
    "title": "Conditional Distributions",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\nFour data sets with identical summary statistics:\n\nSame means, SDs, and correlation\nSame regression line\n\n\nYet the data look completely different!"
  },
  {
    "objectID": "conditioning-slides.html#anscombes-quartet-the-data",
    "href": "conditioning-slides.html#anscombes-quartet-the-data",
    "title": "Conditional Distributions",
    "section": "Anscombe’s Quartet: The Data",
    "text": "Anscombe’s Quartet: The Data\n\n\nFigure 6: Four data sets with identical regression statistics"
  },
  {
    "objectID": "conditioning-slides.html#the-lesson",
    "href": "conditioning-slides.html#the-lesson",
    "title": "Conditional Distributions",
    "section": "The Lesson",
    "text": "The Lesson\nAlways visualize your data!\n\nSummary statistics can hide:\n\nNonlinear relationships\nOutliers\nClusters\nData errors"
  },
  {
    "objectID": "conditioning-slides.html#definition",
    "href": "conditioning-slides.html#definition",
    "title": "Conditional Distributions",
    "section": "Definition",
    "text": "Definition\nRandom variables \\((X, Y)\\) are independent if:\n\\[P(X \\in A, Y \\in B) = P(X \\in A) \\cdot P(Y \\in B)\\]\nfor all sets \\(A\\) and \\(B\\).\n\nImplication: Knowing \\(X\\) tells you nothing about \\(Y\\)."
  },
  {
    "objectID": "conditioning-slides.html#independence-and-correlation",
    "href": "conditioning-slides.html#independence-and-correlation",
    "title": "Conditional Distributions",
    "section": "Independence and Correlation",
    "text": "Independence and Correlation\nIf \\((X, Y)\\) are independent, then \\(r = 0\\).\n\nBut the converse is false!\n\n\n\\(r = 0\\) only means no linear association.\nVariables can be dependent but uncorrelated."
  },
  {
    "objectID": "conditioning-slides.html#testing-independence-categorical-variables",
    "href": "conditioning-slides.html#testing-independence-categorical-variables",
    "title": "Conditional Distributions",
    "section": "Testing Independence: Categorical Variables",
    "text": "Testing Independence: Categorical Variables\nFor categorical variables, use the chi-squared test.\n\nIdea: Compare observed counts to expected counts under independence.\n\\[\\chi^2 = \\sum_{j,k} \\frac{(O_{jk} - E_{jk})^2}{E_{jk}}\\]"
  },
  {
    "objectID": "conditioning-slides.html#example-handedness-and-sex",
    "href": "conditioning-slides.html#example-handedness-and-sex",
    "title": "Conditional Distributions",
    "section": "Example: Handedness and Sex",
    "text": "Example: Handedness and Sex\n\n\n\n\nTable 3\n\n\n\n\n\n\nhandedness\nmale\nfemale\n\n\n\n\nright\n934\n1070\n\n\nleft\n113\n92\n\n\nambi\n20\n8\n\n\n\n\n\n\n\n\n\nIs handedness independent of sex?"
  },
  {
    "objectID": "conditioning-slides.html#chi-squared-test-result",
    "href": "conditioning-slides.html#chi-squared-test-result",
    "title": "Conditional Distributions",
    "section": "Chi-Squared Test Result",
    "text": "Chi-Squared Test Result\n\\(\\chi^2 =\\) 11.8, df = 2, p-value = 0.0027\n\nConclusion: Strong evidence against independence.\nMales are more likely to be left-handed or ambidextrous."
  },
  {
    "objectID": "conditioning-slides.html#uc-berkeley-admissions-1973",
    "href": "conditioning-slides.html#uc-berkeley-admissions-1973",
    "title": "Conditional Distributions",
    "section": "UC Berkeley Admissions (1973)",
    "text": "UC Berkeley Admissions (1973)\n\n\n\n\nTable 4\n\n\n\n\n\n\nSex\nAdmission Rate\n\n\n\n\nMale\n44.5%\n\n\nFemale\n30.4%\n\n\n\n\n\n\n\n\n\nThis looks like clear evidence of bias against women."
  },
  {
    "objectID": "conditioning-slides.html#but-wait",
    "href": "conditioning-slides.html#but-wait",
    "title": "Conditional Distributions",
    "section": "But Wait…",
    "text": "But Wait…\n\n\n\n\nTable 5\n\n\n\n\n\n\nDept\nMale %\nFemale %\n\n\n\n\nA\n62\n82\n\n\nB\n63\n68\n\n\nC\n37\n34\n\n\nD\n33\n35\n\n\nE\n28\n24\n\n\nF\n6\n7\n\n\n\n\n\n\n\n\n\nFour of six departments admitted women at higher rates!"
  },
  {
    "objectID": "conditioning-slides.html#what-happened",
    "href": "conditioning-slides.html#what-happened",
    "title": "Conditional Distributions",
    "section": "What Happened?",
    "text": "What Happened?\nWomen applied disproportionately to departments with low overall admission rates.\n\nDepartment is a confounding variable.\n\n\nThe aggregate pattern reverses when we condition on department."
  },
  {
    "objectID": "conditioning-slides.html#simpsons-paradox-the-lesson",
    "href": "conditioning-slides.html#simpsons-paradox-the-lesson",
    "title": "Conditional Distributions",
    "section": "Simpson’s Paradox: The Lesson",
    "text": "Simpson’s Paradox: The Lesson\nA pattern in aggregated data can reverse when data are disaggregated by a relevant variable.\n\nAlways ask: Is there a confounding variable I’m missing?"
  },
  {
    "objectID": "conditioning-slides.html#what-weve-covered",
    "href": "conditioning-slides.html#what-weve-covered",
    "title": "Conditional Distributions",
    "section": "What We’ve Covered",
    "text": "What We’ve Covered\n\n\n\nVariables\nMeasure\n\n\n\n\nBoth continuous\nCorrelation \\(r\\)\n\n\nBoth categorical\nChi-squared \\(\\chi^2\\)\n\n\n\n\nBoth measure departure from independence."
  },
  {
    "objectID": "conditioning-slides.html#looking-ahead",
    "href": "conditioning-slides.html#looking-ahead",
    "title": "Conditional Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nInformation theory provides an alternative framework:\n\nEntropy\nMutual information\nKL divergence\n\n\nThese capture nonlinear relationships that correlation might miss.\n→ Chapter 6"
  },
  {
    "objectID": "conditioning-slides.html#chapter-2-key-takeaways",
    "href": "conditioning-slides.html#chapter-2-key-takeaways",
    "title": "Conditional Distributions",
    "section": "Chapter 2: Key Takeaways",
    "text": "Chapter 2: Key Takeaways\n\nConditional distributions show how one variable varies given another\nThe graph of averages can be approximated by the regression line\nCorrelation measures linear association; \\(r = 0\\) doesn’t mean independence\nAlways visualize — summary statistics can deceive (Anscombe)\nSimpson’s paradox reminds us to look for confounders"
  },
  {
    "objectID": "conditioning-slides.html#key-formulas",
    "href": "conditioning-slides.html#key-formulas",
    "title": "Conditional Distributions",
    "section": "Key Formulas",
    "text": "Key Formulas\n\n\n\nConcept\nFormula\n\n\n\n\nZ-score\n\\(\\hat{Z}_x = \\frac{x - \\bar{x}}{s_x}\\)\n\n\nRegression line\n\\(\\hat{Z}_y = r \\cdot \\hat{Z}_x\\)\n\n\nCorrelation\n\\(r = \\frac{1}{n-1}\\sum \\hat{Z}_x \\hat{Z}_y\\)\n\n\nChi-squared\n\\(\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\)"
  },
  {
    "objectID": "conditioning-slides.html#team-exercise-1-bivariate-normal-construction",
    "href": "conditioning-slides.html#team-exercise-1-bivariate-normal-construction",
    "title": "Conditional Distributions",
    "section": "Team Exercise 1: Bivariate Normal Construction",
    "text": "Team Exercise 1: Bivariate Normal Construction\nGiven independent standard normal \\(X\\) and \\(Z\\), and correlation \\(r\\):\n\nConstruct \\(Y = rX + \\sqrt{1 - r^2} Z\\)\nWhat are the unconditional mean and SD of \\(Y\\)?\nWhat is \\(\\text{Cor}(X, Y)\\)?\nHow would you generalize to arbitrary means \\((\\mu_x, \\mu_y)\\) and SDs \\((\\sigma_x, \\sigma_y)\\)?"
  },
  {
    "objectID": "conditioning-slides.html#team-exercise-2-simpsons-paradox",
    "href": "conditioning-slides.html#team-exercise-2-simpsons-paradox",
    "title": "Conditional Distributions",
    "section": "Team Exercise 2: Simpson’s Paradox",
    "text": "Team Exercise 2: Simpson’s Paradox\nThe UC Berkeley admissions example showed an aggregate bias that reversed within departments.\n\nAs a team, construct a different example of Simpson’s paradox (can be hypothetical).\nWhat is the lurking variable in your example?\nWhich analysis gives the “correct” answer—aggregated or disaggregated?"
  },
  {
    "objectID": "conditioning-slides.html#team-exercise-3-correlation-vs.-independence",
    "href": "conditioning-slides.html#team-exercise-3-correlation-vs.-independence",
    "title": "Conditional Distributions",
    "section": "Team Exercise 3: Correlation vs. Independence",
    "text": "Team Exercise 3: Correlation vs. Independence\nConstruct an example where \\(X\\) and \\(Y\\) are statistically dependent but have \\(r = 0\\).\n\nSketch the joint distribution of \\((X, Y)\\).\nWhy does correlation fail to detect the dependence?\nWhat does this imply for feature selection in machine learning?\n\n\nExercise 1 builds simulation skills. Exercise 2 reinforces the importance of conditioning. Exercise 3 connects to mutual information (Chapter 6)."
  },
  {
    "objectID": "conditioning-slides.html#discussion-questions",
    "href": "conditioning-slides.html#discussion-questions",
    "title": "Conditional Distributions",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\n“Correlation does not imply causation.” When does correlation suggest causation?\nIn what situations is an aggregated analysis appropriate despite Simpson’s paradox?\nHow would you explain conditional expectation to a manager?"
  },
  {
    "objectID": "clustering-slides.html#the-central-question",
    "href": "clustering-slides.html#the-central-question",
    "title": "Clustering",
    "section": "The Central Question",
    "text": "The Central Question\nWhen we have many variables, how do we simplify?\n\nOne approach: Group observations with similar profiles.\n\n\nThis is clustering—EDA extended to higher dimensions."
  },
  {
    "objectID": "clustering-slides.html#unsupervised-learning",
    "href": "clustering-slides.html#unsupervised-learning",
    "title": "Clustering",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nIn supervised learning, we have a response variable \\(Y\\) to predict from features \\(X\\).\n\nIn unsupervised learning, we have only \\(X\\)—no labels, no “right answer.”\n\n\nGoal: Discover structure in the data itself."
  },
  {
    "objectID": "clustering-slides.html#the-eda-spirit",
    "href": "clustering-slides.html#the-eda-spirit",
    "title": "Clustering",
    "section": "The EDA Spirit",
    "text": "The EDA Spirit\n\n“We are looking for unanticipated patterns in the data.”\n\n\nClustering shares EDA’s exploratory mindset:\n\nNo single “correct” answer\nResults depend on choices we make\nInterpretation requires domain knowledge"
  },
  {
    "objectID": "clustering-slides.html#us-colleges-1995",
    "href": "clustering-slides.html#us-colleges-1995",
    "title": "Clustering",
    "section": "US Colleges (1995)",
    "text": "US Colleges (1995)\n\n\n\n\nTable 1: Selected variables from ISLR2::College\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPrivate or public institution\n\n\nApps\nApplications received\n\n\nAccept\nApplications accepted\n\n\nEnroll\nNew students enrolled\n\n\nTop10perc\nPct. from top 10% of H.S. class\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate"
  },
  {
    "objectID": "clustering-slides.html#exploring-the-data",
    "href": "clustering-slides.html#exploring-the-data",
    "title": "Clustering",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\n\nFigure 1: Expenditure vs. Top 10% students"
  },
  {
    "objectID": "clustering-slides.html#class-exercise",
    "href": "clustering-slides.html#class-exercise",
    "title": "Clustering",
    "section": "Class Exercise",
    "text": "Class Exercise\nBefore running any algorithm:\n\n\nHow many private vs. public schools?\nWhat is the range of expenditure?\nWhich variables are correlated?\nWhat groupings would you expect to find?\n\n\n\n→ Build intuition before automation."
  },
  {
    "objectID": "clustering-slides.html#manual-grouping",
    "href": "clustering-slides.html#manual-grouping",
    "title": "Clustering",
    "section": "Manual Grouping",
    "text": "Manual Grouping\nWe can create groups from the data ourselves:\n\nSplit each variable at its median → four groups.\n\n\n\n\n\nGroup\nTop10perc\nExpend\n\n\n\n\ng_00\nbelow median\nbelow median\n\n\ng_01\nbelow median\nabove median\n\n\ng_10\nabove median\nbelow median\n\n\ng_11\nabove median\nabove median"
  },
  {
    "objectID": "clustering-slides.html#the-four-groups",
    "href": "clustering-slides.html#the-four-groups",
    "title": "Clustering",
    "section": "The Four Groups",
    "text": "The Four Groups\n\n\nFigure 2: Manual grouping by median splits"
  },
  {
    "objectID": "clustering-slides.html#the-limitation",
    "href": "clustering-slides.html#the-limitation",
    "title": "Clustering",
    "section": "The Limitation",
    "text": "The Limitation\nManual grouping works when we have 2-3 variables.\n\nBut the college data has 17 numeric variables.\n\n\nWe need an algorithm that can find groups in high-dimensional space."
  },
  {
    "objectID": "clustering-slides.html#euclidean-distance",
    "href": "clustering-slides.html#euclidean-distance",
    "title": "Clustering",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\nTo measure similarity, we use distance:\n\\[d(a, b) = \\sqrt{\\sum_{j=1}^{p} (a_j - b_j)^2}\\]\n\nObservations that are “close” in this sense have similar profiles."
  },
  {
    "objectID": "clustering-slides.html#the-scale-problem",
    "href": "clustering-slides.html#the-scale-problem",
    "title": "Clustering",
    "section": "The Scale Problem",
    "text": "The Scale Problem\n\n\n\n\nTable 2: Variable ranges differ dramatically\n\n\n\n\n\n\nVariable\nMin\nMax\nRange\n\n\n\n\nExpend\n3186\n56233\n53047\n\n\nApps\n81\n48094\n48013\n\n\nF.Undergrad\n139\n31643\n31504\n\n\nAccept\n72\n26330\n26258\n\n\nP.Undergrad\n1\n21836\n21835\n\n\n\n\n\n\n\n\n\nVariables with larger ranges dominate the distance calculation."
  },
  {
    "objectID": "clustering-slides.html#the-solution-z-scores",
    "href": "clustering-slides.html#the-solution-z-scores",
    "title": "Clustering",
    "section": "The Solution: Z-Scores",
    "text": "The Solution: Z-Scores\nStandardize each variable:\n\\[z_j = \\frac{x_j - \\bar{x}_j}{s_j}\\]\n\nAfter transformation:\n\nAll variables have mean 0 and SD 1\nA 1-unit difference represents one standard deviation\nVariables contribute equally to distance"
  },
  {
    "objectID": "clustering-slides.html#the-core-idea",
    "href": "clustering-slides.html#the-core-idea",
    "title": "Clustering",
    "section": "The Core Idea",
    "text": "The Core Idea\nPartition \\(n\\) observations into \\(K\\) clusters such that each observation belongs to the cluster with the nearest centroid.\n\nObjective: Minimize the total within-cluster sum of squares (WCSS):\n\\[\\min \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|x_i - \\mu_k\\|^2\\]"
  },
  {
    "objectID": "clustering-slides.html#the-algorithm",
    "href": "clustering-slides.html#the-algorithm",
    "title": "Clustering",
    "section": "The Algorithm",
    "text": "The Algorithm\n\nInitialize: Randomly select \\(K\\) observations as initial centroids\n\n\n\nAssign: Assign each observation to the nearest centroid\n\n\n\n\nUpdate: Recalculate centroids as the mean of assigned observations\n\n\n\n\nRepeat: Go to step 2 until assignments stop changing"
  },
  {
    "objectID": "clustering-slides.html#k-means-on-college-data",
    "href": "clustering-slides.html#k-means-on-college-data",
    "title": "Clustering",
    "section": "K-Means on College Data",
    "text": "K-Means on College Data\n\n\nFigure 3: K-means clusters (K=4) projected onto two variables"
  },
  {
    "objectID": "clustering-slides.html#practical-note-nstart",
    "href": "clustering-slides.html#practical-note-nstart",
    "title": "Clustering",
    "section": "Practical Note: nstart",
    "text": "Practical Note: nstart\nK-means depends on random initialization.\n\nDifferent starting points → different solutions.\n\n\nSolution: Run multiple times, keep the best result.\nkmeans(X, centers = 4, nstart = 25)\n\n\nRule of thumb: nstart between 20 and 50."
  },
  {
    "objectID": "clustering-slides.html#within-cluster-sum-of-squares",
    "href": "clustering-slides.html#within-cluster-sum-of-squares",
    "title": "Clustering",
    "section": "Within-Cluster Sum of Squares",
    "text": "Within-Cluster Sum of Squares\n\\[\\text{WCSS} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|x_i - \\mu_k\\|^2\\]\n\nLower WCSS = tighter clusters.\n\n\nBut WCSS always decreases as \\(K\\) increases—even with meaningless clusters."
  },
  {
    "objectID": "clustering-slides.html#choosing-k-the-elbow-method",
    "href": "clustering-slides.html#choosing-k-the-elbow-method",
    "title": "Clustering",
    "section": "Choosing K: The Elbow Method",
    "text": "Choosing K: The Elbow Method\n\n\nFigure 4: WCSS decreases as K increases—look for the ‘elbow’"
  },
  {
    "objectID": "clustering-slides.html#reading-the-elbow-plot",
    "href": "clustering-slides.html#reading-the-elbow-plot",
    "title": "Clustering",
    "section": "Reading the Elbow Plot",
    "text": "Reading the Elbow Plot\nSteep decline: Adding clusters captures real structure.\n\nGradual decline: Diminishing returns—overfitting.\n\n\nThe “elbow” is where the rate of decrease sharply levels off.\n\n\nCaveat: The elbow is subjective. Domain knowledge matters."
  },
  {
    "objectID": "clustering-slides.html#jaccard-similarity",
    "href": "clustering-slides.html#jaccard-similarity",
    "title": "Clustering",
    "section": "Jaccard Similarity",
    "text": "Jaccard Similarity\nHow do we compare two different clusterings?\n\nThe Jaccard index measures agreement by looking at pairs:\n\\[J = \\frac{|A \\cap B|}{|A \\cup B|}\\]\n\n\n\n\\(J = 1\\): Perfect agreement\n\\(J \\approx 0\\): No agreement\n\n\n\nUse cases: Compare K-means runs for stability; compare to known groupings."
  },
  {
    "objectID": "clustering-slides.html#manual-grouping-versus-k-means",
    "href": "clustering-slides.html#manual-grouping-versus-k-means",
    "title": "Clustering",
    "section": "Manual Grouping versus K-means",
    "text": "Manual Grouping versus K-means\nHow well does K-means recover our manual grouping?\n\n\n\n\nTable 3: K-means clusters vs. manual groups\n\n\n\n\n\n\ngroup_label\n1\n2\n3\n4\n\n\n\n\n00\n21\n225\n18\n0\n\n\n01\n40\n50\n11\n1\n\n\n10\n47\n51\n26\n0\n\n\n11\n161\n11\n39\n76\n\n\n\n\n\n\n\n\n\nSome clusters align well with manual groups; others split across them."
  },
  {
    "objectID": "clustering-slides.html#what-do-clusters-mean",
    "href": "clustering-slides.html#what-do-clusters-mean",
    "title": "Clustering",
    "section": "What Do Clusters Mean?",
    "text": "What Do Clusters Mean?\nThe algorithm found groups—now ask what characterizes each one.\n\nProfile each cluster:\n\nCompute cluster means for each variable\nCompare to overall means\nLook at distributions within clusters"
  },
  {
    "objectID": "clustering-slides.html#cluster-profiles",
    "href": "clustering-slides.html#cluster-profiles",
    "title": "Clustering",
    "section": "Cluster Profiles",
    "text": "Cluster Profiles\n\n\n\n\nTable 4: Cluster means for selected variables\n\n\n\n\n\n\ncluster\nn\nTop10perc\nExpend\nGrad.Rate\n\n\n\n\n1\n269\n31\n9975\n74\n\n\n2\n337\n16\n7025\n56\n\n\n3\n94\n31\n9204\n59\n\n\n4\n77\n62\n20651\n85\n\n\n\n\n\n\n\n\n\nWhat story do these profiles tell?"
  },
  {
    "objectID": "clustering-slides.html#example-interpretation",
    "href": "clustering-slides.html#example-interpretation",
    "title": "Clustering",
    "section": "Example Interpretation",
    "text": "Example Interpretation\nReview of additional variable profiles suggests:\n\n\nCluster 1: High selectivity, high expenditure → “Elite institutions”\nCluster 2: Lower selectivity, moderate expenditure → “Regional schools”\nCluster 3: Large enrollment, lower cost → “Large state universities”\nCluster 4: Small, moderate selectivity → “Small privates”\n\n\n\nDoes this match domain knowledge?"
  },
  {
    "objectID": "clustering-slides.html#clustering-in-the-ml-pipeline",
    "href": "clustering-slides.html#clustering-in-the-ml-pipeline",
    "title": "Clustering",
    "section": "Clustering in the ML Pipeline",
    "text": "Clustering in the ML Pipeline\nClustering connects to supervised learning:\n\nPreprocessing: Standardization is essential for many ML algorithms, not just clustering.\n\n\nFeature engineering: Cluster membership can become a derived feature.\n\n\nModel selection: The elbow heuristic → cross-validation, regularization."
  },
  {
    "objectID": "clustering-slides.html#other-clustering-methods",
    "href": "clustering-slides.html#other-clustering-methods",
    "title": "Clustering",
    "section": "Other Clustering Methods",
    "text": "Other Clustering Methods\n\n\n\nType\nExamples\n\n\n\n\nCentroid-based\nK-means, K-medoids\n\n\nDensity-based\nDBSCAN, HDBSCAN\n\n\nHierarchical\nAgglomerative, Divisive\n\n\nSoft clustering\nGaussian mixture models\n\n\n\n\nK-means assumes spherical clusters of similar size. Other methods relax these assumptions."
  },
  {
    "objectID": "clustering-slides.html#chapter-3-key-takeaways",
    "href": "clustering-slides.html#chapter-3-key-takeaways",
    "title": "Clustering",
    "section": "Chapter 3: Key Takeaways",
    "text": "Chapter 3: Key Takeaways\n\nClustering is EDA for high-dimensional data\nStandardize first—z-scores equalize variable contributions\nK-means minimizes WCSS via iterative assignment and update\nThe elbow method guides (but doesn’t determine) K selection\nInterpretation completes the analysis—profile your clusters"
  },
  {
    "objectID": "clustering-slides.html#key-formulas",
    "href": "clustering-slides.html#key-formulas",
    "title": "Clustering",
    "section": "Key Formulas",
    "text": "Key Formulas\n\n\n\nConcept\nFormula\n\n\n\n\nZ-score\n\\(z_j = \\frac{x_j - \\bar{x}_j}{s_j}\\)\n\n\nEuclidean distance\n\\(d(a,b) = \\sqrt{\\sum (a_j - b_j)^2}\\)\n\n\nWCSS\n\\(\\sum_k \\sum_{i \\in C_k} \\|x_i - \\mu_k\\|^2\\)\n\n\nJaccard index\n\\(J = \\frac{|A \\cap B|}{|A \\cup B|}\\)"
  },
  {
    "objectID": "clustering-slides.html#team-exercise-1-iris-k-means",
    "href": "clustering-slides.html#team-exercise-1-iris-k-means",
    "title": "Clustering",
    "section": "Team Exercise 1: Iris K-means",
    "text": "Team Exercise 1: Iris K-means\nUsing datasets::iris:\n\nApply K-means with \\(K = 3\\) to the four numeric measurements.\nCross-tabulate clusters with Species. How well do they align?\nWhich species is hardest to separate? Why might this be?"
  },
  {
    "objectID": "clustering-slides.html#team-exercise-2-college-data-exploration",
    "href": "clustering-slides.html#team-exercise-2-college-data-exploration",
    "title": "Clustering",
    "section": "Team Exercise 2: College Data Exploration",
    "text": "Team Exercise 2: College Data Exploration\nUsing ISLR2::College:\n\nHow many private vs. public schools? What is the range of expenditure per student?\nUse GGally::ggpairs() on 4–5 variables. Which appear correlated?\nWhat groupings would you expect to find—elite vs. non-elite? Large vs. small?\nRun K-means with \\(K = 3\\). Do the clusters match your expectations?"
  },
  {
    "objectID": "clustering-slides.html#team-exercise-3-choosing-k",
    "href": "clustering-slides.html#team-exercise-3-choosing-k",
    "title": "Clustering",
    "section": "Team Exercise 3: Choosing K",
    "text": "Team Exercise 3: Choosing K\nYou have customer transaction data and want to segment customers.\n\nHow would you decide on the number of clusters?\nWhat are the pros and cons of the elbow method vs. silhouette scores?\nYour marketing team wants exactly 5 segments. How do you respond?\n\n\nExercise 1 is quick and concrete. Exercise 2 encourages domain thinking before algorithms. Exercise 3 addresses real-world constraints."
  },
  {
    "objectID": "clustering-slides.html#discussion-questions",
    "href": "clustering-slides.html#discussion-questions",
    "title": "Clustering",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhen is clustering exploratory vs. confirmatory?\nK-means finds spherical clusters. What real-world data might violate this assumption?\nHow would you validate clusters when there are no true labels?"
  },
  {
    "objectID": "eda-slides.html#exploratory-data-analysis",
    "href": "eda-slides.html#exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nJohn Tukey (1977): Statisticians had become too focused on formal inference at the expense of simply looking at data.\n\nEDA is the corrective:\n\nVisualization\nSummary\nIterative development of questions"
  },
  {
    "objectID": "eda-slides.html#eda-vs.-confirmatory-analysis",
    "href": "eda-slides.html#eda-vs.-confirmatory-analysis",
    "title": "Exploratory Data Analysis",
    "section": "EDA vs. Confirmatory Analysis",
    "text": "EDA vs. Confirmatory Analysis\n\n\n\nConfirmatory\nExploratory\n\n\n\n\nBegins with a hypothesis\nBegins with data\n\n\nAsks: do data support or refute?\nAsks: what patterns are present?\n\n\nTests pre-specified claims\nGenerates new hypotheses\n\n\n\n\nA complete analysis typically involves both."
  },
  {
    "objectID": "eda-slides.html#why-eda-matters-for-ml",
    "href": "eda-slides.html#why-eda-matters-for-ml",
    "title": "Exploratory Data Analysis",
    "section": "Why EDA Matters for ML",
    "text": "Why EDA Matters for ML\n\nUnderstanding data before modeling\n\nVariable types, distributions, missing values, outliers\n\nGenerating hypotheses\n\nPatterns become candidates for formal modeling\n\nDiagnosing models\n\nResidual plots reveal what models miss\n\nCommunicating findings\n\nVisualizations convey results to stakeholders"
  },
  {
    "objectID": "eda-slides.html#eda-is-a-mindset",
    "href": "eda-slides.html#eda-is-a-mindset",
    "title": "Exploratory Data Analysis",
    "section": "EDA is a Mindset",
    "text": "EDA is a Mindset\n\nA willingness to look at data from multiple angles, to transform variables, to compare subgroups, and above all to ask questions."
  },
  {
    "objectID": "eda-slides.html#data-as-sample-from-population",
    "href": "eda-slides.html#data-as-sample-from-population",
    "title": "Exploratory Data Analysis",
    "section": "Data as Sample from Population",
    "text": "Data as Sample from Population\n\n\nPopulation\nThe complete set of cases we care about\n\nAll possible customers\nAll manufacturing runs\nAll images of a certain type\n\n\nSample\nThe subset we actually observe\n\nOur goal: learn about the population\nEDA helps assess whether the sample supports generalization"
  },
  {
    "objectID": "eda-slides.html#central-value-two-approaches",
    "href": "eda-slides.html#central-value-two-approaches",
    "title": "Exploratory Data Analysis",
    "section": "Central Value: Two Approaches",
    "text": "Central Value: Two Approaches\n\n\nMean (Average)\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\n\nMinimizes sum of squared deviations\nSensitive to outliers\n\n\nMedian\nMiddle value when sorted\n\nHalf above, half below\nRobust to outliers\nOften better for skewed data"
  },
  {
    "objectID": "eda-slides.html#dispersion-two-approaches",
    "href": "eda-slides.html#dispersion-two-approaches",
    "title": "Exploratory Data Analysis",
    "section": "Dispersion: Two Approaches",
    "text": "Dispersion: Two Approaches\n\n\nStandard Deviation\n\\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\nTypical distance from mean\nSensitive to outliers\n\n\nInterquartile Range (IQR)\n\\[\\text{IQR} = Q_3 - Q_1\\]\n\nSpread of middle 50%\nRobust to outliers"
  },
  {
    "objectID": "eda-slides.html#choosing-among-summaries",
    "href": "eda-slides.html#choosing-among-summaries",
    "title": "Exploratory Data Analysis",
    "section": "Choosing Among Summaries",
    "text": "Choosing Among Summaries\nKey insight: Compute both and compare.\n\nLarge discrepancy between mean and median signals:\n\nSkewness, or\nOutliers\n\n\n\n→ Warrants further investigation"
  },
  {
    "objectID": "eda-slides.html#questions-drive-the-analysis",
    "href": "eda-slides.html#questions-drive-the-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Questions Drive the Analysis",
    "text": "Questions Drive the Analysis\nStandard starting questions:\n\nWhat is the distribution of each variable?\nAre there outliers or unusual values?\nAre there missing values?\nHow are pairs of variables related?\n\n\nDomain-specific questions arise from context and purpose."
  },
  {
    "objectID": "eda-slides.html#eda-throughout-modeling",
    "href": "eda-slides.html#eda-throughout-modeling",
    "title": "Exploratory Data Analysis",
    "section": "EDA Throughout Modeling",
    "text": "EDA Throughout Modeling\n\n\n\nStage\nEDA Role\n\n\n\n\nBefore\nUnderstand distributions, detect quality issues\n\n\nDuring\nCompare models, identify difficult cases\n\n\nAfter\nExamine residuals, check assumptions"
  },
  {
    "objectID": "eda-slides.html#historical-context",
    "href": "eda-slides.html#historical-context",
    "title": "Exploratory Data Analysis",
    "section": "Historical Context",
    "text": "Historical Context\nFrancis Galton (1885): Investigated whether height is hereditary\nKarl Pearson: Founded first statistics department (UCL, 1911); collected father-son height data\n\nThis data set introduced:\n\nCorrelation\nRegression\n“Regression to the mean”"
  },
  {
    "objectID": "eda-slides.html#the-data",
    "href": "eda-slides.html#the-data",
    "title": "Exploratory Data Analysis",
    "section": "The Data",
    "text": "The Data\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nNumber of pairs\n1078\n\n\nFather mean height\n67.7 inches\n\n\nSon mean height\n68.7 inches"
  },
  {
    "objectID": "eda-slides.html#first-look-scatter-plot",
    "href": "eda-slides.html#first-look-scatter-plot",
    "title": "Exploratory Data Analysis",
    "section": "First Look: Scatter Plot",
    "text": "First Look: Scatter Plot\n\n\nFigure 1: Heights of father-son pairs"
  },
  {
    "objectID": "eda-slides.html#what-the-scatter-plot-reveals",
    "href": "eda-slides.html#what-the-scatter-plot-reveals",
    "title": "Exploratory Data Analysis",
    "section": "What the Scatter Plot Reveals",
    "text": "What the Scatter Plot Reveals\n\nPositive association: Taller fathers → taller sons\nRoughly linear relationship\nSubstantial scatter around the trend\nDensity in middle makes individual points hard to see\n\n\nOne visualization → one question answered, new questions raised"
  },
  {
    "objectID": "eda-slides.html#conditional-distributions",
    "href": "eda-slides.html#conditional-distributions",
    "title": "Exploratory Data Analysis",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\n\n\nFigure 2: Son’s height by father’s height interval"
  },
  {
    "objectID": "eda-slides.html#what-the-box-plots-reveal",
    "href": "eda-slides.html#what-the-box-plots-reveal",
    "title": "Exploratory Data Analysis",
    "section": "What the Box Plots Reveal",
    "text": "What the Box Plots Reveal\n\nMedian increases with father’s height\nSpread (IQR) similar across groups\nA few outliers among sons of shorter fathers\n\n\nThis shows the conditional distribution of son’s height given father’s height.\n→ Central concept in Chapter 2"
  },
  {
    "objectID": "eda-slides.html#are-sons-taller-than-fathers",
    "href": "eda-slides.html#are-sons-taller-than-fathers",
    "title": "Exploratory Data Analysis",
    "section": "Are Sons Taller Than Fathers?",
    "text": "Are Sons Taller Than Fathers?\n\n\nFigure 3: Comparing height distributions"
  },
  {
    "objectID": "eda-slides.html#summary-fathers-vs.-sons",
    "href": "eda-slides.html#summary-fathers-vs.-sons",
    "title": "Exploratory Data Analysis",
    "section": "Summary: Fathers vs. Sons",
    "text": "Summary: Fathers vs. Sons\n\n\n\n\nTable 1\n\n\n\n\n\n\nGeneration\nn\nMean\nMedian\nSD\n\n\n\n\nfather\n1078\n67.7\n67.8\n2.7\n\n\nson\n1078\n68.7\n68.6\n2.8\n\n\n\n\n\n\n\n\n\nSons are about one inch taller on average."
  },
  {
    "objectID": "eda-slides.html#paired-differences",
    "href": "eda-slides.html#paired-differences",
    "title": "Exploratory Data Analysis",
    "section": "Paired Differences",
    "text": "Paired Differences\n\n\nFigure 4: Distribution of son’s height minus father’s height"
  },
  {
    "objectID": "eda-slides.html#regression-to-the-mean",
    "href": "eda-slides.html#regression-to-the-mean",
    "title": "Exploratory Data Analysis",
    "section": "Regression to the Mean",
    "text": "Regression to the Mean\nGalton’s observation:\n\nExtremely tall fathers tend to have sons who are tall, but not quite as extreme.\n\n\nThis is a statistical phenomenon, not a genetic one.\nIt arises whenever two variables are correlated but not perfectly so.\n\n\n→ Explored mathematically in Chapter 2"
  },
  {
    "objectID": "eda-slides.html#showing-your-work",
    "href": "eda-slides.html#showing-your-work",
    "title": "Exploratory Data Analysis",
    "section": "Showing Your Work",
    "text": "Showing Your Work\nThroughout this book, code is generally hidden.\n\nBut seeing how visualizations are constructed is part of learning EDA:\n\n\nCode\nfather_son_ht |&gt; \n  ggplot2::ggplot(aes(x = father, y = son)) + \n  ggplot2::geom_point(alpha = 0.4) + \n  ggplot2::labs(\n    x = \"Father's height (inches)\",\n    y = \"Son's height (inches)\"\n  )"
  },
  {
    "objectID": "eda-slides.html#the-grammar-of-graphics",
    "href": "eda-slides.html#the-grammar-of-graphics",
    "title": "Exploratory Data Analysis",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\nStart with data\nSpecify aesthetic mappings (variables → visual properties)\nAdd geometric objects (points, lines, bars)\nCustomize labels and appearance\n\n\nThis framework (ggplot2) provides flexibility for constructing visualizations."
  },
  {
    "objectID": "eda-slides.html#class-exercises",
    "href": "eda-slides.html#class-exercises",
    "title": "Exploratory Data Analysis",
    "section": "Class Exercises",
    "text": "Class Exercises\nDiamond Data (ggplot2::diamonds)\n\n~54,000 diamonds with price and attributes\nExplore distributions, relationships, predictors of price\n\n\nGeneral Social Survey (forcats::gss_cat)\n\nUS survey tracking social attitudes\nPractice with categorical variables\nExplore changes over time"
  },
  {
    "objectID": "eda-slides.html#guiding-questions-for-any-data-set",
    "href": "eda-slides.html#guiding-questions-for-any-data-set",
    "title": "Exploratory Data Analysis",
    "section": "Guiding Questions for Any Data Set",
    "text": "Guiding Questions for Any Data Set\n\nHow many observations? Variables?\nWhat are the variable types?\nWhat are the distributions?\nAre there missing values? Outliers?\nWhat relationships exist between variables?\nWhat would I need to know to make a decision?"
  },
  {
    "objectID": "eda-slides.html#part-1-foundations-of-eda",
    "href": "eda-slides.html#part-1-foundations-of-eda",
    "title": "Exploratory Data Analysis",
    "section": "Part 1: Foundations of EDA",
    "text": "Part 1: Foundations of EDA\n\n\n\nChapter\nTopic\n\n\n\n\n2\nConditional Distributions\n\n\n3\nClustering\n\n\n4\nStatistical Simulation\n\n\n5\nSampling and Study Design\n\n\n6\nInformation Theory"
  },
  {
    "objectID": "eda-slides.html#key-themes",
    "href": "eda-slides.html#key-themes",
    "title": "Exploratory Data Analysis",
    "section": "Key Themes",
    "text": "Key Themes\n\nConditional distributions underlie regression and classification\nClustering exemplifies EDA in higher dimensions\nSimulation builds intuition about variability\nStudy design determines what conclusions are valid\nInformation theory provides vocabulary for ML"
  },
  {
    "objectID": "eda-slides.html#the-eda-mindset",
    "href": "eda-slides.html#the-eda-mindset",
    "title": "Exploratory Data Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\n\nLook at the data, ask questions, and let the data guide your understanding.\n\n\nThis mindset remains central throughout the book."
  },
  {
    "objectID": "eda-slides.html#key-takeaways",
    "href": "eda-slides.html#key-takeaways",
    "title": "Exploratory Data Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nEDA is question-driven exploration of data\nCentral value and dispersion have multiple implementations\nData = sample from population frames our inferences\nMultiple views of the same data reveal different patterns\nEDA continues throughout the modeling process"
  },
  {
    "objectID": "eda-slides.html#team-exercise-1-response-vs.-predictor",
    "href": "eda-slides.html#team-exercise-1-response-vs.-predictor",
    "title": "Exploratory Data Analysis",
    "section": "Team Exercise 1: Response vs. Predictor",
    "text": "Team Exercise 1: Response vs. Predictor\nFor the father-son height data:\n\nWhich variable would you designate as the “response” and which as the “predictor”?\nCould a reasonable argument be made for the reverse?\nDescribe a situation where this distinction would not apply."
  },
  {
    "objectID": "eda-slides.html#team-exercise-2-regression-to-the-mean",
    "href": "eda-slides.html#team-exercise-2-regression-to-the-mean",
    "title": "Exploratory Data Analysis",
    "section": "Team Exercise 2: Regression to the Mean",
    "text": "Team Exercise 2: Regression to the Mean\nGalton observed that extremely tall or short fathers tend to have sons who are not quite so extreme.\n\nUsing the summary table of sons’ heights by father’s height, can you see evidence of this phenomenon?\nHow would you quantify it?\nWhy does this happen? (Hint: think about the role of the mother’s height and random variation.)"
  },
  {
    "objectID": "eda-slides.html#team-exercise-3-misleading-summaries",
    "href": "eda-slides.html#team-exercise-3-misleading-summaries",
    "title": "Exploratory Data Analysis",
    "section": "Team Exercise 3: Misleading Summaries",
    "text": "Team Exercise 3: Misleading Summaries\n\nFind or construct an example where summary statistics (mean, SD, correlation) give a misleading impression.\nWhat visualization would reveal the true structure?\nWhat does this imply about the order of operations in EDA?\n\n\nExercise 1 connects to the dual aims theme. Exercise 2 is historically important and counterintuitive. Exercise 3 leads naturally to Anscombe’s quartet in the next chapter."
  },
  {
    "objectID": "eda-slides.html#discussion-questions",
    "href": "eda-slides.html#discussion-questions",
    "title": "Exploratory Data Analysis",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nA colleague says “I have 10 million rows—I don’t need to look at the data.” How would you respond?\nWhen might prediction and understanding conflict? Give an example.\nWhat would Tukey think about large language models doing data analysis?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDA for Machine Learning: Slides",
    "section": "",
    "text": "Lecture slides for Exploratory Data Analysis for Machine Learning."
  },
  {
    "objectID": "index.html#part-1-foundations-of-eda",
    "href": "index.html#part-1-foundations-of-eda",
    "title": "EDA for Machine Learning: Slides",
    "section": "Part 1: Foundations of EDA",
    "text": "Part 1: Foundations of EDA\n\n\n\nChapter\nSlides\n\n\n\n\n1. Exploratory Data Analysis\neda-slides.html\n\n\n2. Conditional Distributions\nconditioning-slides.html\n\n\n3. Clustering\nclustering-slides.html\n\n\n4. Statistical Simulation\nsimulation-slides.html\n\n\n5. Sampling and Study Design\nstudy-design-slides.html\n\n\n6. Information Theory\ninfo-theory-slides.html"
  },
  {
    "objectID": "index.html#part-2-linear-algebra-methods",
    "href": "index.html#part-2-linear-algebra-methods",
    "title": "EDA for Machine Learning: Slides",
    "section": "Part 2: Linear Algebra Methods",
    "text": "Part 2: Linear Algebra Methods\n\n\n\nChapter\nSlides\n\n\n\n\n7. Linear Regression\nlin-reg-slides.html\n\n\n8. Principal Component Analysis\npca-slides.html\n\n\n9. Linear Discriminant Analysis\nlin-discr-slides.html"
  },
  {
    "objectID": "index.html#part-3-text-data",
    "href": "index.html#part-3-text-data",
    "title": "EDA for Machine Learning: Slides",
    "section": "Part 3: Text Data",
    "text": "Part 3: Text Data\n\n\n\nChapter\nSlides\n\n\n\n\n10. Text as Data\ntext-as-data-slides.html\n\n\n11. Topic Models\ntopic-models-slides.html"
  },
  {
    "objectID": "index.html#part-4-time-series-data",
    "href": "index.html#part-4-time-series-data",
    "title": "EDA for Machine Learning: Slides",
    "section": "Part 4: Time Series Data",
    "text": "Part 4: Time Series Data\n\n\n\nChapter\nSlides\n\n\n\n\n12. Time Series Data\nts-data-slides.html\n\n\n13. Time Domain Methods\nts-time-domain-slides.html\n\n\n14. Frequency Domain Methods\nts-freq-domain-slides.html"
  },
  {
    "objectID": "index.html#part-5-graph-data",
    "href": "index.html#part-5-graph-data",
    "title": "EDA for Machine Learning: Slides",
    "section": "Part 5: Graph Data",
    "text": "Part 5: Graph Data\n\n\n\nChapter\nSlides\n\n\n\n\n15. Graph Theory for Machine Learning\ngraph-theory-slides.html"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "EDA for Machine Learning: Slides",
    "section": "Usage",
    "text": "Usage\nThese slides are designed as visual overviews for lecture or self-study orientation. We recommend using them as the first step in the slides → workbooks → book learning sequence.\n\nSlides: Survey the terrain (“What am I about to learn?”)\nWorkbooks: Attempt exercises (“Can I do this?”)\nBook: Read for understanding (“Why does this work?”)"
  },
  {
    "objectID": "index.html#related-resources",
    "href": "index.html#related-resources",
    "title": "EDA for Machine Learning: Slides",
    "section": "Related Resources",
    "text": "Related Resources\n\nBook (published online)\nData + Workbooks (eda4mlr R package)\nSource repository"
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "EDA for Machine Learning: Slides",
    "section": "How to Cite",
    "text": "How to Cite\n\nThrall, T. (2025). Exploratory Data Analysis for Machine Learning. https://tthrall.github.io/eda4ml/"
  },
  {
    "objectID": "lin-discr-slides.html#the-core-question",
    "href": "lin-discr-slides.html#the-core-question",
    "title": "Linear Discriminant Analysis",
    "section": "The Core Question",
    "text": "The Core Question\nIn Chapter 8, PCA asked: “Along which directions do features vary most?”\n\nNow we ask a different question:\n\n\n\n\n\n\n\n\nCore Question\n\n\nAlong which directions are the classes best separated?\n\n\n\n\n\nWhen class labels are available, we can do better than maximizing variance."
  },
  {
    "objectID": "lin-discr-slides.html#pca-vs-lda-the-key-distinction",
    "href": "lin-discr-slides.html#pca-vs-lda-the-key-distinction",
    "title": "Linear Discriminant Analysis",
    "section": "PCA vs LDA: The Key Distinction",
    "text": "PCA vs LDA: The Key Distinction\n\n\n\n\nPCA\nLDA\n\n\n\n\nInput\nFeatures only\nFeatures + class labels\n\n\nObjective\nMaximize variance\nMaximize class separation\n\n\nType\nUnsupervised\nSupervised\n\n\n\n\nBoth find directions for projection—but “best” means different things."
  },
  {
    "objectID": "lin-discr-slides.html#when-pca-fails",
    "href": "lin-discr-slides.html#when-pca-fails",
    "title": "Linear Discriminant Analysis",
    "section": "When PCA Fails",
    "text": "When PCA Fails\n\nWhen projected to PC1, the two classes completely overlap\nPC1 (dashed): Maximum variance—but useless for classification!\nLD1 (solid): Maximum class separation—the direction we need."
  },
  {
    "objectID": "lin-discr-slides.html#the-lda-solution",
    "href": "lin-discr-slides.html#the-lda-solution",
    "title": "Linear Discriminant Analysis",
    "section": "The LDA Solution",
    "text": "The LDA Solution\nLDA finds the direction(s) that:\n\n\nMaximize the spread of class means (between-class variance)\n\n\n\n\nMinimize the spread within each class (within-class variance)\n\n\n\n\n\n\n\n\n\nLDA in One Sentence\n\n\nMaximize separation between classes while keeping each class tight."
  },
  {
    "objectID": "lin-discr-slides.html#two-types-of-variance",
    "href": "lin-discr-slides.html#two-types-of-variance",
    "title": "Linear Discriminant Analysis",
    "section": "Two Types of Variance",
    "text": "Two Types of Variance\nWhen we project data onto a direction \\(a\\), we can measure:\n\nBetween-class variance: How spread out are the projected class means?\n\n\nWithin-class variance: How spread out are observations around their class mean?\n\n\nGood classification directions have high between-class and low within-class variance."
  },
  {
    "objectID": "lin-discr-slides.html#visual-within-vs-between",
    "href": "lin-discr-slides.html#visual-within-vs-between",
    "title": "Linear Discriminant Analysis",
    "section": "Visual: Within vs Between",
    "text": "Visual: Within vs Between\n\n\nFigure 1: Within-class spread (ellipses) vs between-class spread (line connecting means)"
  },
  {
    "objectID": "lin-discr-slides.html#fishers-criterion",
    "href": "lin-discr-slides.html#fishers-criterion",
    "title": "Linear Discriminant Analysis",
    "section": "Fisher’s Criterion",
    "text": "Fisher’s Criterion\nR.A. Fisher (1936) proposed: find direction \\(a\\) that maximizes the ratio\n\\[\n\\frac{\\text{Between-class variance}}{\\text{Within-class variance}} = \\frac{a^\\top B \\, a}{a^\\top W \\, a}\n\\]\n\nwhere:\n\n\\(B\\) = between-class covariance matrix\n\\(W\\) = within-class (pooled) covariance matrix\n\n\n\nThis Rayleigh quotient balances separation against spread."
  },
  {
    "objectID": "lin-discr-slides.html#the-eigenvalue-solution",
    "href": "lin-discr-slides.html#the-eigenvalue-solution",
    "title": "Linear Discriminant Analysis",
    "section": "The Eigenvalue Solution",
    "text": "The Eigenvalue Solution\n\n\n\n\n\n\nKey Result\n\n\nThe optimal direction \\(a\\) is the eigenvector of \\(W^{-1}B\\) with the largest eigenvalue.\n\n\n\n\n\\[\nW^{-1} B \\, a = \\lambda \\, a\n\\]\n\n\nLike PCA, LDA reduces to an eigenvalue problem—we solve directly, no iteration."
  },
  {
    "objectID": "lin-discr-slides.html#how-many-directions",
    "href": "lin-discr-slides.html#how-many-directions",
    "title": "Linear Discriminant Analysis",
    "section": "How Many Directions?",
    "text": "How Many Directions?\nWith \\(K\\) classes, the matrix \\(B\\) has rank at most \\(K - 1\\).\n\nWhy? The \\(K\\) class means live in an affine subspace of dimension \\(K - 1\\).\n\n\n\n\n\n\n\n\nDimension Reduction\n\n\nLDA produces at most \\(\\min(d, K-1)\\) discriminant directions.\n\n\\(K = 2\\) classes → 1 direction (LD1)\n\\(K = 3\\) classes → 2 directions (LD1, LD2)\n\n\n\n\n\n\nRegardless of how many features \\(d\\) you start with!"
  },
  {
    "objectID": "lin-discr-slides.html#connection-to-pca",
    "href": "lin-discr-slides.html#connection-to-pca",
    "title": "Linear Discriminant Analysis",
    "section": "Connection to PCA",
    "text": "Connection to PCA\nBoth PCA and LDA find optimal projection directions:\n\n\n\n\n\n\n\n\n\nPCA\nLDA\n\n\n\n\nMaximizes\n\\(\\dfrac{a^\\top T \\, a}{a^\\top I \\, a}\\)\n\\(\\dfrac{a^\\top B \\, a}{a^\\top W \\, a}\\)\n\n\nMatrix\nTotal covariance \\(T\\)\nBetween/Within ratio\n\n\nConstraint\nUnit length\nUnit length (in \\(W\\) metric)\n\n\n\n\nKey insight: LDA is like PCA, but measuring spread relative to within-class variation rather than identity."
  },
  {
    "objectID": "lin-discr-slides.html#the-gaussian-assumption",
    "href": "lin-discr-slides.html#the-gaussian-assumption",
    "title": "Linear Discriminant Analysis",
    "section": "The Gaussian Assumption",
    "text": "The Gaussian Assumption\nLDA assumes each class has a multivariate normal distribution:\n\\[\nf_k(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu_k)^\\top \\Sigma^{-1} (x - \\mu_k) \\right)\n\\]\n\nKey assumption: All classes share the same covariance \\(\\Sigma\\).\n\n\n\n\\(\\Sigma\\): covariance matrix common to all classes\n\\(\\mu_k\\): distinct mean values\n\\(\\pi_k\\): prior probabilities of class occurrence"
  },
  {
    "objectID": "lin-discr-slides.html#from-bayes-rule-to-linear-boundaries",
    "href": "lin-discr-slides.html#from-bayes-rule-to-linear-boundaries",
    "title": "Linear Discriminant Analysis",
    "section": "From Bayes Rule to Linear Boundaries",
    "text": "From Bayes Rule to Linear Boundaries\nThe optimal classifier assigns \\(x\\) to the class with highest posterior probability.\n\nUnder Gaussian assumptions with common covariance, this simplifies to:\n\\[\n\\delta_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log(\\pi_k)\n\\]\n\n\n\n\n\n\n\n\nDiscriminant Function\n\n\n\\(\\delta_k(x)\\) is linear in \\(x\\)—it tells you how much class \\(k\\) “likes” observation \\(x\\)."
  },
  {
    "objectID": "lin-discr-slides.html#decision-boundaries",
    "href": "lin-discr-slides.html#decision-boundaries",
    "title": "Linear Discriminant Analysis",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\nClassify to class \\(k\\) if \\(\\delta_k(x) &gt; \\delta_j(x)\\) for all \\(j \\neq k\\).\n\nThe boundary between classes \\(j\\) and \\(k\\) is where \\(\\delta_j(x) = \\delta_k(x)\\).\n\n\nSince both are linear in \\(x\\), the boundary is a hyperplane:\n\nIn 2D: a line\nIn 3D: a plane\nIn \\(d\\) dimensions: a \\((d-1)\\)-dimensional hyperplane"
  },
  {
    "objectID": "lin-discr-slides.html#two-routes-same-destination",
    "href": "lin-discr-slides.html#two-routes-same-destination",
    "title": "Linear Discriminant Analysis",
    "section": "Two Routes, Same Destination",
    "text": "Two Routes, Same Destination\n\n\n\n\n\n\nEquivalence\n\n\nFisher’s geometric criterion and the Bayesian/Gaussian derivation yield exactly the same discriminant directions—when class covariances are equal.\n\n\n\n\nFisher: No distributional assumptions, purely geometric\nBayes: Provides posterior probabilities, principled classification rule\n\n\nUse whichever perspective helps your intuition!"
  },
  {
    "objectID": "lin-discr-slides.html#what-if-covariances-differ",
    "href": "lin-discr-slides.html#what-if-covariances-differ",
    "title": "Linear Discriminant Analysis",
    "section": "What If Covariances Differ?",
    "text": "What If Covariances Differ?\nLDA assumes all classes share the same covariance matrix.\n\nQuadratic Discriminant Analysis (QDA) relaxes this:\n\nEach class \\(k\\) has its own covariance \\(\\Sigma_k\\)\nDecision boundaries become quadratic (curved)"
  },
  {
    "objectID": "lin-discr-slides.html#visual-comparison",
    "href": "lin-discr-slides.html#visual-comparison",
    "title": "Linear Discriminant Analysis",
    "section": "Visual Comparison",
    "text": "Visual Comparison\n\n\nFigure 2: LDA (linear boundary) vs QDA (curved boundary)"
  },
  {
    "objectID": "lin-discr-slides.html#lda-vs-qda-trade-offs",
    "href": "lin-discr-slides.html#lda-vs-qda-trade-offs",
    "title": "Linear Discriminant Analysis",
    "section": "LDA vs QDA: Trade-offs",
    "text": "LDA vs QDA: Trade-offs\n\n\n\n\nLDA\nQDA\n\n\n\n\nAssumption\nCommon covariance\nClass-specific covariances\n\n\nBoundary\nLinear (hyperplane)\nQuadratic (curved)\n\n\nParameters\nFewer\nMore\n\n\nBias\nHigher if assumption violated\nLower\n\n\nVariance\nLower\nHigher\n\n\n\n\nRule of thumb: Use LDA unless you have strong evidence of unequal covariances and enough data to estimate them reliably."
  },
  {
    "objectID": "lin-discr-slides.html#the-wine-quality-data",
    "href": "lin-discr-slides.html#the-wine-quality-data",
    "title": "Linear Discriminant Analysis",
    "section": "The Wine Quality Data",
    "text": "The Wine Quality Data\nIn Chapter 8, PCA discovered that wine color was the dominant source of variation—without using color labels.\n\nNow we ask: given that we want to classify red vs white, what direction achieves the best separation?\n\n\nThis is the LDA question."
  },
  {
    "objectID": "lin-discr-slides.html#two-features-density-and-residual-sugar",
    "href": "lin-discr-slides.html#two-features-density-and-residual-sugar",
    "title": "Linear Discriminant Analysis",
    "section": "Two Features: Density and Residual Sugar",
    "text": "Two Features: Density and Residual Sugar\n\n\nFigure 3: Wine color: LDA decision boundary with two features"
  },
  {
    "objectID": "lin-discr-slides.html#two-feature-lda-coefficients",
    "href": "lin-discr-slides.html#two-feature-lda-coefficients",
    "title": "Linear Discriminant Analysis",
    "section": "Two-Feature LDA: Coefficients",
    "text": "Two-Feature LDA: Coefficients\n\n\n\n\nTable 1\n\n\n\n\n\n\nFeature\nLD1 Coefficient\n\n\n\n\ndensity\n-1.72\n\n\nres_sugar\n1.67\n\n\n\n\n\n\n\n\n\nInterpretation: Wines with lower density and higher residual sugar → white\nWines with higher density and lower residual sugar → red"
  },
  {
    "objectID": "lin-discr-slides.html#two-feature-performance",
    "href": "lin-discr-slides.html#two-feature-performance",
    "title": "Linear Discriminant Analysis",
    "section": "Two-Feature Performance",
    "text": "Two-Feature Performance\n\n\n\nConfusion matrix: 2 features\n\n\n\nred\nwhite\n\n\n\n\nred\n1403\n151\n\n\nwhite\n196\n4747\n\n\n\n\n\n\nWith just 2 features: ~88% accuracy on red, ~97% on white.\nCan we do better with more features?"
  },
  {
    "objectID": "lin-discr-slides.html#all-eleven-features",
    "href": "lin-discr-slides.html#all-eleven-features",
    "title": "Linear Discriminant Analysis",
    "section": "All Eleven Features",
    "text": "All Eleven Features\n\n\n\n\nTable 2: LD1 coefficients for all 11 features\n\n\n\n\n\n\nFeature\nLD1 Coefficient\n\n\n\n\ndensity\n-2.73\n\n\nres_sugar\n1.67\n\n\ntotal_so2\n1.13\n\n\nalcohol\n-0.98\n\n\nvol_acidity\n-0.50\n\n\nfix_acidity\n0.42\n\n\nfree_so2\n-0.34\n\n\nchlorides\n-0.18\n\n\npH\n0.18\n\n\nsulphates\n-0.13\n\n\ncitric_acid\n0.13"
  },
  {
    "objectID": "lin-discr-slides.html#eleven-feature-performance",
    "href": "lin-discr-slides.html#eleven-feature-performance",
    "title": "Linear Discriminant Analysis",
    "section": "Eleven-Feature Performance",
    "text": "Eleven-Feature Performance\n\n\n\nConfusion matrix: 11 features\n\n\n\nred\nwhite\n\n\n\n\nred\n1580\n16\n\n\nwhite\n19\n4882\n\n\n\n\n\n\nMisclassification drops to ~1.2% for red and ~0.3% for white."
  },
  {
    "objectID": "lin-discr-slides.html#ld1-score-distribution",
    "href": "lin-discr-slides.html#ld1-score-distribution",
    "title": "Linear Discriminant Analysis",
    "section": "LD1 Score Distribution",
    "text": "LD1 Score Distribution\n\n\nFigure 4: Distribution of LD1 scores by wine color\n\nThe decision boundary (dashed line) falls between the two distributions."
  },
  {
    "objectID": "lin-discr-slides.html#lda-vs-pca-same-answer",
    "href": "lin-discr-slides.html#lda-vs-pca-same-answer",
    "title": "Linear Discriminant Analysis",
    "section": "LDA vs PCA: Same Answer?",
    "text": "LDA vs PCA: Same Answer?\nIn Chapter 8, PCA found that PC1 separated wine colors.\n\nHere, LDA’s LD1 also separates wine colors.\n\n\nAre they the same direction?\n\n\nIn this case, nearly so—because the classes differ primarily along directions of high variance.\n\n\n\n\n\n\n\n\nWhen They Differ\n\n\nIf classes differed along a direction of low variance, PCA would miss it while LDA would find it."
  },
  {
    "objectID": "lin-discr-slides.html#a-harder-problem-k-3",
    "href": "lin-discr-slides.html#a-harder-problem-k-3",
    "title": "Linear Discriminant Analysis",
    "section": "A Harder Problem \\((K = 3)\\)",
    "text": "A Harder Problem \\((K = 3)\\)\nNow classify wines by quality level (low/medium/high) instead of color.\n\n\n\n\n\nQuality Level\nCount\n\n\n\n\nlow\n2384\n\n\nmedium\n2836\n\n\nhigh\n1277\n\n\n\n\n\n\nNote: Classes are imbalanced—most wines are “medium” quality."
  },
  {
    "objectID": "lin-discr-slides.html#three-classes-two-directions",
    "href": "lin-discr-slides.html#three-classes-two-directions",
    "title": "Linear Discriminant Analysis",
    "section": "Three Classes → Two Directions",
    "text": "Three Classes → Two Directions\nWith \\(K = 3\\) classes, LDA produces at most \\(K - 1 = 2\\) directions.\n\n\nFigure 5: White wines projected onto LD1 and LD2 (3 quality levels)"
  },
  {
    "objectID": "lin-discr-slides.html#quality-classification-results",
    "href": "lin-discr-slides.html#quality-classification-results",
    "title": "Linear Discriminant Analysis",
    "section": "Quality Classification: Results",
    "text": "Quality Classification: Results\n\n\n\nConfusion matrix: wine quality (white wines)\n\n\n\nlow\nmedium\nhigh\n\n\n\n\nlow\n939\n504\n77\n\n\nmedium\n666\n1390\n603\n\n\nhigh\n35\n304\n380\n\n\n\n\n\n\nMost wines are classified as “medium”—the classifier struggles to separate quality levels.\n\n\n\n\n\n\n\n\nReality Check\n\n\nLDA can’t work miracles. Predicting quality from chemistry alone is genuinely hard—winemaking involves factors not captured in these measurements."
  },
  {
    "objectID": "lin-discr-slides.html#key-takeaways",
    "href": "lin-discr-slides.html#key-takeaways",
    "title": "Linear Discriminant Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nLDA finds directions of maximum class separation—not just maximum variance\n\n\n\nFisher’s criterion: Maximize between-class variance relative to within-class variance\n\n\n\n\nClosed-form solution via eigenvalue decomposition—like PCA\n\n\n\n\nDimension reduction: \\(K\\) classes → at most \\(K - 1\\) discriminant directions\n\n\n\n\nLinear boundaries arise from equal covariance assumption; QDA relaxes this"
  },
  {
    "objectID": "lin-discr-slides.html#key-formulas",
    "href": "lin-discr-slides.html#key-formulas",
    "title": "Linear Discriminant Analysis",
    "section": "Key Formulas",
    "text": "Key Formulas\nFisher’s criterion: \\[\\max_a \\; \\frac{a^\\top B \\, a}{a^\\top W \\, a}\\]\n\nEigenvalue problem: \\[W^{-1} B \\, a = \\lambda \\, a\\]\n\n\nDiscriminant function: \\[\\delta_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log(\\pi_k)\\]\n\n\nClassification rule: Assign \\(x\\) to class \\(k\\) with largest \\(\\delta_k(x)\\)"
  },
  {
    "objectID": "lin-discr-slides.html#connections-to-part-2",
    "href": "lin-discr-slides.html#connections-to-part-2",
    "title": "Linear Discriminant Analysis",
    "section": "Connections to Part 2",
    "text": "Connections to Part 2\n\n\n\nChapter\nSubspace Basis\nDetermined by\nUsed for\n\n\n\n\n7: Regression\n\\(\\text{col}(X)\\)\nModel specification\nPrediction\n\n\n8: PCA\nPrincipal components\nData covariance\nExploration\n\n\n9: LDA\nDiscriminant directions\nClass labels\nClassification\n\n\n\n\nAll three use orthogonal projection—they differ in how the target subspace is determined and used."
  },
  {
    "objectID": "lin-discr-slides.html#looking-ahead",
    "href": "lin-discr-slides.html#looking-ahead",
    "title": "Linear Discriminant Analysis",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nPart III: Text Data\n\nA new kind of high-dimensional data: term-document matrices\nTopic models for discovering thematic structure\nLatent Dirichlet Allocation—the other LDA!\n\n\n\n\n\n\n\n\nThe Name Collision\n\n\n“LDA” means Linear Discriminant Analysis here, but Latent Dirichlet Allocation in text analysis. Context makes clear which is intended."
  },
  {
    "objectID": "lin-discr-slides.html#team-exercise-1-fishers-criterion",
    "href": "lin-discr-slides.html#team-exercise-1-fishers-criterion",
    "title": "Linear Discriminant Analysis",
    "section": "Team Exercise 1: Fisher’s Criterion",
    "text": "Team Exercise 1: Fisher’s Criterion\nFor a two-class problem in 2D:\n\nExplain in words what the within-class covariance matrix \\(W\\) measures.\nExplain in words what the between-class covariance matrix \\(B\\) measures.\nWhy does maximizing \\(a^\\top B a / a^\\top W a\\) yield a good classification direction?\nFor \\(K = 2\\) classes, \\(B\\) has rank 1. Why?"
  },
  {
    "objectID": "lin-discr-slides.html#team-exercise-2-when-pca-fails",
    "href": "lin-discr-slides.html#team-exercise-2-when-pca-fails",
    "title": "Linear Discriminant Analysis",
    "section": "Team Exercise 2: When PCA Fails",
    "text": "Team Exercise 2: When PCA Fails\nConstruct a 2D example where PC1 is perpendicular to LD1:\n\nSketch two classes whose means differ along the x-axis, but with much larger variance along the y-axis.\nWhat direction will PCA find? What direction will LDA find?\nWhat does this illustrate about supervised vs unsupervised dimension reduction?"
  },
  {
    "objectID": "lin-discr-slides.html#team-exercise-3-lda-vs-logistic-regression",
    "href": "lin-discr-slides.html#team-exercise-3-lda-vs-logistic-regression",
    "title": "Linear Discriminant Analysis",
    "section": "Team Exercise 3: LDA vs Logistic Regression",
    "text": "Team Exercise 3: LDA vs Logistic Regression\nBoth LDA and logistic regression produce linear decision boundaries.\n\nWhat distributional assumptions does LDA make? What about logistic regression?\nWhen would you prefer LDA? When logistic regression?\nFit both to the wine color data and compare decision boundaries."
  },
  {
    "objectID": "lin-discr-slides.html#team-exercise-4-qda",
    "href": "lin-discr-slides.html#team-exercise-4-qda",
    "title": "Linear Discriminant Analysis",
    "section": "Team Exercise 4: QDA",
    "text": "Team Exercise 4: QDA\nUsing the wine data:\n\nCompute the covariance matrix of features separately for red and white wines.\nAre they similar? How would you test this?\nFit QDA and compare its decision boundary to LDA.\nDoes QDA improve classification accuracy?"
  },
  {
    "objectID": "lin-discr-slides.html#references",
    "href": "lin-discr-slides.html#references",
    "title": "Linear Discriminant Analysis",
    "section": "References",
    "text": "References\n\nAn Introduction to Statistical Learning (ISLR2)\n\nChapter 4 covers LDA\n\nThe Elements of Statistical Learning\n\nChapter 4 for advanced treatment\n\nMASS::lda() documentation\n\nR’s classic LDA implementation\n\ndiscrim package\n\ntidymodels interface to discriminant analysis"
  },
  {
    "objectID": "pca-slides.html#the-core-question",
    "href": "pca-slides.html#the-core-question",
    "title": "Principal Component Analysis",
    "section": "The Core Question",
    "text": "The Core Question\nGiven a dataset with multiple features, we ask:\n\n\n\n\n\n\n\nCore Question\n\n\nAlong which directions do the features vary most?\n\n\n\n\n\nLet’s build intuition through examples before formulas."
  },
  {
    "objectID": "pca-slides.html#example-1-galton-heights-2d",
    "href": "pca-slides.html#example-1-galton-heights-2d",
    "title": "Principal Component Analysis",
    "section": "Example 1: Galton Heights (2D)",
    "text": "Example 1: Galton Heights (2D)\nFather and son heights from Galton’s 1885 study:\n\n\nFigure 1: Father-son heights (centred)\nThe cloud of points has a clear direction of elongation."
  },
  {
    "objectID": "pca-slides.html#finding-pc1-maximum-variance",
    "href": "pca-slides.html#finding-pc1-maximum-variance",
    "title": "Principal Component Analysis",
    "section": "Finding PC1: Maximum Variance",
    "text": "Finding PC1: Maximum Variance\n\n\nFigure 2: PC1 captures the direction of greatest spread\nPC1 is the line that captures the most variance when we project onto it."
  },
  {
    "objectID": "pca-slides.html#pc2-orthogonal-to-pc1",
    "href": "pca-slides.html#pc2-orthogonal-to-pc1",
    "title": "Principal Component Analysis",
    "section": "PC2: Orthogonal to PC1",
    "text": "PC2: Orthogonal to PC1\n\n\nFigure 3: PC1 and PC2 form an orthogonal basis\nPC2 captures the remaining variance, orthogonal to PC1."
  },
  {
    "objectID": "pca-slides.html#galton-variance-explained",
    "href": "pca-slides.html#galton-variance-explained",
    "title": "Principal Component Analysis",
    "section": "Galton: Variance Explained",
    "text": "Galton: Variance Explained\n\n\n\n\n\nComponent\nVariance Explained\n\n\n\n\nPC1\n75.0%\n\n\nPC2\n25.0%\n\n\n\n\n\n\nIn 2D, PCA simply rotates the coordinate system to align with the data’s natural axes of variation."
  },
  {
    "objectID": "pca-slides.html#example-2-us-arrests-4d",
    "href": "pca-slides.html#example-2-us-arrests-4d",
    "title": "Principal Component Analysis",
    "section": "Example 2: US Arrests (4D)",
    "text": "Example 2: US Arrests (4D)\nFrom McNeil (1977): violent crime rates per 100,000 population (1973)\n\n\n\n\nVariable\nDescription\n\n\n\n\nAssault\nAssault arrests\n\n\nRape\nRape arrests\n\n\nMurder\nMurder arrests\n\n\nUrbanPop\nPercent urban population\n\n\n\n\n\nWith 4 variables, we can’t simply “look” at the data."
  },
  {
    "objectID": "pca-slides.html#visualizing-4-variables",
    "href": "pca-slides.html#visualizing-4-variables",
    "title": "Principal Component Analysis",
    "section": "Visualizing 4 Variables",
    "text": "Visualizing 4 Variables\n\n\nFigure 4: US Arrests: pairwise scatter plots"
  },
  {
    "objectID": "pca-slides.html#what-do-we-see",
    "href": "pca-slides.html#what-do-we-see",
    "title": "Principal Component Analysis",
    "section": "What Do We See?",
    "text": "What Do We See?\nStrong correlations among crime variables:\n\nMurder–Assault: \\(r \\approx 0.80\\)\nMurder–Rape: \\(r \\approx 0.56\\)\nAssault–Rape: \\(r \\approx 0.67\\)\n\n\nThe data don’t fill 4D space—they lie near a lower-dimensional structure."
  },
  {
    "objectID": "pca-slides.html#us-arrests-pca-results",
    "href": "pca-slides.html#us-arrests-pca-results",
    "title": "Principal Component Analysis",
    "section": "US Arrests: PCA Results",
    "text": "US Arrests: PCA Results\n\n\n\n\n\nComponent\nVariance Explained\n\n\n\n\nPC1\n62.4%\n\n\nPC2\n24.4%\n\n\nPC3\n9.0%\n\n\nPC4\n4.3%\n\n\n\n\n\n\nPC1 alone captures 62% of the variance in four variables."
  },
  {
    "objectID": "pca-slides.html#what-does-pc1-represent",
    "href": "pca-slides.html#what-does-pc1-represent",
    "title": "Principal Component Analysis",
    "section": "What Does PC1 Represent?",
    "text": "What Does PC1 Represent?\n\n\nFigure 5: US Arrests: PC1 loadings\n\nAll loadings are similar in magnitude—PC1 is a weighted average: a “crime index.”"
  },
  {
    "objectID": "pca-slides.html#pc1-scores-a-crime-index",
    "href": "pca-slides.html#pc1-scores-a-crime-index",
    "title": "Principal Component Analysis",
    "section": "PC1 Scores: A Crime Index",
    "text": "PC1 Scores: A Crime Index\n\n\nFigure 6: States ranked by PC1 score (crime index)"
  },
  {
    "objectID": "pca-slides.html#d-visualization-with-pc1",
    "href": "pca-slides.html#d-visualization-with-pc1",
    "title": "Principal Component Analysis",
    "section": "3D Visualization with PC1",
    "text": "3D Visualization with PC1\n\n\n\n\n\n\n\n\nFigure 7: US Arrests in 3D, colored by PC1 score\n\n\n\n\nThe gradient shows PC1 capturing the main axis of variation."
  },
  {
    "objectID": "pca-slides.html#light-explanation-what-is-pca-doing",
    "href": "pca-slides.html#light-explanation-what-is-pca-doing",
    "title": "Principal Component Analysis",
    "section": "Light Explanation: What is PCA Doing?",
    "text": "Light Explanation: What is PCA Doing?\n\n\n\n\n\n\nPCA in One Sentence\n\n\nPCA finds orthogonal directions that maximize variance, ordered from most to least important.\n\n\n\n\nMathematically: Find unit vector \\(\\mathbf{v}_1\\) that maximizes \\(\\text{Var}(\\mathbf{X}\\mathbf{v}_1)\\).\n\n\nGeometrically: Rotate coordinates to align with the data’s natural axes of spread.\n\n\nBut why would we want to do this?"
  },
  {
    "objectID": "pca-slides.html#the-curse-of-dimensionality",
    "href": "pca-slides.html#the-curse-of-dimensionality",
    "title": "Principal Component Analysis",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nModern datasets often have many features:\n\n\n\nDomain\nTypical Features\n\n\n\n\nGenomics\n20,000+ genes\n\n\nImages\n1,000,000+ pixels\n\n\nText\n100,000+ terms\n\n\nSensors\n1,000s of channels\n\n\n\n\nHigh dimensions cause problems."
  },
  {
    "objectID": "pca-slides.html#problem-1-distances-become-meaningless",
    "href": "pca-slides.html#problem-1-distances-become-meaningless",
    "title": "Principal Component Analysis",
    "section": "Problem 1: Distances Become Meaningless",
    "text": "Problem 1: Distances Become Meaningless\nIn high dimensions, all points become approximately equidistant.\n\n\nFigure 8: Distribution of pairwise distances as dimension increases\nThe distributions concentrate—distances lose discriminative power."
  },
  {
    "objectID": "pca-slides.html#problem-2-space-becomes-empty",
    "href": "pca-slides.html#problem-2-space-becomes-empty",
    "title": "Principal Component Analysis",
    "section": "Problem 2: Space Becomes Empty",
    "text": "Problem 2: Space Becomes Empty\n\n\nFigure 9: Volume of ball inscribed within unit cube vanishes in high dimensions\nIn 50 dimensions, 99%+ of the volume is in the “corners.”"
  },
  {
    "objectID": "pca-slides.html#problem-3-estimation-burden",
    "href": "pca-slides.html#problem-3-estimation-burden",
    "title": "Principal Component Analysis",
    "section": "Problem 3: Estimation Burden",
    "text": "Problem 3: Estimation Burden\nWith \\(d\\) features, a covariance matrix has \\(\\frac{d(d+1)}{2}\\) parameters.\n\n\n\n\nFeatures\nCovariance Parameters\n\n\n\n\n10\n55\n\n\n100\n5,050\n\n\n1,000\n500,500\n\n\n10,000\n50,005,000\n\n\n\n\n\nWithout dimension reduction, we need impossibly large samples."
  },
  {
    "objectID": "pca-slides.html#the-opportunity",
    "href": "pca-slides.html#the-opportunity",
    "title": "Principal Component Analysis",
    "section": "The Opportunity",
    "text": "The Opportunity\n\n\n\n\n\n\nDonoho (2000)\n\n\nWhile randomly generated high-dimensional data behave pathologically, actual data often occupy much lower-dimensional structures.\n\n\n\n\nExamples:\n\nCrime rates are correlated (not independent)\nChemical properties are constrained by fermentation\nGene expression is regulated by pathways"
  },
  {
    "objectID": "pca-slides.html#how-pca-addresses-the-curse",
    "href": "pca-slides.html#how-pca-addresses-the-curse",
    "title": "Principal Component Analysis",
    "section": "How PCA Addresses the Curse",
    "text": "How PCA Addresses the Curse\n\n\n\n\n\n\nThe PCA Solution\n\n\nCompress the data to a lower-dimensional subspace that retains most of the variance.\n\n\n\n\nIf the first \\(k\\) principal components capture 90% of the variance:\n\nReduce from \\(d\\) dimensions to \\(k\\)\nDistances become meaningful again\nEstimation becomes tractable"
  },
  {
    "objectID": "pca-slides.html#the-wine-quality-dataset",
    "href": "pca-slides.html#the-wine-quality-dataset",
    "title": "Principal Component Analysis",
    "section": "The Wine Quality Dataset",
    "text": "The Wine Quality Dataset\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nObservations\n6,497\n\n\nFeatures\n11\n\n\nRed wines\n1,599\n\n\nWhite wines\n4,898\n\n\n\n\n\n\n11 chemical properties measured on ~6,500 wines.\nQuestion: Can PCA discover the red/white distinction without being told about color?"
  },
  {
    "objectID": "pca-slides.html#the-scaling-decision",
    "href": "pca-slides.html#the-scaling-decision",
    "title": "Principal Component Analysis",
    "section": "The Scaling Decision",
    "text": "The Scaling Decision\n\n\n\n\nTable 1\n\n\n\n\n\n\nfeature\nmax\nmin\nrange\n\n\n\n\ntotal_so2\n440.0\n6.0\n434.0\n\n\nfree_so2\n289.0\n1.0\n288.0\n\n\nres_sugar\n65.8\n0.6\n65.2\n\n\nfix_acidity\n15.9\n3.8\n12.1\n\n\nalcohol\n14.9\n8.0\n6.9\n\n\n\n\n\n\n\n\n\nVariables have very different scales. Without scaling, total sulfur dioxide would dominate."
  },
  {
    "objectID": "pca-slides.html#wine-quality-pca",
    "href": "pca-slides.html#wine-quality-pca",
    "title": "Principal Component Analysis",
    "section": "Wine Quality: PCA",
    "text": "Wine Quality: PCA\n\n\nFigure 10: Scree plot: variance explained by each PC\n\nFirst 4 components capture ~75% of variance."
  },
  {
    "objectID": "pca-slides.html#pc1-vs-pc2-the-reveal",
    "href": "pca-slides.html#pc1-vs-pc2-the-reveal",
    "title": "Principal Component Analysis",
    "section": "PC1 vs PC2: The Reveal",
    "text": "PC1 vs PC2: The Reveal\n\n\nFigure 11: Wine samples projected onto first two principal components"
  },
  {
    "objectID": "pca-slides.html#unsupervised-discovery",
    "href": "pca-slides.html#unsupervised-discovery",
    "title": "Principal Component Analysis",
    "section": "Unsupervised Discovery",
    "text": "Unsupervised Discovery\n\n\n\n\n\n\nKey Insight\n\n\nPCA separated red and white wines without knowing about color labels.\n\n\n\n\nThe chemical properties that vary most across wines happen to distinguish red from white.\n\n\nThis is the power of unsupervised learning: discovering structure the analyst didn’t anticipate."
  },
  {
    "objectID": "pca-slides.html#interpreting-the-loadings",
    "href": "pca-slides.html#interpreting-the-loadings",
    "title": "Principal Component Analysis",
    "section": "Interpreting the Loadings",
    "text": "Interpreting the Loadings\n\n\nFigure 12: Which variables drive PC1?\n\nPC1: Sulfur compounds (negative) vs volatile acidity/chlorides (positive)"
  },
  {
    "objectID": "pca-slides.html#notation",
    "href": "pca-slides.html#notation",
    "title": "Principal Component Analysis",
    "section": "Notation",
    "text": "Notation\nLet \\(X_{\\bullet,\\bullet}\\) be the centred \\(n \\times d\\) feature matrix.\n\n\nRows: observations\nColumns: features (mean = 0)\n\n\n\nThe covariance matrix:\n\\[\n\\text{Cov}(X) = \\frac{1}{n-1} X^\\top X\n\\]"
  },
  {
    "objectID": "pca-slides.html#first-principal-component",
    "href": "pca-slides.html#first-principal-component",
    "title": "Principal Component Analysis",
    "section": "First Principal Component",
    "text": "First Principal Component\nThe first PC is a linear combination:\n\\[\nc_{\\bullet,1} = X_{\\bullet,\\bullet} \\, v_{\\bullet,1} \\quad \\text{with } \\|v_{\\bullet,1}\\| = 1\n\\]\n\nThe vector \\(v_{\\bullet,1}\\) maximizes variance:\n\\[\n\\text{var}(c_{\\bullet,1}) = \\max \\left\\{ \\text{var}(X v) : \\|v\\| = 1 \\right\\}\n\\]"
  },
  {
    "objectID": "pca-slides.html#the-eigenvalue-solution",
    "href": "pca-slides.html#the-eigenvalue-solution",
    "title": "Principal Component Analysis",
    "section": "The Eigenvalue Solution",
    "text": "The Eigenvalue Solution\n\n\n\n\n\n\nKey Result\n\n\n\\(v_{\\bullet,1}\\) is the eigenvector of \\(X^\\top X\\) with largest eigenvalue \\(\\sigma_1^2\\).\n\n\n\n\\[\nX^\\top X \\, v_{\\bullet,1} = \\sigma_1^2 \\, v_{\\bullet,1}\n\\]\n\nThe eigenvalue \\(\\sigma_1^2\\) equals the variance of \\(c_{\\bullet,1}\\) (up to factor \\(n-1\\))."
  },
  {
    "objectID": "pca-slides.html#subsequent-components",
    "href": "pca-slides.html#subsequent-components",
    "title": "Principal Component Analysis",
    "section": "Subsequent Components",
    "text": "Subsequent Components\nThe \\(k\\)th principal component maximizes variance subject to orthogonality:\n\\[\nc_{\\bullet,k} = X \\, v_{\\bullet,k} \\quad \\text{where } v_{\\bullet,k} \\perp v_{\\bullet,1}, \\ldots, v_{\\bullet,k-1}\n\\]\n\nSolution: \\((v_{\\bullet,1}, \\ldots, v_{\\bullet,d})\\) are the eigenvectors of \\(X^\\top X\\), ordered by decreasing eigenvalue."
  },
  {
    "objectID": "pca-slides.html#why-pca-has-a-closed-form",
    "href": "pca-slides.html#why-pca-has-a-closed-form",
    "title": "Principal Component Analysis",
    "section": "Why PCA Has a Closed Form",
    "text": "Why PCA Has a Closed Form\n\n\n\n\n\n\nWhy PCA Is Special\n\n\nWe solve directly via eigenvalue decomposition—no iteration needed.\n\n\n\n\nContrast with:\n\n\\(k\\)-means: iterates to local optimum\nTopic models: requires MCMC or variational inference\nNeural networks: gradient descent\n\n\n\nThe closed form exists because maximizing a quadratic form (variance) subject to a quadratic constraint (unit norm) yields a linear eigenvalue problem."
  },
  {
    "objectID": "pca-slides.html#computation-svd",
    "href": "pca-slides.html#computation-svd",
    "title": "Principal Component Analysis",
    "section": "Computation: SVD",
    "text": "Computation: SVD\nIf \\(X = U \\Sigma V^\\top\\), then:\n\n\nColumns of \\(V\\): Principal component directions (\\(v_{\\bullet,k}\\))\n\n\n\n\nDiagonal of \\(\\Sigma\\): Square roots of eigenvalues (\\(\\sigma_k\\))\n\n\n\n\nColumns of \\(U \\Sigma\\): Scores (\\(c_{\\bullet,k}\\))"
  },
  {
    "objectID": "pca-slides.html#using-prcomp-in-r",
    "href": "pca-slides.html#using-prcomp-in-r",
    "title": "Principal Component Analysis",
    "section": "Using prcomp() in R",
    "text": "Using prcomp() in R\n\n\nCode\n# Basic PCA workflow\npca_result &lt;- prcomp(X, center = TRUE, scale. = TRUE)\n\n# Loadings (PC directions)\npca_result$rotation\n\n# Scores (observations in PC space)\npca_result$x\n\n# Standard deviations (sqrt of eigenvalues)\npca_result$sdev\n\n\n\nNote: scale. = TRUE standardizes features to unit variance—essential when features are on different scales."
  },
  {
    "objectID": "pca-slides.html#key-takeaways",
    "href": "pca-slides.html#key-takeaways",
    "title": "Principal Component Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPCA finds directions of maximum variance — these capture the most information about differences among observations\n\n\n\nClosed-form solution via eigenvalue decomposition — no iteration required\n\n\n\n\nDimension reduction — project onto first \\(k\\) PCs to reduce noise and enable visualization\n\n\n\n\nUnsupervised — uses only feature covariance, not labels"
  },
  {
    "objectID": "pca-slides.html#key-formulas",
    "href": "pca-slides.html#key-formulas",
    "title": "Principal Component Analysis",
    "section": "Key Formulas",
    "text": "Key Formulas\nPrincipal component: \\[c_{\\bullet,k} = X_{\\bullet,\\bullet} v_{\\bullet,k}\\]\n\nEigenvalue problem: \\[X^\\top X \\, v_{\\bullet,k} = \\sigma_k^2 \\, v_{\\bullet,k}\\]\n\n\nVariance explained: \\[\\text{var}(c_{\\bullet,k}) \\propto \\sigma_k^2\\]\n\n\nScore (projection): \\[\\text{score}_{i,k} = \\langle x_{i,\\bullet}, v_{\\bullet,k} \\rangle\\]"
  },
  {
    "objectID": "pca-slides.html#connections-to-part-2",
    "href": "pca-slides.html#connections-to-part-2",
    "title": "Principal Component Analysis",
    "section": "Connections to Part 2",
    "text": "Connections to Part 2\n\n\n\nChapter\nSubspace Basis\nDetermined by\n\n\n\n\n7: Regression\n\\(\\text{col}(X)\\)\nModel specification\n\n\n8: PCA\nPrincipal components\nData covariance\n\n\n9: LDA\nDiscriminant directions\nClass labels\n\n\n\n\nAll three use orthogonal projection—they differ in how the target subspace is determined and used."
  },
  {
    "objectID": "pca-slides.html#looking-ahead",
    "href": "pca-slides.html#looking-ahead",
    "title": "Principal Component Analysis",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nChapter 9: Linear Discriminant Analysis\n\nWhat if class labels are available?\nFind directions that separate classes, not just maximize variance\nSupervised counterpart to PCA\n\n\n\n\n\n\n\n\nThe Contrast\n\n\nPCA asks: “Where is variance?”\nLDA asks: “Where are classes separated?”"
  },
  {
    "objectID": "pca-slides.html#team-exercise-1-by-hand",
    "href": "pca-slides.html#team-exercise-1-by-hand",
    "title": "Principal Component Analysis",
    "section": "Team Exercise 1: By Hand",
    "text": "Team Exercise 1: By Hand\nFor the \\(4 \\times 2\\) matrix:\n\\[\nX = \\begin{pmatrix} 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\end{pmatrix}\n\\]\n\nCenter the columns\nCompute \\(X^\\top X\\) for the centered matrix\nFind eigenvalues and eigenvectors\nWhat proportion of variance does PC1 capture?"
  },
  {
    "objectID": "pca-slides.html#team-exercise-2-scaling",
    "href": "pca-slides.html#team-exercise-2-scaling",
    "title": "Principal Component Analysis",
    "section": "Team Exercise 2: Scaling",
    "text": "Team Exercise 2: Scaling\nA researcher measures height (cm) and weight (kg) for patients.\n\nWithout scaling, which variable will dominate PC1? Why?\nThe researcher converts height to meters. How does this change PCA?\nWhen should you scale features before PCA?"
  },
  {
    "objectID": "pca-slides.html#team-exercise-3-interpretation",
    "href": "pca-slides.html#team-exercise-3-interpretation",
    "title": "Principal Component Analysis",
    "section": "Team Exercise 3: Interpretation",
    "text": "Team Exercise 3: Interpretation\nFor the US Arrests data:\n\nAll crime variables load positively on PC1. What does a high PC1 score mean?\nMurder and Rape have opposite signs on PC2. What does this PC capture?\nIf you had to create a single “crime index,” would you use PC1? What are the trade-offs?"
  },
  {
    "objectID": "pca-slides.html#team-exercise-4-pca-vs-regression",
    "href": "pca-slides.html#team-exercise-4-pca-vs-regression",
    "title": "Principal Component Analysis",
    "section": "Team Exercise 4: PCA vs Regression",
    "text": "Team Exercise 4: PCA vs Regression\nBoth PCA and regression involve projection. Explain:\n\nFor each method, how is the subspace determined and used?\nWhy might maximum-variance directions differ from predictive directions?\nDesign a 2D example where PC1 is useless for predicting a response."
  },
  {
    "objectID": "pca-slides.html#references",
    "href": "pca-slides.html#references",
    "title": "Principal Component Analysis",
    "section": "References",
    "text": "References\n\nAn Introduction to Statistical Learning (ISLR2) — Chapter 12 covers PCA\nThe Elements of Statistical Learning — Chapter 14 for advanced treatment\nLearnPCA — R package\nPCA: A Practical Guide — Shlens (2014), excellent tutorial\nprcomp() documentation — R’s SVD-based PCA"
  },
  {
    "objectID": "study-design-slides.html#the-big-data-paradox",
    "href": "study-design-slides.html#the-big-data-paradox",
    "title": "Sampling and Study Design",
    "section": "The Big Data Paradox",
    "text": "The Big Data Paradox\nIf we have millions of records, why does sampling theory matter?\n\nAnswer: As soon as you ask “what would happen if…” you’re generalizing beyond your data.\n\n\n\n\n\n\n\n\nML Connection\n\n\nTraining data should be a sample from the deployment environment.\n\n\n\n\nThis is the key insight for a data science audience. Even “big data” is a sample from some larger process. The question is whether it’s a representative sample."
  },
  {
    "objectID": "study-design-slides.html#chapter-roadmap",
    "href": "study-design-slides.html#chapter-roadmap",
    "title": "Sampling and Study Design",
    "section": "Chapter Roadmap",
    "text": "Chapter Roadmap\n\nObservational studies: Learning from data you didn’t generate\nExperimental studies: Learning from conditions you control\n\nMeasurement: Bias, chance error, and uncertainty\nThe role of EDA: Bridging design and modeling\n\n\nInstructor note: This chapter draws heavily from Freedman, Pisani, and Purves (FPP). The examples are classic but the lessons are timeless."
  },
  {
    "objectID": "study-design-slides.html#literary-digest-1936-the-largest-poll-in-history",
    "href": "study-design-slides.html#literary-digest-1936-the-largest-poll-in-history",
    "title": "Sampling and Study Design",
    "section": "Literary Digest, 1936: The Largest Poll in History",
    "text": "Literary Digest, 1936: The Largest Poll in History\n\n\n\n\n\nSource\nFDR Predicted %\n\n\n\n\nDigest\n43\n\n\nGallup re Digest\n44\n\n\nGallup re election\n56\n\n\nelection result\n62\n\n\n\n\n\n\nThe Digest’s error of 19 percentage points is the largest in polling history.\n\nPause here to let the numbers sink in. 2.4 million respondents got it catastrophically wrong. 50,000 got it roughly right. This sets up the key lesson."
  },
  {
    "objectID": "study-design-slides.html#what-went-wrong",
    "href": "study-design-slides.html#what-went-wrong",
    "title": "Sampling and Study Design",
    "section": "What Went Wrong?",
    "text": "What Went Wrong?\nThe Digest sampled from:\n\nAutomobile registrations\nTelephone directories\nMagazine subscription lists\n\n\nIn 1936, these skewed wealthy—and wealthy voters favored Landon.\n\n\n\n\n\n\n\n\nSelection Bias\n\n\nThe sampling frame excluded the population of interest.\n\n\n\n\n\n\n\n\n\nML Connection\n\n\nIf your training data excludes a subpopulation, your model won’t serve them."
  },
  {
    "objectID": "study-design-slides.html#truman-vs.-dewey-1948",
    "href": "study-design-slides.html#truman-vs.-dewey-1948",
    "title": "Sampling and Study Design",
    "section": "Truman vs. Dewey, 1948:",
    "text": "Truman vs. Dewey, 1948:\nQuota Sampling Fails\n\n\n\n\n\nsource\nTruman\nDewey\nThurmond\nWallace\n\n\n\n\nCrossley\n45\n50\n2\n3\n\n\nGallup\n44\n50\n2\n4\n\n\nRoper\n38\n53\n5\n4\n\n\nelection result\n50\n45\n3\n2\n\n\n\n\n\n\nAll three major polls predicted Dewey by 5+ points. All three were wrong.\n\nThis is the famous “Dewey Defeats Truman” headline moment. Ask the class if they’ve seen the photo of Truman holding that newspaper."
  },
  {
    "objectID": "study-design-slides.html#quota-sampling-the-problem",
    "href": "study-design-slides.html#quota-sampling-the-problem",
    "title": "Sampling and Study Design",
    "section": "Quota Sampling: The Problem",
    "text": "Quota Sampling: The Problem\nQuota sampling: Match sample demographics to population demographics.\n\nInterviewers given quotas: X women, Y employed, Z from each region…\nOtherwise free to choose subjects\n\n\nThe hidden bias: Interviewers chose “convenient” subjects within quotas—who tended to vote Republican.\n\nThe key insight is that there are many factors influencing voting, and you can’t control for all of them with quotas. Interviewers unconsciously selected people who were easier to interview."
  },
  {
    "objectID": "study-design-slides.html#probability-sampling",
    "href": "study-design-slides.html#probability-sampling",
    "title": "Sampling and Study Design",
    "section": "Probability Sampling",
    "text": "Probability Sampling\nKey insight: Too many known and unknown factors to control them all.\n\nSolution: Let chance create a representative sample.\n\n\nSimple random sampling: Every individual has a known, equal probability of selection.\n\n\n\n\n\n\n\n\nHallmark of Probability Sampling\n\n\nThe probability of including any given individual can be calculated in advance."
  },
  {
    "objectID": "study-design-slides.html#practical-probability-sampling",
    "href": "study-design-slides.html#practical-probability-sampling",
    "title": "Sampling and Study Design",
    "section": "Practical Probability Sampling",
    "text": "Practical Probability Sampling\nSimple random sampling is often impractical (cost, logistics).\nMulti-stage cluster sampling: Randomly select regions → towns → precincts → households\n\nKey features preserved:\n\nNo interviewer discretion in subject selection\nPrescribed procedure involving planned use of chance"
  },
  {
    "objectID": "study-design-slides.html#post-1948-probability-sampling-works",
    "href": "study-design-slides.html#post-1948-probability-sampling-works",
    "title": "Sampling and Study Design",
    "section": "Post-1948: Probability Sampling Works",
    "text": "Post-1948: Probability Sampling Works\n\n\n\n\n\nYear\nSample Size\nWinner\nGallup %\nActual %\nError\n\n\n\n\n1952\n5385\nEisenhower\n51\n55.1\n-4.1\n\n\n1960\n8015\nKennedy\n51\n49.7\n1.3\n\n\n1976\n3439\nCarter\n48\n50.1\n-2.1\n\n\n1984\n3456\nReagan\n59\n58.8\n0.2\n\n\n2000\n3571\nBush\n48\n47.9\n0.1\n\n\n2004\n2014\nBush\n49\n50.6\n-1.6\n\n\n\n\n\nErrors mostly within ±3 percentage points—with samples of 2,000-8,000, not millions.\n\nThis is a subset of the full table in the textbook. The point is the contrast with the Literary Digest: smaller samples with probability sampling beat huge samples with biased sampling."
  },
  {
    "objectID": "study-design-slides.html#uc-berkeley-admissions-simpsons-paradox",
    "href": "study-design-slides.html#uc-berkeley-admissions-simpsons-paradox",
    "title": "Sampling and Study Design",
    "section": "UC Berkeley Admissions: Simpson’s Paradox",
    "text": "UC Berkeley Admissions: Simpson’s Paradox\nIn the 1970s, concern arose that graduate admissions were biased against women.\n\nOverall admission rate: Men 44%, Women 35%\n\n\nBut department-by-department analysis told a different story…\n\nThis is one of the most famous examples of Simpson’s Paradox. Build the suspense before revealing the twist."
  },
  {
    "objectID": "study-design-slides.html#the-paradox-revealed",
    "href": "study-design-slides.html#the-paradox-revealed",
    "title": "Sampling and Study Design",
    "section": "The Paradox Revealed",
    "text": "The Paradox Revealed\nMost departments admitted a higher percentage of female applicants.\n\nWhat happened? Women applied disproportionately to departments with lower overall admission rates.\n\n\nAggregating across departments reversed the apparent pattern.\n\n\n\n\n\n\n\n\nML Connection\n\n\nA feature (gender) appeared predictive of the outcome (admission) only because both were associated with a confounder (department choice). Stratify by potential confounders."
  },
  {
    "objectID": "study-design-slides.html#uc-berkeley-the-sampling-lesson",
    "href": "study-design-slides.html#uc-berkeley-the-sampling-lesson",
    "title": "Sampling and Study Design",
    "section": "UC Berkeley: The Sampling Lesson",
    "text": "UC Berkeley: The Sampling Lesson\n\nData included all applicants in 1973 (not a sample)\nStandard statistical formulas assume probability sampling\nIf data aren’t from a probability sample, standard errors don’t apply\n\n\n\n\n\n\n\n\nML Connection\n\n\nYour test set is a sample; your deployment data may not be from the same distribution.\n\n\n\n\nThis is a subtle but important point. Many ML practitioners compute confidence intervals on test metrics, but if the test set isn’t representative of deployment, those intervals are meaningless."
  },
  {
    "objectID": "study-design-slides.html#why-experiment",
    "href": "study-design-slides.html#why-experiment",
    "title": "Sampling and Study Design",
    "section": "Why Experiment?",
    "text": "Why Experiment?\nObservational data: Subjects choose their own “treatment”\n\nConfounding: Factors that affect both treatment choice and outcome\n\n\nExperiment: Researcher assigns treatment, breaking confounding\n\nDraw a simple DAG on the board if you have time: Confounder → Treatment → Outcome, with Confounder also → Outcome. Randomization breaks the Confounder → Treatment arrow."
  },
  {
    "objectID": "study-design-slides.html#salk-vaccine-trial-nfip-design",
    "href": "study-design-slides.html#salk-vaccine-trial-nfip-design",
    "title": "Sampling and Study Design",
    "section": "Salk Vaccine Trial: NFIP Design",
    "text": "Salk Vaccine Trial: NFIP Design\n\n\n\n\n\nGrade\nGroup\nSize\nPolio Rate (per 100k)\n\n\n\n\n2\ntreatment\n225000\n25\n\n\n1, 3\ncontrol\n725000\n54\n\n\n2\nno_consent\n125000\n44\n\n\n\n\n\n\nProblems:\n\nGrade might affect polio transmission\nConsent might be confounded with risk factors"
  },
  {
    "objectID": "study-design-slides.html#salk-vaccine-trial-double-blind-design",
    "href": "study-design-slides.html#salk-vaccine-trial-double-blind-design",
    "title": "Sampling and Study Design",
    "section": "Salk Vaccine Trial: Double-Blind Design",
    "text": "Salk Vaccine Trial: Double-Blind Design\n\n\n\n\n\nGroup\nSize\nPolio Rate (per 100k)\n\n\n\n\ntreatment\n200000\n28\n\n\ncontrol\n200000\n71\n\n\nno_consent\n350000\n46\n\n\n\n\n\n\nKey insight: Non-consent group had lower rate than placebo group.\nConsent itself was confounded with risk!\n\nHigher SES parents were more likely to consent, but their children were at higher risk (less early exposure → less natural immunity). This is a beautiful example of confounding that only became visible with proper randomization."
  },
  {
    "objectID": "study-design-slides.html#double-blind-design-why-it-matters",
    "href": "study-design-slides.html#double-blind-design-why-it-matters",
    "title": "Sampling and Study Design",
    "section": "Double-Blind Design: Why It Matters",
    "text": "Double-Blind Design: Why It Matters\nRandomization: Consenting parents’ children assigned to vaccine or placebo by chance\nBlinding: Neither parents, doctors, nor evaluators knew assignment\n\nThis eliminates:\n\nSelection bias in treatment assignment\nUnconscious bias in outcome measurement\n\n\n\n\n\n\n\nML Connection\n\n\nIn supervised ML, annotators should not have access to information that could bias their labels. The NFIP’s initial design mirrors label leakage."
  },
  {
    "objectID": "study-design-slides.html#the-portacaval-shunt-design-determines-conclusions",
    "href": "study-design-slides.html#the-portacaval-shunt-design-determines-conclusions",
    "title": "Sampling and Study Design",
    "section": "The Portacaval Shunt: Design Determines Conclusions",
    "text": "The Portacaval Shunt: Design Determines Conclusions\n\n\n\n\n\nStudy Design\nMarked Enthusiasm\nModerate\nNone\n\n\n\n\nno controls\n24\n7\n1\n\n\ncontrols not randomized\n10\n3\n2\n\n\nrandomized controlled\n0\n1\n3\n\n\n\n\n\n\nThe same surgery looked beneficial or useless depending on study design.\n\nThis is one of the most striking tables in all of statistics. 75% of uncontrolled studies were enthusiastic; 0% of randomized studies were. The surgery doesn’t work, but bad study design made it look like it did."
  },
  {
    "objectID": "study-design-slides.html#the-mechanism-of-bias",
    "href": "study-design-slides.html#the-mechanism-of-bias",
    "title": "Sampling and Study Design",
    "section": "The Mechanism of Bias",
    "text": "The Mechanism of Bias\n\n\n\n\n\nDesign\nSurgery Survival %\nControl Survival %\n\n\n\n\nrandomized\n60\n60\n\n\nnot randomized\n60\n45\n\n\n\n\n\n\nSurgery patients: ~60% survival in both study types\nControl patients: 60% (randomized) vs 45% (non-randomized)\n\n\nNon-randomized studies used sicker patients as controls.\n\n\n\n\n\n\nML Connection\n\n\nEvaluating a model on a non-exchangeable test set overstates performance. Randomized train/test splits guard against this bias."
  },
  {
    "objectID": "study-design-slides.html#experimental-design-principles",
    "href": "study-design-slides.html#experimental-design-principles",
    "title": "Sampling and Study Design",
    "section": "Experimental Design Principles",
    "text": "Experimental Design Principles\n\nRandomization: Breaks confounding\nControl group: Provides counterfactual\n\nBlinding: Prevents unconscious bias in measurement\n\n\n\n\n\n\n\n\nML Connection\n\n\nA/B tests are randomized controlled experiments. Observational “causal” claims require strong assumptions."
  },
  {
    "objectID": "study-design-slides.html#nb10-precision-measurement",
    "href": "study-design-slides.html#nb10-precision-measurement",
    "title": "Sampling and Study Design",
    "section": "NB10: Precision Measurement",
    "text": "NB10: Precision Measurement\nNB10: A standard weight, nominally 10 grams\nStudy: 100 measurements under identical conditions at the National Bureau of Standards\n\n\nFigure 1"
  },
  {
    "objectID": "study-design-slides.html#bias-vs.-chance-error",
    "href": "study-design-slides.html#bias-vs.-chance-error",
    "title": "Sampling and Study Design",
    "section": "Bias vs. Chance Error",
    "text": "Bias vs. Chance Error\nEach measurement = true value + bias + chance error\n\n\nChance error: Varies randomly, averages toward zero\nBias: Systematic, does not average out\n\n\n\nNB10 is biased: It weighs ~405 μg less than 10g\n\n\n\n\n\n\n\n\nML Connection\n\n\nModel error = bias + variance. EDA reveals both."
  },
  {
    "objectID": "study-design-slides.html#how-accurate-is-the-sample-average",
    "href": "study-design-slides.html#how-accurate-is-the-sample-average",
    "title": "Sampling and Study Design",
    "section": "How Accurate is the Sample Average?",
    "text": "How Accurate is the Sample Average?\n\\[\\text{SE of mean} = \\frac{\\text{SD}}{\\sqrt{n}}\\]\n\nFor NB10:\n\\[\\text{SE} = \\frac{6.5}{\\sqrt{100}} = 0.65 \\text{ micrograms}\\]\n\n\nOur estimate of 405 μg is probably within ~2 μg of the true value.\n\n\n\n\n\n\n\n\nKey Insight\n\n\nUncertainty shrinks with \\(\\sqrt{n}\\), not \\(n\\). Precision and accuracy are distinct."
  },
  {
    "objectID": "study-design-slides.html#outliers-and-non-normality",
    "href": "study-design-slides.html#outliers-and-non-normality",
    "title": "Sampling and Study Design",
    "section": "Outliers and Non-Normality",
    "text": "Outliers and Non-Normality\n\n\nFigure 2\nMeasurements at z ≈ ±5 would be &lt; 1 in a million under normality.\nEven careful measurement processes produce non-normal tails.\n\n\n\n\n\n\nML Connection\n\n\nDon’t assume normality. Look at your data."
  },
  {
    "objectID": "study-design-slides.html#eda-bridges-design-and-modeling",
    "href": "study-design-slides.html#eda-bridges-design-and-modeling",
    "title": "Sampling and Study Design",
    "section": "EDA Bridges Design and Modeling",
    "text": "EDA Bridges Design and Modeling\nStudy design determines what data could be collected.\n\nEDA reveals what was actually collected.\n\n\nEDA answers:\n\nDoes the feature distribution match deployment expectations?\nAre there systematic patterns in missing data?\nDo labels exhibit expected reliability?\nAre there outliers warranting investigation?\n\n\nThis is the bridge to the rest of the course. Study design sets intentions; EDA verifies whether those intentions were achieved."
  },
  {
    "objectID": "study-design-slides.html#the-bottom-line",
    "href": "study-design-slides.html#the-bottom-line",
    "title": "Sampling and Study Design",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nNo algorithm can overcome fundamentally flawed data collection.\n\nEDA is the diagnostic step that reveals whether data support the intended use.\n\n\nThe examples in this chapter—from the Literary Digest poll to the NB10 measurements—show that study design flaws are often invisible until the data are examined."
  },
  {
    "objectID": "study-design-slides.html#key-takeaways",
    "href": "study-design-slides.html#key-takeaways",
    "title": "Sampling and Study Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nHow data are collected determines what conclusions are valid\nSample size without representative sampling is worthless\nObservational data cannot establish causation without strong assumptions\nRandomized experiments are the gold standard for causal claims\nAll estimates have uncertainty—quantify it"
  },
  {
    "objectID": "study-design-slides.html#key-concepts",
    "href": "study-design-slides.html#key-concepts",
    "title": "Sampling and Study Design",
    "section": "Key Concepts",
    "text": "Key Concepts\n\n\n\nConcept\nDefinition\n\n\n\n\nSelection bias\nSample systematically differs from population\n\n\nConfounding\nThird variable creates spurious association\n\n\nRandomization\nAssignment by chance mechanism\n\n\nDouble-blind\nNeither subject nor evaluator knows assignment\n\n\nStandard error\nSD of a sample statistic: \\(\\sigma / \\sqrt{n}\\)"
  },
  {
    "objectID": "study-design-slides.html#looking-ahead-ml-implications",
    "href": "study-design-slides.html#looking-ahead-ml-implications",
    "title": "Sampling and Study Design",
    "section": "Looking Ahead: ML Implications",
    "text": "Looking Ahead: ML Implications\n\nTrain/validation/test splits are sampling problems\nDistribution shift: Deployment ≠ training distribution\nFairness: If subgroups are undersampled, models underperform for them\nCausal inference: When can observational data support cause-and-effect claims?\n\n\nThese are the bridges to later ML material. Each of these could be a full lecture in its own right."
  },
  {
    "objectID": "study-design-slides.html#team-exercise-1-observational-studies-at-work",
    "href": "study-design-slides.html#team-exercise-1-observational-studies-at-work",
    "title": "Sampling and Study Design",
    "section": "Team Exercise 1: Observational Studies at Work",
    "text": "Team Exercise 1: Observational Studies at Work\nBreak into teams:\n\nIdentify 2–3 examples of observational studies in your work context.\nWhich would be most valuable? What questions would they answer?\nWhat are the potential pitfalls (confounding, selection bias, etc.)?\nCould any be converted to experiments? At what cost?"
  },
  {
    "objectID": "study-design-slides.html#team-exercise-2-the-quiz-puzzle",
    "href": "study-design-slides.html#team-exercise-2-the-quiz-puzzle",
    "title": "Sampling and Study Design",
    "section": "Team Exercise 2: The Quiz Puzzle",
    "text": "Team Exercise 2: The Quiz Puzzle\nA TA gives a 10-question quiz. After grading:\n\nAverage number right: 6.4, SD: 2.0\nAverage number wrong: [?], SD: [?]\n\nFill in the blanks—or do you need the raw data? Explain briefly."
  },
  {
    "objectID": "study-design-slides.html#team-exercise-3-left-handedness-puzzle",
    "href": "study-design-slides.html#team-exercise-3-left-handedness-puzzle",
    "title": "Sampling and Study Design",
    "section": "Team Exercise 3: Left-Handedness Puzzle",
    "text": "Team Exercise 3: Left-Handedness Puzzle\nIn a large health survey, the percentage of left-handed respondents decreased from 10% at age 20 to 4% at age 70.\n“The data show that many people change from left-handed to right-handed as they get older.”\n\nTrue or false? Explain.\nIf false, what explains the pattern?\nWhat study design would test your alternative hypothesis?"
  },
  {
    "objectID": "study-design-slides.html#team-exercise-4-normal-distribution",
    "href": "study-design-slides.html#team-exercise-4-normal-distribution",
    "title": "Sampling and Study Design",
    "section": "Team Exercise 4: Normal Distribution",
    "text": "Team Exercise 4: Normal Distribution\nThe 25th percentile of height is 62.2 inches; the 75th is 65.8 inches. If the distribution is normal, find the 90th percentile.\n\nExercise 1 grounds concepts in participants’ experience. Exercises 2–3 are from FPP and test reasoning about study design. Exercise 4 refreshes familiarity with the normal (Gaussian) distribution."
  },
  {
    "objectID": "study-design-slides.html#discussion-questions",
    "href": "study-design-slides.html#discussion-questions",
    "title": "Sampling and Study Design",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhat modern datasets might have Literary Digest-style selection bias?\nWhen is a randomized experiment unethical or impractical?\nHow would you detect distribution shift between training and deployment?\n\n\nThese are the FPP exercises from the chapter. They reinforce the core concepts and can be used as in-class activities or homework."
  },
  {
    "objectID": "study-design-slides.html#timing-guide",
    "href": "study-design-slides.html#timing-guide",
    "title": "Sampling and Study Design",
    "section": "Timing Guide",
    "text": "Timing Guide\n\n\n\nSection\nSlides\nSuggested Time\n\n\n\n\nOpening\n1-2\n5 min\n\n\nObservational Studies\n3-12\n25 min\n\n\nExperimental Studies\n13-20\n20 min\n\n\nMeasurement/Uncertainty\n21-24\n15 min\n\n\nRole of EDA\n25-26\n5 min\n\n\nSynthesis\n27-32\n15 min\n\n\n\nTotal: ~85 minutes with discussion"
  },
  {
    "objectID": "study-design-slides.html#customization-options",
    "href": "study-design-slides.html#customization-options",
    "title": "Sampling and Study Design",
    "section": "Customization Options",
    "text": "Customization Options\nFor shorter sessions (50 min): Cut slides 8, 12, 24; condense synthesis\nFor longer sessions: Expand Simpson’s Paradox with actual department data; add more discussion time\nFor ML-focused audience: Expand “ML Connection” callouts with code examples\nFor statistics-focused audience: Show the mathematical derivations from FPP"
  },
  {
    "objectID": "study-design-slides.html#data-sources",
    "href": "study-design-slides.html#data-sources",
    "title": "Sampling and Study Design",
    "section": "Data Sources",
    "text": "Data Sources\nAll FPP datasets are loaded from the eda4mldata package:\n\nlit_digest — Literary Digest 1936 poll\ntruman_dewey — 1948 election predictions\nus_elections — Gallup accuracy 1952-2004\nsalk_nfip — Salk vaccine NFIP design\nsalk_blind — Salk vaccine double-blind design\nportacaval_studies — Portacaval shunt study results\nportacaval_survival — Shunt survival rates\nnb10 — NB10 weight measurements"
  },
  {
    "objectID": "topic-models-slides.html#from-chapter-10",
    "href": "topic-models-slides.html#from-chapter-10",
    "title": "Topic Models",
    "section": "From Chapter 10",
    "text": "From Chapter 10\nWe established:\n\n\nText → tokens → term-document matrix\n\n\n\n\nTF-IDF surfaces distinctive words\n\n\n\n\nTopic models discover latent themes\n\n\n\nNow we go deeper: How do topic models work?"
  },
  {
    "objectID": "topic-models-slides.html#the-core-insight",
    "href": "topic-models-slides.html#the-core-insight",
    "title": "Topic Models",
    "section": "The Core Insight",
    "text": "The Core Insight\nA topic model makes two key assumptions:\n\n\nEach document is a mixture of topics\n\n“This article is 70% business, 30% politics”\n\n\n\n\n\nEach topic is a mixture of words\n\nBusiness topic: “market”, “profit”, “growth” have high probability\nPolitics topic: “congress”, “vote”, “campaign” have high probability"
  },
  {
    "objectID": "topic-models-slides.html#the-generative-story",
    "href": "topic-models-slides.html#the-generative-story",
    "title": "Topic Models",
    "section": "The Generative Story",
    "text": "The Generative Story\nImagine generating a document:\n\nStep 1: Choose a mixture of topics for this document\n\nTopic mixture: “70% Topic A, 30% Topic B”\n\n\n\nStep 2: For each word position:\n\nRoll the topic dice → generate a topic\nFrom that topic’s word distribution → generate a word\n\n\n\nLDA inverts this process: Given documents, infer the topics."
  },
  {
    "objectID": "topic-models-slides.html#the-experiment",
    "href": "topic-models-slides.html#the-experiment",
    "title": "Topic Models",
    "section": "The Experiment",
    "text": "The Experiment\nCan LDA distinguish two very different books?\n\nAnimal Farm (George Orwell, 1945)\n\nFarm animals overthrow humans, pigs become tyrants\nAllegory for Soviet communism\n\n\n\nThe Butter Battle Book (Dr. Seuss, 1984)\n\nYooks vs Zooks over which side to butter bread\nAllegory for Cold War nuclear arms race\n\n\n\nWe use Wikipedia’s plot summaries, treating each paragraph as a “document.”"
  },
  {
    "objectID": "topic-models-slides.html#loading-the-texts",
    "href": "topic-models-slides.html#loading-the-texts",
    "title": "Topic Models",
    "section": "Loading the Texts",
    "text": "Loading the Texts\nOur mini-corpus of paragraphs within plot summaries:\n\n\n\n\n\nbook\nn_paragraphs\nn_words\n\n\n\n\nAnimal Farm\n5\n553\n\n\nButter Battle Book\n4\n172"
  },
  {
    "objectID": "topic-models-slides.html#building-the-document-term-matrix",
    "href": "topic-models-slides.html#building-the-document-term-matrix",
    "title": "Topic Models",
    "section": "Building the Document-Term Matrix",
    "text": "Building the Document-Term Matrix\n\n\nCode\n# Count words per paragraph (our \"documents\")\naf_bbb_counts &lt;- af_bbb_token_tbl |&gt;\n  dplyr::mutate(doc_id = paste(src, pdx, sep = \"_\")) |&gt;\n  dplyr::count(doc_id, word)\n\n# Convert to document-term matrix\naf_bbb_dtm &lt;- af_bbb_counts |&gt;\n  tidytext::cast_dtm(document = doc_id, term = word, value = n)\n\naf_bbb_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 9, terms: 486)&gt;&gt;\nNon-/sparse entries: 598/3776\nSparsity           : 86%\nMaximal term length: 18\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "topic-models-slides.html#fitting-lda-with-k-2",
    "href": "topic-models-slides.html#fitting-lda-with-k-2",
    "title": "Topic Models",
    "section": "Fitting LDA with K = 2",
    "text": "Fitting LDA with K = 2\n\n\nCode\naf_bbb_lda &lt;- topicmodels::LDA(\n  x      = af_bbb_dtm,\n  k      = 2,\n  method = \"Gibbs\",\n  control = list(seed = 42)\n)\n\n\n\nWe asked for 2 topics—matching our 2 source books.\nCan LDA recover the distinction?"
  },
  {
    "objectID": "topic-models-slides.html#word-distribution-beta-per-topic",
    "href": "topic-models-slides.html#word-distribution-beta-per-topic",
    "title": "Topic Models",
    "section": "Word Distribution \\((\\beta)\\) per Topic",
    "text": "Word Distribution \\((\\beta)\\) per Topic\n\n\nFigure 1: Top words in each topic (Animal Farm vs Butter Battle)"
  },
  {
    "objectID": "topic-models-slides.html#interpreting-the-results",
    "href": "topic-models-slides.html#interpreting-the-results",
    "title": "Topic Models",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\nTopic 1 top words: animals, napoleon, farm, snowball…\n→ Animal Farm ✓\n\nTopic 2 top words: yooks, zooks, wall, battle…\n→ The Butter Battle Book ✓\n\n\nLDA successfully separated the two sources—without being told which paragraphs came from which book."
  },
  {
    "objectID": "topic-models-slides.html#log-ratio-visualization",
    "href": "topic-models-slides.html#log-ratio-visualization",
    "title": "Topic Models",
    "section": "Log-Ratio Visualization",
    "text": "Log-Ratio Visualization\nWhich words most distinguish the topics?\n\n\nFigure 2: Log₂ ratio: positive = Topic 2 (Butter Battle), negative = Topic 1 (Animal Farm)"
  },
  {
    "objectID": "topic-models-slides.html#toy-example-takeaways",
    "href": "topic-models-slides.html#toy-example-takeaways",
    "title": "Topic Models",
    "section": "Toy Example: Takeaways",
    "text": "Toy Example: Takeaways\n\n\nLDA works: It found the two sources without supervision\n\n\n\n\nInterpretation requires judgment: “yooks” means nothing without context\n\n\n\n\nLimitations exposed: With only 9 documents (paragraphs), this is a toy. Real corpora have thousands of documents.\n\n\n\nLet’s see what happens at scale."
  },
  {
    "objectID": "topic-models-slides.html#associated-press-articles",
    "href": "topic-models-slides.html#associated-press-articles",
    "title": "Topic Models",
    "section": "Associated Press Articles",
    "text": "Associated Press Articles\n2,246 news articles from the Associated Press.\n\nIn Chapter 10 we fit \\(K = 2\\) topics and found:\n\nTopic 1: Business/Finance\nTopic 2: Politics/Government\n\n\n\nNow: What if we ask for \\(K = 4\\) topics?"
  },
  {
    "objectID": "topic-models-slides.html#fitting-lda-with-k-4",
    "href": "topic-models-slides.html#fitting-lda-with-k-4",
    "title": "Topic Models",
    "section": "Fitting LDA with K = 4",
    "text": "Fitting LDA with K = 4"
  },
  {
    "objectID": "topic-models-slides.html#four-topics-discovered",
    "href": "topic-models-slides.html#four-topics-discovered",
    "title": "Topic Models",
    "section": "Four Topics Discovered",
    "text": "Four Topics Discovered\n\n\nFigure 3: Top words in each of 4 topics (AP articles)"
  },
  {
    "objectID": "topic-models-slides.html#interpreting-the-four-topics",
    "href": "topic-models-slides.html#interpreting-the-four-topics",
    "title": "Topic Models",
    "section": "Interpreting the Four Topics",
    "text": "Interpreting the Four Topics\n\n\n\n\nTopic\nTop Words\nInterpretation\n\n\n\n\n1\npercent, million, market, stock\nBusiness/Finance\n\n\n2\npresident, government, party\nPolitics (Domestic)\n\n\n3\nsoviet, united, states, military\nInternational/Cold War\n\n\n4\npolice, people, city, reported\nLocal News/Crime\n\n\n\n\n\nThese interpretations require domain knowledge. The algorithm provides the word distributions; you provide the labels."
  },
  {
    "objectID": "topic-models-slides.html#topic-mixtures-per-document-gamma",
    "href": "topic-models-slides.html#topic-mixtures-per-document-gamma",
    "title": "Topic Models",
    "section": "Topic Mixtures per Document \\((\\gamma)\\)",
    "text": "Topic Mixtures per Document \\((\\gamma)\\)\nEach document has a mixture of topics:\n\n\n\n\n\nDoc ID\nTopic 1\nTopic 2\nTopic 3\nTopic 4\n\n\n\n\n195\n0.53\n0.16\n0.14\n0.18\n\n\n526\n0.18\n0.44\n0.15\n0.23\n\n\n1842\n0.12\n0.62\n0.13\n0.13\n\n\n2227\n0.10\n0.09\n0.72\n0.09\n\n\n\n\n\n\nMost documents lean toward one topic, but more even mixtures are common."
  },
  {
    "objectID": "topic-models-slides.html#visualizing-document-mixtures",
    "href": "topic-models-slides.html#visualizing-document-mixtures",
    "title": "Topic Models",
    "section": "Visualizing Document Mixtures",
    "text": "Visualizing Document Mixtures\n\n\nFigure 4: Distribution of dominant topic proportions\n\nA few articles are dominated by one topic \\((\\gamma &gt; 0.8)\\), but most articles are a mixture of two or more topics."
  },
  {
    "objectID": "topic-models-slides.html#why-dirichlet-in-lda",
    "href": "topic-models-slides.html#why-dirichlet-in-lda",
    "title": "Topic Models",
    "section": "Why “Dirichlet” in LDA?",
    "text": "Why “Dirichlet” in LDA?\nLDA needs a probability distribution over mixtures.\n\n\nEach document has a particular mixture of topics\nThe mixture is given as probability vector like (0.7, 0.3)\n\n\n\nThe Dirichlet distribution is a distribution over such probability vectors."
  },
  {
    "objectID": "topic-models-slides.html#a-distribution-over-mixtures",
    "href": "topic-models-slides.html#a-distribution-over-mixtures",
    "title": "Topic Models",
    "section": "A Distribution Over Mixtures",
    "text": "A Distribution Over Mixtures\n\nSamples from Dirichlet distributions with different concentrations"
  },
  {
    "objectID": "topic-models-slides.html#the-concentration-parameter",
    "href": "topic-models-slides.html#the-concentration-parameter",
    "title": "Topic Models",
    "section": "The Concentration Parameter",
    "text": "The Concentration Parameter\nThe Dirichlet has a concentration parameter \\(\\alpha\\):\n\nSmall \\(\\alpha\\) (&lt; 1): Sparse mixtures\n\nDocuments tend toward one dominant topic\n“This article is 95% politics”\n\n\n\nLarge \\(\\alpha\\) (&gt; 1): Uniform mixtures\n\nDocuments spread across many topics\n“This article is 30% politics, 35% business, 35% international”\n\n\n\nLDA typically uses small \\(\\alpha\\), reflecting the intuition that documents are usually about something specific."
  },
  {
    "objectID": "topic-models-slides.html#dirichlet-in-the-lda-model",
    "href": "topic-models-slides.html#dirichlet-in-the-lda-model",
    "title": "Topic Models",
    "section": "Dirichlet in the LDA Model",
    "text": "Dirichlet in the LDA Model\nLDA uses two Dirichlet distributions:\n\n\nα controls topic mixtures per document \\((\\gamma)\\)\n\n“How focused are documents on single topics?”\n\n\n\n\n\nβ (sometimes called \\(\\delta\\) or \\(\\eta\\)) controls word mixtures per topic\n\n“How focused are topics on specific words?”\n\n\n\n\nThese are hyperparameters you can tune—but defaults often work well."
  },
  {
    "objectID": "topic-models-slides.html#the-key-question",
    "href": "topic-models-slides.html#the-key-question",
    "title": "Topic Models",
    "section": "The Key Question",
    "text": "The Key Question\nHow many topics should you request?\n\nThis is analogous to choosing \\(K\\) in K-means clustering.\n\n\nToo few topics: Themes are too broad\n\nBusiness and finance lumped together\nImportant distinctions lost\n\n\n\nToo many topics: Themes are too narrow\n\nRedundant topics\nHard to interpret"
  },
  {
    "objectID": "topic-models-slides.html#no-perfect-answer",
    "href": "topic-models-slides.html#no-perfect-answer",
    "title": "Topic Models",
    "section": "No Perfect Answer",
    "text": "No Perfect Answer\nUnlike clustering, there’s no “elbow” method that works reliably.\n\nPractical approaches:\n\n\n\nDomain knowledge: “I expect 5–10 themes in this corpus”\n\n\n\n\nInterpretability: Can you name each topic? Do the top words cohere?\n\n\n\n\nIteration: Fit K = 2, 3, 5, 10. Compare results.\n\n\n\n\nCoherence metrics: Statistical measures of topic quality (advanced)"
  },
  {
    "objectID": "topic-models-slides.html#example-k-2-vs-k-4-vs-k-8",
    "href": "topic-models-slides.html#example-k-2-vs-k-4-vs-k-8",
    "title": "Topic Models",
    "section": "Example: K = 2 vs K = 4 vs K = 8",
    "text": "Example: K = 2 vs K = 4 vs K = 8\nK = 2: i, people, two, president, government\n\nK = 4: Splits into Business, Politics, International, Local\n\n\nK = 8: Further splits may reveal Sports, Legal, Technology…\n\n\n…or may produce redundant, uninterpretable topics."
  },
  {
    "objectID": "topic-models-slides.html#guidance-for-choosing-k",
    "href": "topic-models-slides.html#guidance-for-choosing-k",
    "title": "Topic Models",
    "section": "Guidance for Choosing K",
    "text": "Guidance for Choosing K\n\n\n\n\n\n\nPractical Recommendations\n\n\n\nStart with domain intuition — How many themes do you expect?\nTry a range — e.g., K = 2, 3, 5, 10\nEvaluate interpretability — Can you name each topic?\nLook for redundancy — Do topics overlap too much?\nConsider your goal — Exploration (fewer topics) vs. fine-grained analysis (more topics)"
  },
  {
    "objectID": "topic-models-slides.html#key-takeaways",
    "href": "topic-models-slides.html#key-takeaways",
    "title": "Topic Models",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nTopic models assume documents are mixtures of topics, and topics are mixtures of words\n\n\n\nLDA discovers both word distributions \\((\\beta)\\) per topic and topic distributions \\((\\gamma)\\) per document\n\n\n\n\nInterpretation requires judgment — the algorithm finds patterns; you provide meaning\n\n\n\n\nChoosing K is an art — iterate, evaluate, use domain knowledge\n\n\n\n\nThe Dirichlet distribution provides the “mixture over mixtures” structure LDA needs"
  },
  {
    "objectID": "topic-models-slides.html#key-notation",
    "href": "topic-models-slides.html#key-notation",
    "title": "Topic Models",
    "section": "Key Notation",
    "text": "Key Notation\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(K\\)\nNumber of topics (you choose)\n\n\n\\(\\alpha\\)\nDirichlet concentration of topic mixtures per doc\n\n\n\\(\\beta_{k,v}\\)\nProbability of term \\(v\\) in topic \\(k\\)\n\n\n\\(\\gamma_{d,k}\\)\nProportion of topic \\(k\\) in document \\(d\\)\n\n\n\n\nFor each topic \\(k\\): \\(\\sum_v \\beta_{k,v} = 1\\)\nFor each document \\(d\\): \\(\\sum_k \\gamma_{d,k} = 1\\)"
  },
  {
    "objectID": "topic-models-slides.html#connections-to-earlier-material",
    "href": "topic-models-slides.html#connections-to-earlier-material",
    "title": "Topic Models",
    "section": "Connections to Earlier Material",
    "text": "Connections to Earlier Material\n\n\n\nEarlier Concept\nTopic Model Analog\n\n\n\n\nK-means clustering\nLDA (documents → topics)\n\n\nCluster centroids\nWord mixtures (β) per topic\n\n\nCluster assignments\nTopic mixtures (γ) per document\n\n\nChoosing K\nChoosing K\n\n\nPCA loadings\nWord weights per topic\n\n\n\n\nTopic models are “soft clustering” — documents belong partially to multiple topics."
  },
  {
    "objectID": "topic-models-slides.html#the-two-ldas-resolved",
    "href": "topic-models-slides.html#the-two-ldas-resolved",
    "title": "Topic Models",
    "section": "The Two LDAs Resolved",
    "text": "The Two LDAs Resolved\n\n\n\n\n\n\nTwo Different Techniques\n\n\nLinear Discriminant Analysis (Chapter 9)\n\nSupervised: uses class labels\nFinds directions that separate known classes\n\nLatent Dirichlet Allocation (Chapter 11)\n\nUnsupervised: discovers latent structure\n\nFinds topics from word co-occurrence\n\nBoth “allocate” observations to categories—but with very different goals."
  },
  {
    "objectID": "topic-models-slides.html#team-exercise-1-interpret-topics",
    "href": "topic-models-slides.html#team-exercise-1-interpret-topics",
    "title": "Topic Models",
    "section": "Team Exercise 1: Interpret Topics",
    "text": "Team Exercise 1: Interpret Topics\nUsing the AP News K = 4 results:\n\nPropose alternative labels for each topic. Justify with top words.\nWhich topic is most coherent? Least coherent? Why?\nFind a word that appears in multiple topics. What does this suggest?"
  },
  {
    "objectID": "topic-models-slides.html#team-exercise-2-document-mixtures",
    "href": "topic-models-slides.html#team-exercise-2-document-mixtures",
    "title": "Topic Models",
    "section": "Team Exercise 2: Document Mixtures",
    "text": "Team Exercise 2: Document Mixtures\nFor a document with \\(\\gamma = (0.4, 0.3, 0.2, 0.1)\\):\n\nWhat does this mixture tell you about the document?\nHow would you summarize this to a non-technical colleague?\nWould you classify this document into one topic or multiple? Why?"
  },
  {
    "objectID": "topic-models-slides.html#team-exercise-3-choosing-k",
    "href": "topic-models-slides.html#team-exercise-3-choosing-k",
    "title": "Topic Models",
    "section": "Team Exercise 3: Choosing K",
    "text": "Team Exercise 3: Choosing K\nYou have a corpus of 5,000 customer support tickets.\n\nWhat K would you start with? Why?\nDesign an evaluation process to compare different values of K.\nHow would you explain your choice to stakeholders?"
  },
  {
    "objectID": "topic-models-slides.html#team-exercise-4-lda-vs-llms",
    "href": "topic-models-slides.html#team-exercise-4-lda-vs-llms",
    "title": "Topic Models",
    "section": "Team Exercise 4: LDA vs LLMs",
    "text": "Team Exercise 4: LDA vs LLMs\nFor theme discovery in a large corpus:\n\nWhen would you prefer LDA over asking an LLM?\nWhen would you prefer an LLM over LDA?\nDesign a hybrid workflow combining both approaches."
  },
  {
    "objectID": "topic-models-slides.html#references",
    "href": "topic-models-slides.html#references",
    "title": "Topic Models",
    "section": "References",
    "text": "References\nFoundational:\n\nBlei, Ng, & Jordan (2003). Latent Dirichlet Allocation. JMLR.\nSteyvers & Griffiths (2007). Probabilistic Topic Models. In Handbook of Latent Semantic Analysis.\n\nPractical:\n\nSilge & Robinson. Text Mining with R, Chapter 6.\nAntoniak (2023). LDA in Practice. Blog post with practical advice."
  },
  {
    "objectID": "topic-models-slides.html#r-functions-reference",
    "href": "topic-models-slides.html#r-functions-reference",
    "title": "Topic Models",
    "section": "R Functions Reference",
    "text": "R Functions Reference\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nLDA()\ntopicmodels\nFit LDA model\n\n\ntidy()\ntidytext\nExtract β or γ matrices\n\n\ncast_dtm()\ntidytext\nCreate document-term matrix\n\n\nperplexity()\ntopicmodels\nModel fit metric"
  },
  {
    "objectID": "ts-freq-domain-slides.html#from-forecasting-to-understanding",
    "href": "ts-freq-domain-slides.html#from-forecasting-to-understanding",
    "title": "Frequency Domain Methods",
    "section": "From Forecasting to Understanding",
    "text": "From Forecasting to Understanding\nChapter 13 developed time domain methods—ARIMA models that exploit autocorrelation for forecasting.\n\nThis chapter takes the complementary view: frequency domain methods ask what periodic components are present.\n\n\n\n\n\n\n\n\n\n\nPerspective\nCentral Question\nEmphasis\n\n\n\n\nTime domain\nHow does the past predict the future?\nDecision support\n\n\nFrequency domain\nWhat cycles and rhythms structure the process?\nScientific understanding"
  },
  {
    "objectID": "ts-freq-domain-slides.html#why-frequency",
    "href": "ts-freq-domain-slides.html#why-frequency",
    "title": "Frequency Domain Methods",
    "section": "Why Frequency?",
    "text": "Why Frequency?\nMany natural and human systems exhibit periodic behavior:\n\nSolar activity: ~11-year sunspot cycle\nClimate: annual temperature cycles, El Niño (~4 years)\nEconomics: business cycles, seasonal patterns\nBiology: circadian rhythms, heartbeat\n\n\nThe frequency perspective reveals these periodicities directly—as peaks in the spectrum."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-spectrum-variance-by-frequency",
    "href": "ts-freq-domain-slides.html#the-spectrum-variance-by-frequency",
    "title": "Frequency Domain Methods",
    "section": "The Spectrum: Variance by Frequency",
    "text": "The Spectrum: Variance by Frequency\nThe spectrum \\(f(\\lambda)\\) decomposes variance across frequencies:\n\\[\\sigma_X^2 = \\int_{-\\pi}^{\\pi} f(\\lambda) \\, d\\lambda\\]\n\n\nHigh \\(f(\\lambda)\\) at frequency \\(\\lambda\\) → strong oscillation at that frequency\nPeak in spectrum → dominant cycle in the data\nFlat spectrum → white noise (no preferred frequency)"
  },
  {
    "objectID": "ts-freq-domain-slides.html#complex-exponentials-as-eigenfunctions",
    "href": "ts-freq-domain-slides.html#complex-exponentials-as-eigenfunctions",
    "title": "Frequency Domain Methods",
    "section": "Complex Exponentials as Eigenfunctions",
    "text": "Complex Exponentials as Eigenfunctions\nWhy does Fourier analysis work so naturally for time series?\n\nComplex exponentials \\(e^{i\\lambda t}\\) are eigenfunctions of the back-shift operator:\n\\[\\mathcal{B}^s e^{i\\lambda t} = e^{i\\lambda(t-s)} = e^{-i\\lambda s} \\cdot e^{i\\lambda t}\\]\n\n\nAny linear, time-invariant operation (filtering, smoothing, differencing) acts simply on sinusoidal components—multiplying each by a constant.\n\n\nThis is why decomposing into frequency components is the natural coordinate system for stationary processes."
  },
  {
    "objectID": "ts-freq-domain-slides.html#acf-and-spectrum-fourier-pairs",
    "href": "ts-freq-domain-slides.html#acf-and-spectrum-fourier-pairs",
    "title": "Frequency Domain Methods",
    "section": "ACF and Spectrum: Fourier Pairs",
    "text": "ACF and Spectrum: Fourier Pairs\nThe spectrum and autocovariance contain the same information:\n\\[f(\\lambda) = \\sum_{h=-\\infty}^{\\infty} \\gamma(h) e^{-i\\lambda h}\\]\n\\[\\gamma(h) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(\\lambda) e^{i\\lambda h} \\, d\\lambda\\]\n\nThey are Fourier transform pairs. The ACF tells us about temporal dependence; the spectrum tells us about periodic structure. Same reality, different views."
  },
  {
    "objectID": "ts-freq-domain-slides.html#estimating-the-spectrum",
    "href": "ts-freq-domain-slides.html#estimating-the-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Estimating the Spectrum",
    "text": "Estimating the Spectrum\nGiven data \\(X(0), X(1), \\ldots, X(T-1)\\), how do we estimate \\(f(\\lambda)\\)?\n\nThe periodogram is the natural sample estimate:\n\\[I(\\lambda) = \\frac{1}{T} \\left| \\sum_{t=0}^{T-1} X(t) e^{-i\\lambda t} \\right|^2\\]\n\n\nThis is proportional to the squared magnitude of the finite Fourier transform of the data."
  },
  {
    "objectID": "ts-freq-domain-slides.html#periodogram-intuition",
    "href": "ts-freq-domain-slides.html#periodogram-intuition",
    "title": "Frequency Domain Methods",
    "section": "Periodogram: Intuition",
    "text": "Periodogram: Intuition\nThe periodogram measures how well a sinusoid at frequency \\(\\lambda\\) fits the data.\n\n\nLarge \\(I(\\lambda)\\) → the data oscillate strongly at frequency \\(\\lambda\\)\nSmall \\(I(\\lambda)\\) → little power at that frequency\n\n\n\nAt the Fourier frequencies \\(\\lambda_j = 2\\pi j / T\\), the periodogram can be computed efficiently via the Fast Fourier Transform (FFT)."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-periodogram-problem",
    "href": "ts-freq-domain-slides.html#the-periodogram-problem",
    "title": "Frequency Domain Methods",
    "section": "The Periodogram Problem",
    "text": "The Periodogram Problem\nThe periodogram is asymptotically unbiased:\n\\[E\\{I(\\lambda)\\} \\to f(\\lambda) \\text{ as } T \\to \\infty\\]\n\nBut it is not consistent—its variance does not shrink:\n\\[I(\\lambda) \\sim f(\\lambda) \\cdot \\frac{\\chi_2^2}{2} \\quad \\text{for } \\lambda \\not\\equiv 0 \\pmod{\\pi}\\]\n\n\nThe periodogram fluctuates wildly around the true spectrum, even with large samples."
  },
  {
    "objectID": "ts-freq-domain-slides.html#visualizing-periodogram-variability",
    "href": "ts-freq-domain-slides.html#visualizing-periodogram-variability",
    "title": "Frequency Domain Methods",
    "section": "Visualizing Periodogram Variability",
    "text": "Visualizing Periodogram Variability\n\n\nFigure 1: Raw periodogram of white noise: high variability around flat true spectrum\nThe true spectrum (red dashed) is constant, but the periodogram (blue) varies wildly."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-solution-averaging",
    "href": "ts-freq-domain-slides.html#the-solution-averaging",
    "title": "Frequency Domain Methods",
    "section": "The Solution: Averaging",
    "text": "The Solution: Averaging\nTo reduce variance, we smooth the periodogram by averaging over nearby frequencies.\n\n\\[\\hat{f}(\\lambda) = \\sum_j W_j \\cdot I(\\lambda_j)\\]\nwhere the weights \\(W_j\\) form a smoothing kernel centered at \\(\\lambda\\).\n\n\nMore averaging → lower variance, but also lower resolution (bias)."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-bandwidth-trade-off",
    "href": "ts-freq-domain-slides.html#the-bandwidth-trade-off",
    "title": "Frequency Domain Methods",
    "section": "The Bandwidth Trade-off",
    "text": "The Bandwidth Trade-off\n\n\n\nMore smoothing\nLess smoothing\n\n\n\n\nLower variance\nHigher variance\n\n\nBlurs nearby peaks\nResolves nearby peaks\n\n\nRisk: miss narrow features\nRisk: spurious peaks from noise\n\n\n\n\nThe bandwidth \\(\\beta\\) controls this trade-off.\n\n\nFor a consistent estimator, we need \\(\\beta \\to 0\\) but \\(\\beta \\cdot T \\to \\infty\\) as \\(T \\to \\infty\\)."
  },
  {
    "objectID": "ts-freq-domain-slides.html#modified-daniell-kernel",
    "href": "ts-freq-domain-slides.html#modified-daniell-kernel",
    "title": "Frequency Domain Methods",
    "section": "Modified Daniell Kernel",
    "text": "Modified Daniell Kernel\nA common choice is the modified Daniell kernel—a moving average over adjacent frequencies.\n\nIn R, stats::spec.pgram() uses this approach:\n\n\nCode\n# Smoothed spectrum estimate\nstats::spec.pgram(x, spans = c(5, 5), taper = 0.1)\n\n\n\n\n\nspans: widths of Daniell smoothers (can be repeated for more smoothing)\ntaper: proportion of data tapered at ends (reduces leakage)"
  },
  {
    "objectID": "ts-freq-domain-slides.html#smoothed-vs-raw-periodogram",
    "href": "ts-freq-domain-slides.html#smoothed-vs-raw-periodogram",
    "title": "Frequency Domain Methods",
    "section": "Smoothed vs Raw Periodogram",
    "text": "Smoothed vs Raw Periodogram\n\n\nFigure 2: Smoothing reduces variance while preserving the overall shape"
  },
  {
    "objectID": "ts-freq-domain-slides.html#identifying-periodic-components",
    "href": "ts-freq-domain-slides.html#identifying-periodic-components",
    "title": "Frequency Domain Methods",
    "section": "Identifying Periodic Components",
    "text": "Identifying Periodic Components\nWhen examining a spectrum estimate:\n\n\nPeaks indicate dominant frequencies (cycles)\n\nConvert frequency \\(\\lambda\\) to period: \\(P = 2\\pi/\\lambda\\) (in units of sampling interval)\n\n\n\n\n\nLow-frequency dominance suggests trend or long-memory behavior\n\n\n\n\nFlat spectrum indicates white noise (no temporal structure)\n\n\n\n\nLog scale often helps visualize structure across orders of magnitude"
  },
  {
    "objectID": "ts-freq-domain-slides.html#confidence-intervals",
    "href": "ts-freq-domain-slides.html#confidence-intervals",
    "title": "Frequency Domain Methods",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nFor the smoothed spectrum estimate with \\(\\nu\\) degrees of freedom:\n\\[\\frac{\\nu \\hat{f}(\\lambda)}{f(\\lambda)} \\sim \\chi_\\nu^2\\]\n\nThis gives a confidence interval for the true spectrum:\n\\[\\left[ \\frac{\\nu \\hat{f}(\\lambda)}{\\chi_{\\nu, 1-\\alpha/2}^2}, \\; \\frac{\\nu \\hat{f}(\\lambda)}{\\chi_{\\nu, \\alpha/2}^2} \\right]\\]\n\n\nNote: the interval is multiplicative, not additive—it’s the same width on a log scale at all frequencies."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-log-spectrum",
    "href": "ts-freq-domain-slides.html#the-log-spectrum",
    "title": "Frequency Domain Methods",
    "section": "The Log Spectrum",
    "text": "The Log Spectrum\nPlotting \\(\\log \\hat{f}(\\lambda)\\) has advantages:\n\n\nConfidence band has constant width across frequencies\nEasier to see structure spanning orders of magnitude\nMultiplicative effects (filtering) become additive\n\n\n\nR’s stats::spec.pgram() uses log scale by default."
  },
  {
    "objectID": "ts-freq-domain-slides.html#connecting-time-and-frequency-domains",
    "href": "ts-freq-domain-slides.html#connecting-time-and-frequency-domains",
    "title": "Frequency Domain Methods",
    "section": "Connecting Time and Frequency Domains",
    "text": "Connecting Time and Frequency Domains\nFor ARMA processes, we can derive the theoretical spectrum from the model parameters.\n\nThis connects Chapter 13’s models to Chapter 14’s frequency view.\n\n\nKey insight: AR processes have peaks where roots of \\(\\phi(z)\\) are near the unit circle."
  },
  {
    "objectID": "ts-freq-domain-slides.html#ar1-spectrum",
    "href": "ts-freq-domain-slides.html#ar1-spectrum",
    "title": "Frequency Domain Methods",
    "section": "AR(1) Spectrum",
    "text": "AR(1) Spectrum\nFor AR(1): \\(X(t) = \\phi X(t-1) + W(t)\\)\n\\[f(\\lambda) = \\frac{\\sigma_W^2}{|1 - \\phi e^{-i\\lambda}|^2} = \\frac{\\sigma_W^2}{1 - 2\\phi\\cos\\lambda + \\phi^2}\\]\n\n\n\n\n\n\n\n\n\nFigure 3: AR(1) spectra: positive φ emphasizes low frequencies; negative φ emphasizes high\n\n\n\n\n\n\n\\(\\phi &gt; 0\\): low-frequency dominance (persistence)\n\\(\\phi &lt; 0\\): high-frequency dominance (alternation)"
  },
  {
    "objectID": "ts-freq-domain-slides.html#ma1-spectrum",
    "href": "ts-freq-domain-slides.html#ma1-spectrum",
    "title": "Frequency Domain Methods",
    "section": "MA(1) Spectrum",
    "text": "MA(1) Spectrum\nFor MA(1): \\(X(t) = W(t) + \\theta W(t-1)\\)\n\\[f(\\lambda) = \\sigma_W^2 |1 + \\theta e^{-i\\lambda}|^2 = \\sigma_W^2 (1 + 2\\theta\\cos\\lambda + \\theta^2)\\]\n\n\n\n\n\n\n\n\n\nFigure 4: MA(1) spectra: opposite pattern from AR(1)\n\n\n\n\n\nThe MA(1) spectrum is the inverse pattern of AR(1) with the same parameter."
  },
  {
    "objectID": "ts-freq-domain-slides.html#ar2-spectral-peaks",
    "href": "ts-freq-domain-slides.html#ar2-spectral-peaks",
    "title": "Frequency Domain Methods",
    "section": "AR(2): Spectral Peaks",
    "text": "AR(2): Spectral Peaks\nAR(2) with complex roots produces a spectral peak at a frequency determined by the roots.\n\n\nFigure 5: AR(2) with complex roots: peak indicates quasi-periodic behavior\nThis is the recruitment series model from Chapter 13—the spectral peak corresponds to the damped oscillation in the ACF."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-solar-cycle",
    "href": "ts-freq-domain-slides.html#the-solar-cycle",
    "title": "Frequency Domain Methods",
    "section": "The Solar Cycle",
    "text": "The Solar Cycle\n\n\nFigure 6: Monthly sunspot numbers show quasi-periodic behavior\nAs we saw in Chapter 12, sunspots exhibit an approximately 11-year cycle—but with substantial variation in both amplitude and period."
  },
  {
    "objectID": "ts-freq-domain-slides.html#sunspot-spectrum",
    "href": "ts-freq-domain-slides.html#sunspot-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Sunspot Spectrum",
    "text": "Sunspot Spectrum\n\n\nFigure 7: Sunspot spectrum: dominant peak near 11-year period\nThe dominant peak corresponds to a period of approximately 11 years (132 months)."
  },
  {
    "objectID": "ts-freq-domain-slides.html#reading-the-sunspot-spectrum",
    "href": "ts-freq-domain-slides.html#reading-the-sunspot-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Reading the Sunspot Spectrum",
    "text": "Reading the Sunspot Spectrum\nThe spectrum reveals:\n\n\nDominant peak near frequency \\(1/132\\) cycles/month (~11-year period)\n\n\n\n\nBroad peak rather than sharp line → period varies from cycle to cycle\n\n\n\n\nHarmonics at higher frequencies → the cycle is not purely sinusoidal\n\n\n\n\nLow-frequency power → long-term modulation of cycle amplitude"
  },
  {
    "objectID": "ts-freq-domain-slides.html#annual-temperature-cycle",
    "href": "ts-freq-domain-slides.html#annual-temperature-cycle",
    "title": "Frequency Domain Methods",
    "section": "Annual Temperature Cycle",
    "text": "Annual Temperature Cycle\nDaily temperature data from Honolulu exhibit a clear annual cycle—the earth’s orbit imposes a 365-day periodicity.\n\nQuestion: How much of the total temperature variance is explained by the annual cycle?"
  },
  {
    "objectID": "ts-freq-domain-slides.html#temperature-series",
    "href": "ts-freq-domain-slides.html#temperature-series",
    "title": "Frequency Domain Methods",
    "section": "Temperature Series",
    "text": "Temperature Series\n\n\nFigure 8: Honolulu daily temperatures (1995-2020) show clear annual cycle\nThe annual cycle is visible but embedded in day-to-day variation. Mean temperature is 77°F with standard deviation 3.4°F."
  },
  {
    "objectID": "ts-freq-domain-slides.html#temperature-spectrum",
    "href": "ts-freq-domain-slides.html#temperature-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Temperature Spectrum",
    "text": "Temperature Spectrum\n\n\nFigure 9: Honolulu temperature spectrum: sharp peak at annual frequency"
  },
  {
    "objectID": "ts-freq-domain-slides.html#interpreting-the-temperature-spectrum",
    "href": "ts-freq-domain-slides.html#interpreting-the-temperature-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Interpreting the Temperature Spectrum",
    "text": "Interpreting the Temperature Spectrum\n\n\nSharp peak at \\(\\lambda = 1/365\\) → strong annual cycle\n\n\n\n\nPeak height relative to baseline → proportion of variance at that frequency\n\n\n\n\nLow-frequency power → AR-like day-to-day persistence (today predicts tomorrow)\n\n\n\n\nHarmonics (at 2/365, 3/365, …) → cycle not purely sinusoidal"
  },
  {
    "objectID": "ts-freq-domain-slides.html#variance-decomposition",
    "href": "ts-freq-domain-slides.html#variance-decomposition",
    "title": "Frequency Domain Methods",
    "section": "Variance Decomposition",
    "text": "Variance Decomposition\nThe spectrum answers: “What fraction of variance is at frequency \\(\\lambda\\)?”\n\nFor temperature data:\n\nAnnual cycle (and harmonics): typically 60–80% of variance\nDay-to-day persistence: most of the remainder\nHigh-frequency noise: small contribution\n\n\n\nThis quantifies what the time plot shows qualitatively—the annual rhythm dominates."
  },
  {
    "objectID": "ts-freq-domain-slides.html#cross-spectrum-and-coherence",
    "href": "ts-freq-domain-slides.html#cross-spectrum-and-coherence",
    "title": "Frequency Domain Methods",
    "section": "Cross-Spectrum and Coherence",
    "text": "Cross-Spectrum and Coherence\nWhen we have two time series, we can ask: at which frequencies do they move together?\n\nCoherence measures the correlation between two series at each frequency.\n\nRanges from 0 (no linear relationship) to 1 (perfect linear relationship)\nThe frequency-domain analog of correlation"
  },
  {
    "objectID": "ts-freq-domain-slides.html#hnl-vs-nyc-temperature",
    "href": "ts-freq-domain-slides.html#hnl-vs-nyc-temperature",
    "title": "Frequency Domain Methods",
    "section": "HNL vs NYC Temperature",
    "text": "HNL vs NYC Temperature\n\n\nFigure 10: Daily temperatures at Honolulu and New York City (1995-2020)\nBoth cities show annual cycles, but NYC has much larger amplitude (mid-latitude seasons) and more day-to-day variability."
  },
  {
    "objectID": "ts-freq-domain-slides.html#hnl-nyc-coherence",
    "href": "ts-freq-domain-slides.html#hnl-nyc-coherence",
    "title": "Frequency Domain Methods",
    "section": "HNL-NYC Coherence",
    "text": "HNL-NYC Coherence\n\n\nFigure 11: Coherence between Honolulu and NYC temperatures"
  },
  {
    "objectID": "ts-freq-domain-slides.html#interpreting-the-coherence",
    "href": "ts-freq-domain-slides.html#interpreting-the-coherence",
    "title": "Frequency Domain Methods",
    "section": "Interpreting the Coherence",
    "text": "Interpreting the Coherence\n\nAt the annual frequency: Coherence ≈ 1\n\nBoth cities warm in summer, cool in winter\nShared driver: Earth’s orbital cycle\n\n\n\nAt higher frequencies: Coherence ≈ 0\n\nDay-to-day weather in Honolulu tells us nothing about NYC\nLocal weather systems are independent\n\n\n\nCoherence identifies which periodicities are shared between series."
  },
  {
    "objectID": "ts-freq-domain-slides.html#time-frequency-duality",
    "href": "ts-freq-domain-slides.html#time-frequency-duality",
    "title": "Frequency Domain Methods",
    "section": "Time-Frequency Duality",
    "text": "Time-Frequency Duality\n\n\n\n\n\n\n\nTime Domain\nFrequency Domain\n\n\n\n\nACF, PACF\nSpectrum\n\n\nAR, MA, ARIMA models\nSpectral peaks, bandwidth\n\n\nForecasting\nIdentifying cycles\n\n\n“How does past predict future?”\n“What periodic structure is present?”\n\n\n\n\nThese are equivalent descriptions—Fourier transform pairs—but illuminate different aspects."
  },
  {
    "objectID": "ts-freq-domain-slides.html#when-to-use-which",
    "href": "ts-freq-domain-slides.html#when-to-use-which",
    "title": "Frequency Domain Methods",
    "section": "When to Use Which",
    "text": "When to Use Which\nTime domain excels for forecasting and short-term dependence modeling.\n\nFrequency domain excels for identifying periodicities and understanding cyclic mechanisms.\n\n\nBest practice: Use both—they reveal different aspects of the same reality."
  },
  {
    "objectID": "ts-freq-domain-slides.html#the-duality-in-practice",
    "href": "ts-freq-domain-slides.html#the-duality-in-practice",
    "title": "Frequency Domain Methods",
    "section": "The Duality in Practice",
    "text": "The Duality in Practice\nConsider sunspots:\n\n\nTime domain: AR(2) model captures the quasi-periodic behavior; enables forecasting\nFrequency domain: Spectrum peak at ~11 years reveals the solar cycle; shows it varies\n\n\n\nConsider global temperature:\n\n\n\nTime domain: Strong positive autocorrelation; trending behavior\nFrequency domain: Low-frequency dominance; variance concentrated at long periods"
  },
  {
    "objectID": "ts-freq-domain-slides.html#key-insights",
    "href": "ts-freq-domain-slides.html#key-insights",
    "title": "Frequency Domain Methods",
    "section": "Key Insights",
    "text": "Key Insights\n\nThe spectrum decomposes variance by frequency\n\nPeaks indicate dominant cycles\nFlat spectrum indicates white noise\n\n\n\n\nThe periodogram is the sample estimate\n\nUnbiased but inconsistent (high variance)\nAsymptotically \\(\\chi_2^2\\) distributed\n\n\n\n\n\nSmoothing reduces variance\n\nTrade-off: variance vs frequency resolution\nBandwidth controls the trade-off"
  },
  {
    "objectID": "ts-freq-domain-slides.html#key-insights-continued",
    "href": "ts-freq-domain-slides.html#key-insights-continued",
    "title": "Frequency Domain Methods",
    "section": "Key Insights (continued)",
    "text": "Key Insights (continued)\n\nARMA spectra connect the two domains\n\nAR peaks where polynomial roots near unit circle\nMA has inverse pattern\n\n\n\n\nTime and frequency are Fourier pairs\n\nSame information, different emphasis\nTime domain → forecasting\nFrequency domain → understanding periodic structure\n\n\n\n\n\nUse both perspectives\n\nComplete analysis draws on both\nUnderstanding enables better prediction"
  },
  {
    "objectID": "ts-freq-domain-slides.html#practical-checklist",
    "href": "ts-freq-domain-slides.html#practical-checklist",
    "title": "Frequency Domain Methods",
    "section": "Practical Checklist",
    "text": "Practical Checklist\nWhen performing spectrum analysis:\n\n\nPlot the time series first: Look for obvious periodicity, trend, level shifts\nChoose appropriate smoothing: More data → can use narrower bandwidth\nUse log scale: Constant confidence band width; see structure across magnitudes\nIdentify peaks: Convert frequency to period; interpret physically\nConsider ARMA spectra: Do peaks match expected model structure?\nConnect to time domain: Do ACF patterns match spectral features?"
  },
  {
    "objectID": "ts-freq-domain-slides.html#team-exercise-1-white-noise-spectrum",
    "href": "ts-freq-domain-slides.html#team-exercise-1-white-noise-spectrum",
    "title": "Frequency Domain Methods",
    "section": "Team Exercise 1: White Noise Spectrum",
    "text": "Team Exercise 1: White Noise Spectrum\nGenerate 256 observations of Gaussian white noise using stats::rnorm():\n\nCompute and plot the raw periodogram using stats::spec.pgram(..., spans = NULL).\nThe true spectrum is flat. How much does the periodogram deviate from flat?\nApply smoothing with spans = c(7, 7). How does the estimate improve?\nWhy is the periodogram inconsistent even though it’s unbiased?"
  },
  {
    "objectID": "ts-freq-domain-slides.html#team-exercise-2-identifying-periodicities",
    "href": "ts-freq-domain-slides.html#team-exercise-2-identifying-periodicities",
    "title": "Frequency Domain Methods",
    "section": "Team Exercise 2: Identifying Periodicities",
    "text": "Team Exercise 2: Identifying Periodicities\nUsing the SOI data (astsa::soi):\n\nPlot the spectrum with appropriate smoothing.\nIdentify the two dominant peaks. What periods do they correspond to?\nOne peak is at annual frequency. What physical phenomenon explains the other?\nHow does the spectrum reveal information that the ACF shows less clearly?"
  },
  {
    "objectID": "ts-freq-domain-slides.html#team-exercise-3-bandwidth-trade-off",
    "href": "ts-freq-domain-slides.html#team-exercise-3-bandwidth-trade-off",
    "title": "Frequency Domain Methods",
    "section": "Team Exercise 3: Bandwidth Trade-off",
    "text": "Team Exercise 3: Bandwidth Trade-off\nUsing the sunspot data (astsa::sunspotz):\n\nEstimate the spectrum with spans = c(3, 3) (narrow bandwidth).\nEstimate again with spans = c(15, 15) (wide bandwidth).\nHow do the estimates differ? Which shows the ~11-year peak more clearly?\nWhat would you choose for a final analysis, and why?"
  },
  {
    "objectID": "ts-freq-domain-slides.html#team-exercise-4-coherence-interpretation",
    "href": "ts-freq-domain-slides.html#team-exercise-4-coherence-interpretation",
    "title": "Frequency Domain Methods",
    "section": "Team Exercise 4: Coherence Interpretation",
    "text": "Team Exercise 4: Coherence Interpretation\nFor the HNL-NYC temperature data:\n\nAt what frequency is coherence highest? Why?\nAt what frequencies is coherence near zero? What does this mean physically?\nIf you added a third city (e.g., London), what coherence pattern would you expect with NYC?\nHow is coherence related to correlation? When might they differ?\n\n\nExercise 1 builds intuition for periodogram variability. Exercise 2 practices reading spectra. Exercise 3 addresses the key practical decision. Exercise 4 extends to bivariate analysis."
  },
  {
    "objectID": "ts-freq-domain-slides.html#discussion-questions",
    "href": "ts-freq-domain-slides.html#discussion-questions",
    "title": "Frequency Domain Methods",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\n“The spectrum and ACF contain the same information.” Why might the spectrum be more useful for some questions?\nYou observe a spectral peak but aren’t sure if it’s real or noise. How would you assess this?\nWhen would you prefer time domain methods over frequency domain methods for the same data?"
  }
]